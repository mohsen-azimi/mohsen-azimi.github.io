{"Affiliation":[{"label":"Affiliation","value":"Applied Science, Faculty of","attrs":{"lang":"en","ns":"http:\/\/vivoweb.org\/ontology\/core#departmentOrSchool","classmap":"vivo:EducationalProcess","property":"vivo:departmentOrSchool"},"iri":"http:\/\/vivoweb.org\/ontology\/core#departmentOrSchool","explain":"VIVO-ISF Ontology V1.6 Property; The department or school name within institution; Not intended to be an institution name."},{"label":"Affiliation","value":"Civil Engineering, Department of","attrs":{"lang":"en","ns":"http:\/\/vivoweb.org\/ontology\/core#departmentOrSchool","classmap":"vivo:EducationalProcess","property":"vivo:departmentOrSchool"},"iri":"http:\/\/vivoweb.org\/ontology\/core#departmentOrSchool","explain":"VIVO-ISF Ontology V1.6 Property; The department or school name within institution; Not intended to be an institution name."}],"AggregatedSourceRepository":[{"label":"AggregatedSourceRepository","value":"DSpace","attrs":{"lang":"en","ns":"http:\/\/www.europeana.eu\/schemas\/edm\/dataProvider","classmap":"ore:Aggregation","property":"edm:dataProvider"},"iri":"http:\/\/www.europeana.eu\/schemas\/edm\/dataProvider","explain":"A Europeana Data Model Property; The name or identifier of the organization who contributes data indirectly to an aggregation service (e.g. Europeana)"}],"Campus":[{"label":"Campus","value":"UBCV","attrs":{"lang":"en","ns":"https:\/\/open.library.ubc.ca\/terms#degreeCampus","classmap":"oc:ThesisDescription","property":"oc:degreeCampus"},"iri":"https:\/\/open.library.ubc.ca\/terms#degreeCampus","explain":"UBC Open Collections Metadata Components; Local Field; Identifies the name of the campus from which the graduate completed their degree."}],"Creator":[{"label":"Creator","value":"Azimi, Mohsen","attrs":{"lang":"en","ns":"http:\/\/purl.org\/dc\/terms\/creator","classmap":"dpla:SourceResource","property":"dcterms:creator"},"iri":"http:\/\/purl.org\/dc\/terms\/creator","explain":"A Dublin Core Terms Property; An entity primarily responsible for making the resource.; Examples of a Contributor include a person, an organization, or a service."}],"DateAvailable":[{"label":"DateAvailable","value":"2024-04-10T21:47:12Z","attrs":{"lang":"en","ns":"http:\/\/purl.org\/dc\/terms\/issued","classmap":"edm:WebResource","property":"dcterms:issued"},"iri":"http:\/\/purl.org\/dc\/terms\/issued","explain":"A Dublin Core Terms Property; Date of formal issuance (e.g., publication) of the resource."}],"DateIssued":[{"label":"DateIssued","value":"2024","attrs":{"lang":"en","ns":"http:\/\/purl.org\/dc\/terms\/issued","classmap":"oc:SourceResource","property":"dcterms:issued"},"iri":"http:\/\/purl.org\/dc\/terms\/issued","explain":"A Dublin Core Terms Property; Date of formal issuance (e.g., publication) of the resource."}],"Degree":[{"label":"Degree","value":"Doctor of Philosophy - PhD","attrs":{"lang":"en","ns":"http:\/\/vivoweb.org\/ontology\/core#relatedDegree","classmap":"vivo:ThesisDegree","property":"vivo:relatedDegree"},"iri":"http:\/\/vivoweb.org\/ontology\/core#relatedDegree","explain":"VIVO-ISF Ontology V1.6 Property; The thesis degree; Extended Property specified by UBC, as per https:\/\/wiki.duraspace.org\/display\/VIVO\/Ontology+Editor%27s+Guide"}],"DegreeGrantor":[{"label":"DegreeGrantor","value":"University of British Columbia","attrs":{"lang":"en","ns":"https:\/\/open.library.ubc.ca\/terms#degreeGrantor","classmap":"oc:ThesisDescription","property":"oc:degreeGrantor"},"iri":"https:\/\/open.library.ubc.ca\/terms#degreeGrantor","explain":"UBC Open Collections Metadata Components; Local Field; Indicates the institution where thesis was granted."}],"Description":[{"label":"Description","value":"Advances in computer vision and automated data collection methods, such as the\r\nintegration of physical and computational components in cyber-physical systems, have\r\nprovided opportunities for rapid and automated monitoring of infrastructures. The recent\r\nshift towards edge computing in AI-based applications has enabled data processing closer\r\nto the source, reducing the need for costly data transmission and providing a real-time\r\nresponse with lower latency and cost. Additionally, this approach can extend operations\r\nto remote locations where reliable high-speed internet is inaccessible.\r\nThe application of mobile robots in various fields, including structural health\r\nmonitoring, has gained significant attention in recent years. However, research and\r\ndevelopment in unmanned vehicles for structural health monitoring has been limited and\r\nprimarily focuses on manual outdoor operations with GPS availability. Indoor operations,\r\nsuch as construction site hazard inspections or indoor mapping, pose significant navigation\r\nand data collection challenges. An intelligent ground robot equipped with advanced\r\nhardware and software is needed to efficiently collect high-resolution data for structural\r\nhealth monitoring and navigation, process information in real-time, and save data for\r\npost-processing.\r\nThe integration of robots has accelerated data collection and processing in structural\r\nhealth monitoring. However, current applications need more autonomous capabilities,\r\nleading to time-consuming, subjective, and expensive applications. To address the\r\nlimitations of the earlier studies and fill the gaps found in the literature, this dissertation\r\nproposes an affordable solution for SHM using a cyber-physical system that incorporates\r\nboth unmanned aerial vehicles and unmanned ground vehicles through a wireless robot\r\noperating system network.\r\nIn addition, a novel transformer-based technique is proposed in this dissertation for\r\nhigh-resolution image segmentation to enable more accurate detection and quantification of\r\nstructural elements and damages. The key feature and contribution of the proposed method\r\nare to achieve high-accuracy pixel-level segmentation in a faster way. An extension of image\r\nsegmentation is developed to process point cloud data generated from 3D mapping and 3D\r\nscene reconstruction.","attrs":{"lang":"en","ns":"http:\/\/purl.org\/dc\/terms\/description","classmap":"dpla:SourceResource","property":"dcterms:description"},"iri":"http:\/\/purl.org\/dc\/terms\/description","explain":"A Dublin Core Terms Property; An account of the resource.; Description may include but is not limited to: an abstract, a table of contents, a graphical representation, or a free-text account of the resource."}],"DigitalResourceOriginalRecord":[{"label":"DigitalResourceOriginalRecord","value":"https:\/\/circle.library.ubc.ca\/rest\/handle\/2429\/87752?expand=metadata","attrs":{"lang":"en","ns":"http:\/\/www.europeana.eu\/schemas\/edm\/aggregatedCHO","classmap":"ore:Aggregation","property":"edm:aggregatedCHO"},"iri":"http:\/\/www.europeana.eu\/schemas\/edm\/aggregatedCHO","explain":"A Europeana Data Model Property; The identifier of the source object, e.g. the Mona Lisa itself. This could be a full linked open date URI or an internal identifier"}],"FullText":[{"label":"FullText","value":"Development and Implementation ofMulti-Agent Cyber-Physical Systemsfor Vision-Based Structural HealthMonitoringbyMohsen AzimiB.Sc., University of Mohaghegh Ardabili, 2009M.Sc., Iran University of Science and Technology, 2011M.Sc., North Dakota State University, 2017MBA, Quantic School of Business and Technology, 2020A THESIS SUBMITTED IN PARTIAL FULFILLMENT OFTHE REQUIREMENTS FOR THE DEGREE OFDOCTOR OF PHILOSOPHYinTHE FACULTY OF GRADUATE AND POSTDOCTORAL STUDIES(Civil Engineering)THE UNIVERSITY OF BRITISH COLUMBIA(Vancouver)April 2024\u00a9 Mohsen Azimi 2024The following individuals certify that they have read, and recommend to the Faculty ofGraduate and Postdoctoral Studies for acceptance, the thesis entitled:DEVELOPMENT AND IMPLEMENTATION OF MULTI-AGENT CYBER-PHYSICAL SYSTEMS FOR VISION-BASED STRUCTURAL HEALTH MON-ITORINGsubmitted by Mohsen Azimi in partial fulfillment of the requirements for the degree ofDoctor of Philosophy in Civil Engineering.Examining Committee:Tony T.Y. Yang, Professor, Department of Civil Engineering, UBCSupervisorCarlos Ventura, Professor, Department of Civil Engineering, UBCSupervisory Committee MemberNemkumar Banthia, Professor, Department of Civil Engineering, UBCSupervisory Committee MemberRyozo Nagamune, Professor, Department of Mechanical Engineering, UBCUniversity ExaminerBhushan Gopaluni, Professor, Department of Chemical and Biological Engineering, UBCUniversity ExaminerSriram Narasimhan, Professor, Department of Civil and Environmental Engineering,University of California Los AngelesExternal ExamineriiAbstractAdvances in computer vision and automated data collection methods, such as theintegration of physical and computational components in cyber-physical systems, haveprovided opportunities for rapid and automated monitoring of infrastructures. The recentshift towards edge computing in AI-based applications has enabled data processing closerto the source, reducing the need for costly data transmission and providing a real-timeresponse with lower latency and cost. Additionally, this approach can extend operationsto remote locations where reliable high-speed internet is inaccessible.The application of mobile robots in various fields, including structural healthmonitoring, has gained significant attention in recent years. However, research anddevelopment in unmanned vehicles for structural health monitoring has been limited andprimarily focuses on manual outdoor operations with GPS availability. Indoor operations,such as construction site hazard inspections or indoor mapping, pose significant navigationand data collection challenges. An intelligent ground robot equipped with advancedhardware and software is needed to efficiently collect high-resolution data for structuralhealth monitoring and navigation, process information in real-time, and save data forpost-processing.The integration of robots has accelerated data collection and processing in structuralhealth monitoring. However, current applications need more autonomous capabilities,leading to time-consuming, subjective, and expensive applications. To address thelimitations of the earlier studies and fill the gaps found in the literature, this dissertationproposes an affordable solution for SHM using a cyber-physical system that incorporatesboth unmanned aerial vehicles and unmanned ground vehicles through a wireless robotoperating system network.In addition, a novel transformer-based technique is proposed in this dissertation forhigh-resolution image segmentation to enable more accurate detection and quantification ofstructural elements and damages. The key feature and contribution of the proposed methodare to achieve high-accuracy pixel-level segmentation in a faster way. An extension of imagesegmentation is developed to process point cloud data generated from 3D mapping and 3Dscene reconstruction.iiiLay SummaryAs highlighted by reports like The Canadian Infrastructure Report Card, aging infrastruc-ture poses a critical challenge. Several techniques have been proposed to monitor infras-tructure health and address the issue. Recent advancements in computer vision, automateddata collection, and robotics have enabled monitoring infrastructure in real-time, even inremote areas. While mobile robots have been used in various fields, their application instructural health monitoring has been limited to manual operations. This dissertation in-troduces an intelligent robotic system via a wireless robot network with cutting-edge datacollection and real-time processing technology. In addition, this dissertation presents anovel method for precise high-resolution image segmentation and extends this technique toprocess 3D scan data. Furthermore, a framework is proposed to leverage large languagemodels like ChatGPT to enable more natural and intuitive human-robot interactions. Thiscomprehensive approach promises to revolutionize structural health monitoring, making itfaster and more accurate.ivPrefaceThroughout the development of this dissertation, a substantial portion of its content hasbeen published or submitted to journals, with several works currently under evaluation forpublication. This section serves to emphasize my contributions as the primary author orco-author.Published worksI was the primary author of these two papers and was responsible for the literature review,data collection and processing, model development, computational analysis, and presenta-tion. (Chapters 4 and 5). Sina Tavasoli was responsible for developing the control algo-rithms for the aerial robots, and Kasra Jabbari contributed to the design of the controllerfor the ground robot. Armin Dadras Eslamlou contributed to the writing and revising ofthe literature review and validating the research results under the supervision of T.Y. Yangand Shiping Huang.\u2022 Mohsen Azimi, Sina Tavasoli, Kasra Jabbari, and Yang T.Y. Integrated roboticssystem for autonomous mapping and 3D scene reconstruction in structural healthmonitoring applications. In Canadian Conference - Pacific Conference on EarthquakeEngineering, 2023.\u2022 Mohsen Azimi, Armin Dadras Eslamlou, T.Y. Yang, and Shiping Huang. Remotesensing and machine learning for enhanced post-disaster response: Insights from the2023 Tu\u00a8rkiye\u2013Syria earthquake. In Canadian Conference - Pacific Conference onEarthquake Engineering, 2023.The original method and algorithm for the paper below were developed by Armin DadrasEslamlou, the primary author of this research work. I contributed to the machine learn-ing models, visualization of results, validation, and writing the paper. This research wasconducted under the supervision of Dr. Ali Kaveh and Dr. T.Y. Yang. (Chapter 2).\u2022 Armin Dadras Eslamlou, Ali Kaveh, Mohsen Azimi, T. Y. Yang, \u201dStructural healthmonitoring via a group-theoretic WSA for optimal feature selection and data fusion.\u201dStructures. Vol. 57. Elsevier, 2023.For the book chapter below, Yifei Xiao and Yingnan Bao contributed to the roboticsarms section and the controlling algorithms presented in the paper. Xiao Pan was re-sponsible for writing about the application of the computer vision section. Sina TavasolivUnder-review\/accepted paperscontributed to the sections related to aerial robotics, and I wrote the sections related toground robots. Ehsan Noroozinejad Farsangi and Tony TY Yang supervised this project(Chapter 4).\u2022 Yifei Xiao, Xiao Pan, Sina Tavasoli, Mohsen Azimi, Yingnan Bao, Ehsan Noroozine-jad Farsangi, and Tony TY Yang. Autonomous inspection and construction of civilinfrastructure using robots. In Automation in Construction toward Resilience, pages1\u201326. CRC Press, 2024.For the conference paper below, Sina Tavasoli developed the original method for naviga-tion of aerial robots. Xiao Pan and I contributed to the content related to machine learningand the utilization of ground robots. Saudah Gazi assisted with data collection, writing,and revising. The project was conducted under the supervision of TY Yang. (Chapter 4)\u2022 Sina Tavasoli, Xiao Pan, TY Yang, Saudah Gazi, and Mohsen Azimi. Autonomousdamage assessment of structural columns using low-cost micro aerial vehicles andmulti-view computer vision. In Canadian Conference - Pacific Conference on Earth-quake Engineering, 2023.Chapter 2 (Literature Review) incorporates content from my previously published reviewpaper listed below. This paper is the foundation for this dissertation, and I was the primaryauthor. In this paper, Armin Dadras Eslamlou contributed to the vibration-based SHMsection. This study was conducted under the supervision of Gokhan Pekcan before startingmy PhD at UBC.\u2022 Mohsen Azimi, Armin Dadras Eslamlou, and Gokhan Pekcan. Data-driven struc-tural health monitoring and damage detection through deep learning: State-of-the-artreview. Sensors, 20(10):2778, 2020Under-review\/accepted papersI was the primary contributor to the papers below, significantly involved in all aspects andsections of both publications. Sina Tavasoli contributed to designing control algorithms foraerial robots and writing the paper, and Kasra Jabbari contributed to the hardware designof the ground robot. These studies were conducted under the supervision of T.Y. Yang.(Chapters 3 and 4).\u2022 Mohsen Azimi, Sina Tavasoli, Kasra Jabbari, T.Y. Yang, Multi-agent aerial-groundrobotic system for autonomous mapping and 3D reconstruction for structural damageassessment\u2022 Mohsen Azimi, T.Y. Yang, Transformer-based Framework for Accurate Segmenta-tion of High-Resolution Images in Structural Health MonitoringviTable of ContentsAbstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iiiLay Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ivPreface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vTable of Contents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . viiList of Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiList of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiiList of Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xvList of Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xviList of Abbreviations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xviiAcknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxDedication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxi1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11.2 Research Gaps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31.3 Research Goals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41.4 Objectives and Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . 51.5 Organization of the Dissertation . . . . . . . . . . . . . . . . . . . . . . . . 72 Literature Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82.1 Vision-Based SHM Using Deep Learning . . . . . . . . . . . . . . . . . . . 82.1.1 Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102.1.2 Segmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112.1.3 Change Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142.1.4 Change detection using satellite and aerial imagery . . . . . . . . . 152.1.5 Transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172.2 Cyber-Physical Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182.2.1 Ground Robots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192.2.2 Aerial Robots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20viiTable of Contents2.2.3 Multi-agent Cyber-physical Systems . . . . . . . . . . . . . . . . . . 222.3 Simultaneous Localization and Mapping . . . . . . . . . . . . . . . . . . . . 252.3.1 LiDAR-based SLAM . . . . . . . . . . . . . . . . . . . . . . . . . . 252.3.2 Visual SLAM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252.4 Large Language Models for Robotics . . . . . . . . . . . . . . . . . . . . . 273 Transformer-based Segmentation Framework for High-Resolution Images293.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293.2 Vision Transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293.2.1 Attention Mechanism in Vision Transformers . . . . . . . . . . . . . 303.3 Proposed Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313.3.1 Overview and Abbreviations . . . . . . . . . . . . . . . . . . . . . . 313.3.2 Segment Anything Model (SAM) . . . . . . . . . . . . . . . . . . . 323.3.3 Refine Anything Model (RAM) . . . . . . . . . . . . . . . . . . . . 333.3.4 DEtection TRansformer (DETR) . . . . . . . . . . . . . . . . . . . 353.4 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383.4.1 Training Refinement Model . . . . . . . . . . . . . . . . . . . . . . . 383.4.2 Training Object Detection Model . . . . . . . . . . . . . . . . . . . 393.5 Discussions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 403.5.1 Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . 403.5.2 Comparative Analysis of Segmentation Models . . . . . . . . . . . . 413.5.3 Performance of The Refinement Module with Different Input Resolu-tions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 443.6 Case Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 443.6.1 Multi-class Segmentation . . . . . . . . . . . . . . . . . . . . . . . . 443.6.2 Marker-free segmentation and tracking . . . . . . . . . . . . . . . . 463.7 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 484 Multi-agent Cyber-Physical Systems . . . . . . . . . . . . . . . . . . . . . 494.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 494.2 Unmanned Aerial Vehicle (UAV) . . . . . . . . . . . . . . . . . . . . . . . . 504.3 Unmanned Ground Vehicle (UGV) . . . . . . . . . . . . . . . . . . . . . . . 514.3.1 Design and Assembly . . . . . . . . . . . . . . . . . . . . . . . . . . 514.3.2 Hardware Components . . . . . . . . . . . . . . . . . . . . . . . . . 52Omni-directional mecanum wheels chassis . . . . . . . . . . . . . . . 52NVIDIA Jetson Nano 4GB . . . . . . . . . . . . . . . . . . . . . . . 54Arduino\/ESP32 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55Power Supply . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56WiFi and Bluetooth Module . . . . . . . . . . . . . . . . . . . . . . 564.4 Sensors and Cameras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 564.4.1 GPS Module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 564.4.2 RGB Camera . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 574.4.3 RGB-D Cameras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 574.4.4 360\u25e6 LiDAR Sensor . . . . . . . . . . . . . . . . . . . . . . . . . . . 58viiiTable of Contents4.5 Robot Operating System . . . . . . . . . . . . . . . . . . . . . . . . . . . . 604.5.1 ROS Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 604.5.2 Simulation and Visualization Tools for ROS . . . . . . . . . . . . . 614.6 Multi-agent Aerial-ground Systems . . . . . . . . . . . . . . . . . . . . . . 624.6.1 Component-level 3D Reconstruction Using Structure-from-Motion 634.6.2 Scene-level 3D Reconstruction Using Neural Radiance Fields . . . . 654.6.3 Software Components of The CPS . . . . . . . . . . . . . . . . . . . 65Collision-free Path Planning and Navigation Using RRT and GVD 65Hector SLAM Algorithm for Indoor Localization and Mapping . . . 694.7 From 2D to 3D Vision . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 714.8 LiDAR-derived Point Cloud Data . . . . . . . . . . . . . . . . . . . . . . . 724.9 LiDAR-based 3D Localization and Mapping . . . . . . . . . . . . . . . . . 734.10 Structure from Motion (SfM) . . . . . . . . . . . . . . . . . . . . . . . . . 764.10.1 Feature Extraction Using SIFT . . . . . . . . . . . . . . . . . . . . 774.11 Neural Radiance Fields (NeRF) . . . . . . . . . . . . . . . . . . . . . . . . 784.11.1 Multiresolution Hash Encoding . . . . . . . . . . . . . . . . . . . . 794.11.2 Application of NeRF in Post-disaster Response . . . . . . . . . . . 814.12 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 825 Implementations of Multi-agent Cyber-physical System . . . . . . . . . 845.1 Experimental Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 845.1.1 Ground-based vs. Aerial Exploration . . . . . . . . . . . . . . . . . 845.1.2 Remote Monitoring of UGV Hardware Utilization . . . . . . . . . . 855.1.3 Autonomous Exploration and Mapping . . . . . . . . . . . . . . . . 865.1.4 Structural Component Recognition and 3D Reconstruction . . . . . 875.1.5 Efficeint-SfM Using R-SAM . . . . . . . . . . . . . . . . . . . . . . 905.1.6 3D Reconstruction of Scene Using NeRF . . . . . . . . . . . . . . . 915.2 Point Cloud Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 945.2.1 Segmentation of 3D Point Clouds Using RANSAC . . . . . . . . . . 945.2.2 Segmentation of 3D Point Clouds Using Transformers . . . . . . . . 975.3 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1006 Conclusions and Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . 1016.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1016.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1016.3 Limitations and Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . 1036.3.1 Limitations and Future Work for Segmentation Models . . . . . . . 1036.3.2 Challenges and Prospects in 3D Vision Technologies . . . . . . . . . 1036.3.3 Limitations of Multi-agent CPS and Future Horizons . . . . . . . . 1046.3.4 Toward Generative Robotics: Limitations and Future Work for LLMs 104Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106ixTable of ContentsAppendicesA Toward Generative Cyber-Physical Systems . . . . . . . . . . . . . . . . 136A.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136A.2 Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137A.2.1 Prompt Engineering and API Development . . . . . . . . . . . . . . 138A.2.2 Engineering the reasoning . . . . . . . . . . . . . . . . . . . . . . . 139A.2.3 Engineering the response structure . . . . . . . . . . . . . . . . . . 140A.2.4 Speech recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141A.3 Evaluating LLM Capabilities for Enhanced Robotic Solutions: A Scenario-Based Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142A.3.1 Scenario I: Zero-shot problem-solving using ChatGPT . . . . . . . . 142A.3.2 Scenario II: User-LLM interaction for solving complex problems . . 146A.3.3 Scenario III: Embodied intelligence using deep learning . . . . . . . 151A.4 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153B Ground Robot Development Resources . . . . . . . . . . . . . . . . . . . . 154C Solid-state LiDAR Utilization Codes . . . . . . . . . . . . . . . . . . . . . 160D Codes for Creating COCO Annotation Files (*.json) . . . . . . . . . . . 165xList of Tables2.1 Robotic Platforms, Type, and Sensors [30]. . . . . . . . . . . . . . . . . . . 222.2 Comparison of UAVs, UGVs, and collaborative UAV-UGV. . . . . . . . . . 232.3 Popular SLAM algorithms available in ROS . . . . . . . . . . . . . . . . . . 263.1 Comparative analysis of various semantic segmentation models. . . . . . . . 424.1 Comparison of Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 504.2 Payload design for the UAV . . . . . . . . . . . . . . . . . . . . . . . . . . . 514.3 Comparison of solid-state and mechanical LiDAR. . . . . . . . . . . . . . . 60A.1 Principles for creating a robust initial prompt for an LLM. . . . . . . . . . 140xiList of Figures2.1 Concrete surface crack detection using different edge detection filters. . . . 92.2 Illustration of crack classification, localization, and segmentation. . . . . . . 92.3 The YOLOv5 architecture for object detection. . . . . . . . . . . . . . . . . 112.4 The U-Net and Nested U-Net architectures. . . . . . . . . . . . . . . . . . . 142.5 Regional change detection using satellite images . . . . . . . . . . . . . . . . 172.6 Examples of ground robots utilized by researchers for SHM. . . . . . . . . . 192.7 Examples of aerial robots utilized by researchers for SHM. . . . . . . . . . . 212.8 The UAV-UGV system in a grid world with obstacles . . . . . . . . . . . . 232.9 Examples of cooperative UAV-UGV systems . . . . . . . . . . . . . . . . . . 242.10 Block diagram of RTAB-MAP ROS node . . . . . . . . . . . . . . . . . . . 262.11 Expert in the loop vs. user in the loop. . . . . . . . . . . . . . . . . . . . . 283.1 Parallel attention mechanism. . . . . . . . . . . . . . . . . . . . . . . . . . . 313.2 The overview of the proposed R-SAM pipeline. . . . . . . . . . . . . . . . . 323.3 The overview of the Segment Anything Model. . . . . . . . . . . . . . . . . 323.4 The proposed Refine Anything Model (RAM) module. . . . . . . . . . . . . 333.5 Implementation pipeline of the RAM module in this study. . . . . . . . . . 343.6 The high-level architecture of DETR. . . . . . . . . . . . . . . . . . . . . . . 353.7 Detailed architecture of the DETR. . . . . . . . . . . . . . . . . . . . . . . . 363.8 Implementation pipeline of the object detection. . . . . . . . . . . . . . . . 373.9 Multi-level refinement. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383.10 RAM module performance on different imperfect input masks. . . . . . . . 393.11 Object detection results. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 403.12 The ground truth mask and boundary regions with different radiuses. . . . 413.13 Visual comparison of different segmentation methods. . . . . . . . . . . . . 433.14 Segmentation accuracy improvement using larger crop size. . . . . . . . . . 443.15 Multi-class segmentation and damage detection of wind turbine using SAM-based models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453.16 Segmentation and tracking using R-SAM. . . . . . . . . . . . . . . . . . . . 474.1 Components of a cyber-physical system. . . . . . . . . . . . . . . . . . . . . 494.2 Customized PCB for collision avoidance module for the UAV. . . . . . . . . 504.3 The custom-built unmanned ground vehicle (UGV). . . . . . . . . . . . . . 514.4 The omnidirectional robot chassis. . . . . . . . . . . . . . . . . . . . . . . . 524.5 The working principle and motion of the 4WD mecanum wheel UGV. . . . 534.6 Kinematic representation of the mecanum wheels. . . . . . . . . . . . . . . . 534.7 NVIDIA Jetson Nano 4GB Development Kit. . . . . . . . . . . . . . . . . . 55xiiList of Figures4.8 The microcontrollers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 554.9 Power supply units. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 564.10 Wifi and Bluetooth module. . . . . . . . . . . . . . . . . . . . . . . . . . . . 564.11 GPS module (L76X). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 574.12 The RGB camera. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 574.13 Stereo vision vs. time of flight. . . . . . . . . . . . . . . . . . . . . . . . . . 584.14 Visualization of the generated point cloud data using RGB-D cameras . . . 594.15 The mechanical 360\u25e6 LiDAR sensor. . . . . . . . . . . . . . . . . . . . . . . 594.16 A distributed ROS network with multiple sensors and computation nodes. . 614.17 The Gazebo (simulation environment) and RViz (visualization environment). 624.18 The configuration of the proposed multi-agent cyber-physical system. . . . . 634.19 Component-level data collection. . . . . . . . . . . . . . . . . . . . . . . . . 644.20 Component-level 3D reconstruction using Structure-from-Motion (SfM). . . 654.21 Generalized Voronoi Diagram for collision-free path planning for UAV. . . . 654.22 Collision-free path-planning using the RRT algorithm . . . . . . . . . . . . 664.23 Path planning using Voronoi diagram. . . . . . . . . . . . . . . . . . . . . . 674.24 Collision-free path-planning using the RRT algorithm . . . . . . . . . . . . 684.25 Collision-free navigation and mapping. . . . . . . . . . . . . . . . . . . . . . 684.26 Visualization of the Hector SLAM node graph. . . . . . . . . . . . . . . . . 704.27 Visualization of autonomous exploration and mapping. . . . . . . . . . . . . 714.28 3D point cloud data acquisition. . . . . . . . . . . . . . . . . . . . . . . . . 724.29 3D point cloud data acquisition using RGB-D camera . . . . . . . . . . . . 734.30 Real-time 3D mapping using LiDAR sensor. . . . . . . . . . . . . . . . . . . 754.31 3D Reconstruction using SfM. . . . . . . . . . . . . . . . . . . . . . . . . . . 764.32 SfM pipeline. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 774.33 Matching features. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 774.34 The SIFT algorithm. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 784.35 The Neural Radiance Fields. . . . . . . . . . . . . . . . . . . . . . . . . . . 794.36 Multi-resolution hash encoding. . . . . . . . . . . . . . . . . . . . . . . . . . 804.37 3D scene reconstruction using NeRF. . . . . . . . . . . . . . . . . . . . . . . 804.38 Quality assessment of NeRF results. . . . . . . . . . . . . . . . . . . . . . . 814.39 Results of the 3D reconstruction. . . . . . . . . . . . . . . . . . . . . . . . . 825.1 Comparing UAV vs. UGV for indoor mapping. . . . . . . . . . . . . . . . . 855.2 Jetson Nano utilization during the 2D-SLAM. . . . . . . . . . . . . . . . . 865.3 The exploration and mapping results. . . . . . . . . . . . . . . . . . . . . . 875.4 Training history and YOLOv5 performance. . . . . . . . . . . . . . . . . . 885.5 Component and damage localization in different environments. . . . . . . . 895.6 The 3D reconstruction results using the video captured by the UAV. . . . . 895.7 Multi-class segmentation using R-SAM. . . . . . . . . . . . . . . . . . . . . 905.8 3D reconstruction using original images and multi-class masked images. . . 915.9 GVD-based path planning. . . . . . . . . . . . . . . . . . . . . . . . . . . . 915.10 3D scene reconstruction using NeRF. . . . . . . . . . . . . . . . . . . . . . . 935.11 Segmentation of novel views from NeRF. . . . . . . . . . . . . . . . . . . . . 945.12 Flowchart of automatic segmentation of 3D point clouds. . . . . . . . . . . 95xiiiList of Figures5.13 Concrete damage quantification using the proposed algorithm. . . . . . . . 975.14 Overview of the CFS algorithm. . . . . . . . . . . . . . . . . . . . . . . . . . 985.15 Point cloud segmentation using R-SAM . . . . . . . . . . . . . . . . . . . . 995.16 Point cloud segmentation using the proposed method . . . . . . . . . . . . . 100A.1 The pipeline of the proposed ChatGPT-enabled generative robotics. . . . . 138A.2 Defining a task-specific API with explicit descriptions. . . . . . . . . . . . . 139A.3 Efficient tagging of the outputs for automatic parsing. . . . . . . . . . . . . 141A.4 Communicating with the robot through a Telegram bot. . . . . . . . . . . . 142A.5 Zero-shot code generation for measuring concrete crack length using ChatGPT.143A.6 Code verification for generated codes for measuring concrete crack length. . 144A.7 Zero-shot code generation for robot path planning. . . . . . . . . . . . . . . 145A.8 Zero-shot path-planning between two points using ChatGPT. . . . . . . . . 145A.9 Zero-shot code generation for flight control using ChatGPT. . . . . . . . . . 146A.10 Generating ArUco marker using ChatGPT. . . . . . . . . . . . . . . . . . . 147A.11 Generating Python codes for controlling DJI Tello drone using ChatGPT. . 148A.12 Interactive marker-based navigation for UAVs using ChatGPT. . . . . . . . 149A.13 Generating Python codes for 3D reconstruction using ChatGPT. . . . . . . 150A.14 3D reconstruction using the generated codes for COLMAP. . . . . . . . . . 150A.15 Generating Python codes for 3D mapping and detection using ChatGPT. . 152A.16 Validation of the real-time 3D mapping with integrated component detectionand segmentation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153B.1 The 4WD Ground Robot. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154C.1 The RealSense RGB-D Cameras. . . . . . . . . . . . . . . . . . . . . . . . . 160D.1 COCO annotation results. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165xivList of CodesB.1 Python code for the ground robot. . . . . . . . . . . . . . . . . . . . . . . . 154B.2 Arduino code for the ground robot . . . . . . . . . . . . . . . . . . . . . . . 157C.1 Python class for utilizing RealSense L515 LiDAR camera. . . . . . . . . . . 160D.1 Python class to create COCO annotation files. . . . . . . . . . . . . . . . . 165xvList of Algorithms1 UAV-based Scanning Pseudo Code . . . . . . . . . . . . . . . . . . . . . . . 642 RRT (Rapidly-Exploring Random Tree) . . . . . . . . . . . . . . . . . . . . 663 RANSAC (Random Sample Consensus) . . . . . . . . . . . . . . . . . . . . 954 DBSCAN (Density-Based Spatial Clustering) . . . . . . . . . . . . . . . . . 96xviList of Abbreviations1D: One-Dimensional2D: Two-Dimensional3D: Three-DimensionalAI: Artificial IntelligenceANN: Artificial Neural NetworkAPI: Application Programming InterfaceASCE: American Society of Civil EngineersBA: Boundary AccuracyBERT: Bidirectional Encoder Representations from TransformersBIM: Building Information ModelingCIRC: Canadian Infrastructure Report CardCNN: Convolutional Neural NetworkCOLMAP: Structure-from-Motion SoftwareCPS: Cyber-Physical SystemCPU: Central Processing UnitCUDA: Compute Unified Device ArchitectureDCNN: Deep Convolutional Neural NetworkDeiT: Data-efficient image TransformersDETR: Detection TransformerDL: Deep LearningDNN: Deep Neural NetworkFE: Finite ElementFFT: Fast Fourier TransformationxviiList of AbbreviationsGAN: Generative Adversarial NetworkGPU: Graphics Processor UnitGVD: Generalized Voronoi DiagramHD: Hausdorff DistanceIMU: Inertial Measurement UnitIoU: Intersection over UnionKRP: Kidnapped Robot ProblemLTS: Long-Term SupportLiDAR: Light Detection and RangingMAV: Micro-Aerial VehiclemAP: Mean Average PrecisionmBA: Mean Boundary AccuracyML: Machine LearningNLP: Natural Language ProcessingNeRF: Neural Radiance FieldsNF: Neural FieldsOpenCV: Open Source Computer Vision LibraryPSP-Net: Pyramid Spatial Pyramid NetworkPPM: Pyramid Pooling ModuleRAM: Refine Anything ModelR-CNN: Region-based Convolutional Neural NetworkRC: Reinforced ConcreteRNN: Recurrent Neural NetworkROS: Robot Operating SystemRGB: Red, Green, BlueRGB-D: Red, Green, Blue, and DepthRRT: Rapidly-exploring Random TreeSAM: Segment Anything ModelxviiiList of AbbreviationsSfM: Structure from MotionSHM: Structural Health MonitoringSLAM: Simultaneous Localization and MappingSSD: Single Shot DetectorSSL: Solid-State LiDARUGV: Unmanned Ground VehicleUAV: Unmanned Aerial VehicleUSB: Universal Serial BusViT: Vision TransformerYOLO: You Only Look Once (an object detection model)xixAcknowledgementsI want to take this opportunity to express my sincere gratitude to all those who havesupported and guided me throughout my academic journey.First and foremost, I extend my deepest appreciation to my family, who have supportedme with love. My wife, Kosar, has been the source of resilience, standing by my side throughevery challenge. My parents, Ghamar & Ghader, have trusted in my capabilities, guidingme to reach my fullest potential. My parents-in-law, Nasim & Amir, have supported mecontinuously. Their belief in my abilities helped me rebound from failures. My sister,Nasrin, and my brothers, Babak, Hashem, Kasra, and Rastin, have been more than just mywings for reaching the impossible. Their love and support are my greatest blessings, and Iwould never have taken this journey without them by my side and without their sacrifices.I wish to express my gratitude to Dr. Tony Yang, my supervisor at the Universityof British Columbia, for his support and guidance throughout my academic journey. Ialso deeply appreciate the contributions made by Dr. Carlos Ventura and Dr. NemkumarBanthia, members of my supervisory committee. Their constructive and critical feedbackon my research proposal helped me to shape this dissertation.I wish to express my sincere thanks to my esteemed former supervisors: Dr. GokhanPekcan from the University of Nevada, Reno and Dr. Zhibin Lin from North Dakota StateUniversity. Their insights and expertise significantly contributed to shaping the foundationof this dissertation. Despite the unforeseen twists and turns in my Ph.D. journey, theirinfluence will remain a significant part of my academic development.I am profoundly grateful for having great friends and colleagues. Their support, insights,and collaborative spirit have provided exceptional research opportunities and enriched myknowledge in many ways. I especially thank Armin Dadras Eslamlou, MohammadAli Her-avi, Sina Tavasoli, Leila Katebi, Milad Roohi, and Ehsan Vaghefi. Their expertise in bothprogramming and providing academic mentorship has been invaluable. I deeply appreciatefriends like Elmira Shoushtari, Mojtaba Alian, Elmira Faraji, and Erfan Mosaffa, who sup-ported me through the challenging times at the University of British Columbia, particularlythrough four years of financial difficulties.I also would like to acknowledge the use of the following GitHub repositories, as well:DJITelloPy developed by Escote\u00b4, Optical Control for DJI Drone developed by Szepessy,SSL-SLAM developed by Wang, YOLOv5 and YOLOv8 developed by Ultralytics, andDETR and Segment Anything by Meta. The LATEX class used for this dissertation buildsupon the class developed by Michael McNeil Forbes for The University of British Columbia.Partial support for this dissertation was provided through utilizing computational re-sources offered by Advanced Research Computing and Amazon Web Service.xxDedicationTo the angel behind the scenes throughout this endeavor, to my wife.xxiChapter 1Introduction1.1 BackgroundCivil infrastructure systems are susceptible to significant functional impairment from struc-tural deficiencies, primarily material degradation and external stressors, including earth-quakes, wind forces, vehicular loads, and ambient vibrations. In the United States, theinfrastructure\u2019s overall grade, assessed on a scale from A (indicating excellent condition)to F (indicating unacceptable condition), has been distressingly low, reaching a rating ofD+ for infrastructures and C+ for bridges. Retrofitting these infrastructures has been pro-jected to require a substantial investment of approximately $123 billion, as reported in [1].The report highlights that numerous bridges are classified as structurally deficient, fallingsignificantly below standard specifications, and many of their components are approach-ing the end of their service life. Moreover, it is worth noting that more than 30% of theroughly 617,000 highway bridges in the United States require immediate attention due todeteriorating conditions, as detailed in [2].In recent decades, priorities for engineers and researchers have shifted towards ensuringlife safety and reducing inspection costs. Consequently, the importance of cost-effectiveStructural Health Monitoring (SHM) has gained prominence to maintain long-term struc-tural integrity and safety standards, as evident in several publications [3\u20137]. Emerging SHMmethodologies offer the potential to streamline periodic inspections, thus curtailing bothdirect and indirect expenses associated with the undesirable deterioration of aging infras-tructure. These innovations complement traditional inspection and non-destructive evalua-tion techniques, such as impact echo, ultrasonic surface wave analysis, ground-penetratingradar, electrical resistivity, and others [8\u201310].At the core of any SHM approach lies the utilization of sensors and the data theygenerate. Recent strides in sensor and communication technologies, encompassing bothcontact and contactless, wired and wireless solutions, have facilitated the acquisition ofobservable responses at an unprecedented scale. Concurrently, ancillary hardware and soft-ware advancements have been harnessed in various capacities. Notably, significant empha-sis has been placed on developing technologies that allow efficient infrastructure mainte-nance through alternative means, such as unmanned aerial systems (UAS). Present UASresearch concentrated on fully autonomous multirotor platforms capable of prolonged hov-ering and equipped with diverse sensors for SHM purposes. These sensors encompass high-resolution visible-spectrum cameras, infrared and near-infrared imaging devices, LiDARsystems, radar, and sonar. A team of drones can inspect bridges for defects with minimalor no human intervention from the ground [11].Machine learning (ML) is an artificial intelligence (AI) subcategory. ML techniques aim11.1. Backgroundto develop trainable algorithms capable of learning from acquired or simulated data so thatfuture predictions can be made [12, 13]. ML-based SHM models are mainly designed withthe ability to learn by themselves and, from this point of view, DL can be considered as afeature-learning tool and a subset of ML [14\u201316].With the remarkable advancements in computer vision, recent research endeavors haveexplored adapting vision-based deep learning techniques to address civil engineering chal-lenges. Deep Learning (DL)-based Structural Health Monitoring (SHM) methodologieshave found applications in a range of domains, including general SHM [17], multi-leveldamage detection [18], corrosion identification [19], recognition of damage on concrete sur-faces [20], concrete crack detection [21], and pavement crack detection [22]. A commonobjective among these approaches is to replace traditional visual inspections with mod-ern, cost-effective, and safe methods that are adaptable to structures of varying types andscales [3, 23, 24].However, several drawbacks have been reported concerning direct image processingmethods in these applications. A prevalent issue in the majority of algorithms documentedin the literature is their customization to specific datasets or low-resolution data, potentiallyresulting in reduced performance in real-world scenarios marked by challenging conditions[25]. Additionally, the efficacy of these approaches heavily relies on the selection of pre-processing techniques, including but not limited to edge detection.High-resolution image segmentation is essential in structural health monitoring (SHM),enabling more accurate detection and quantification of structural elements and damages.The state-of-the-art deep learning models in vision-based segmentation for SHM were trainedin a supervised way, using low\/medium resolution images collected in controlled or labora-tory conditions. Applying such models in real-world scenarios results in inaccurate detectionand quantification, mainly due to the small training datasets and up-sampling of the low-resolution segmentation masks. Improving the segmentation accuracy for high-resolutionimages is one of the main goals of this research, which is discussed in detail in Chapter 3.In response to the time-consuming and manual nature of traditional infrastructure as-sessment methods, recent developments in computer vision and robotics, including cyber-physical systems (CPS) [26], have played a significant role in advancing SHM[27\u201329]. Ground-based robots [30, 31] and aerial robots [32, 33] have emerged as key players in this domain,offering efficient, autonomous, and adaptable solutions for inspecting structures of varioussizes and types. Ground robots equipped with advanced sensors enhance surface defectdetection [27\u201329], while aerial vehicles, equipped with high-resolution cameras and sensors,excel in assessing challenging-to-reach structures, such as bridges and tall buildings [32, 33].These technologies provide cost-effective, safe, and accurate alternatives to traditional in-spection methods [34], significantly improving the efficiency of structural evaluations. Amore detailed review of single-agent and multi-agent robotics in SHM is provided in Chap-ter 2. Furthermore, for the first time, this research investigated the potential of using largelanguage models in robotics for SHM applications.Transitioning from 2D vision-based methods to advanced 3D spatial data analysis ap-proaches in SHM, recent applications have seen the utilization of photogrammetry-basedtechniques. However, in many instances, spatial data processing still needs to be revised,21.2. Research Gapsleading to limitations in the realism of 3D reconstruction results. These limitations areoften associated with methods such as Structure from Motion (SfM), which extracts 3Dpoint clouds from 2D images [35]. In this dissertation, SfM is a bridge to understandingthe transformation from 2D to 3D spatial data [35]. Building upon the foundations ofSfM, state-of-the-art Neural Radiance Fields (NeRF) approaches are employed to addressthe limitations of complex scene reconstruction [36]. NeRF leverages neural networks todirectly model a scene\u2019s visual characteristics, offering high fidelity in capturing lighting ef-fects and spatial details [37]. To process the generated point clouds, this research introducesa novel pipeline that utilizes a 2D segmentation method for 3D point cloud segmentation,eliminating the need for training deep learning models or manual labeling of point clouddata. This approach opens the door to diverse applications in construction monitoring,structural damage assessment, and post-disaster response [38].1.2 Research GapsThe literature review highlights several gaps in current vision-based SHM, including theneed for deep-leaning models to detect various types of damages in a zero-shot way andwithout manual data annotation and training, integrating structural component recognitionwith damage detection and automation through robotics. Addressing such gaps would leadto more reliable and salable techniques for SHM. Below are the key limitations discussed inthis research.1. Automation and Scalability: Traditional damage detection and assessment meth-ods are labor-intensive, time-consuming, and subjective. There is a need for auto-mated and salable approaches without relying on experts on-site.2. Risk Reduction: Manual operations and reconnaissance methods pose particularrisks to the data collection teams, particularly in the event of a disaster and in inac-cessible or hazardous locations. Automated robotics systems could ensure the safetyof personnel during visual inspection.3. Remote Sensing: The potential of remote sensing methods, such as UAV-baseddata collection for structural health monitoring, is acknowledged; however, exploringthese techniques and integrating multi-agent robotics systems and sensors for damageassessment is necessary.4. Machine Learning: While machine learning methods have shown efficiency in dam-age assessments, there is a need to explore further the potential of transformer-basedmethods for object detection and segmentation tasks, particularly the integration oflanguage and vision modalities. Transformer-based models have shown promise in var-ious vision tasks but have not been extensively explored in the context of structuralhealth monitoring.5. Scalability for High-Resolution Data: Processing high-resolution image datafor traditional deep learning models poses computational and scalability challenges.31.3. Research GoalsThere is a need for scalable models that can handle high-resolution images withoutcompromising accuracy and inference time.6. Integration of Sensor Data: While the literature discusses the use of differenttypes of sensors, such as LiDAR and RGB-D, in robotics, there is a lack of discussionon integrating multi-modal sensor data. This research aims to utilize RGB-D andLiDAR sensors for efficient navigation and data collection using ground and aerialrobots.7. Adaptability in Dynamic Environments: Several robotics systems have been de-veloped and utilized for SHM applications; however, there is limited discussion on howsuch systems adapt to dynamic environments (e.g., construction sites) in real time.Further investigations could explore algorithms enabling robots to autonomously nav-igate, map, and inspect challenging environments with obstacles and changing condi-tions.8. Human-Robot Interaction and Collaboration: Despite the recent advancementsin the collaboration between different types of robots (e.g., UAVs and UGVs), there islimited discussion on human-robot interaction aspects. Future research could explorehow humans can interact with robotic systems for SHM, mainly through generativeAI.1.3 Research GoalsThis dissertation proposes four methods for processing high-resolution images, aiming tobridge the gap between 2D and 3D vision, developing affordable multi-agent robotics sys-tems for SHM, and utilizing large language models for human-robot interactions. The maingoals of this dissertation and the outcomes are summarized below.1. Propose a new framework for segmentation of high-resolution images:Within this framework, a novel method, R-SAM, is presented. R-SAM utilizes thestate-of-the-art Segment Anything Model (SAM) and generates panoptic segmentationmasks by integrating two more modules: refinement and detection. The refinementmodule refines the boundaries of generated masks by SAM, while the detection moduleidentifies the objects of interest. The proposed method\u2019s key feature and contributionare achieving high boundary accuracy and pixel-level segmentation in a faster andsemi-supervised way for high-resolution images.2. Extend the techniques and method from 2D vision to 3D vision: One of themain goals of this dissertation is to bridge the gap between 2D vision methods andtheir applications in 3D vision. To achieve this goal, novel 3D scanning methods areproposed, including neural radiance fields (NeRF) and multi-view 3D reconstructiontechniques. In addition, a lightweight real-time 3D scanning method using solid-stateLiDAR and ground robot is developed. Furthermore, the developed 2D segmentationmethod is extended to process 3D point clouds in an unsupervised approach.41.4. Objectives and Methodology3. Develop an affordable multi-agent cyber-physical system for structuralhealth monitoring: This dissertation focuses on integrating computer vision androbotics for structural health monitoring. To achieve the goal and bridge the gapsin the literature, a novel approach is proposed for an efficient and integrated multi-robot system that leverages the strengths of UAVs and UGVs. By combining thecapabilities of both types of robots, the proposed system aims to mitigate the limi-tations associated with each and improve the efficiency and speed of inspection andmapping tasks. The contribution of this study is to design a collaborative systemfor autonomous indoor inspections by utilizing the advantages of UGVs and UAVs.To achieve optimal performance, the UGV autonomously navigates within an indoorenvironment and scans the scene continuously using Light Detection And Ranging(LiDAR) and RGB-D sensors. Using this approach, scanning a scene at the desiredlocations with either aerial or ground robots is possible. The UGV and UAV contin-uously exchange communication signals within the developed wireless network usingthe ROS publisher-subscriber protocols, and the host computer interprets the vision-based algorithms. Such a multi-agent framework allows the UAV to land on the UGVand save power for more extended operations.4. Integrate large language models and robotics for vision-based SHM: Thisdissertation proposes a novel framework for applying large language models (LLMs),such as GPT4 and Llama-2, as a conversational interface for cyber-physical systems,enabling them to interact with humans more naturally and intuitively. A new strat-egy has been outlined to design prompts, create a high-level application programminginterface (API), and use ChatGPT for the free-form human-level dialog. Three typesof experiments were conducted with different complexities: zero-shot reasoning, ba-sic motion control of robots, and vision-based automation of tasks structural healthmonitoring (SHM). The outcome of this study demonstrates the feasibility of usingLLMs in augmenting human capabilities using cyber-physical systems, where human-machine interaction is primarily via natural language.1.4 Objectives and MethodologyThe primary objectives of this dissertation are to pioneer novel approaches in computervision and robotics using deep learning, with a specific focus on advancing 3D vision methodsand scanning techniques for structural health monitoring.Objective 1: Develop multi-agent cyber-physical systems for structural health monitoringapplications. This research endeavors to push the boundaries in autonomousexploration, mapping, structural component recognition, and 3D reconstruc-tion using a wireless network of multiple robots and computers. To achievethese goals, a comprehensive methodology is developed to build a roboticssystem, leveraging the power of advanced hardware components like theNVIDIA Jetson and Arduino, coupled with a multi-directional 4WD mobil-ity platform, as well as advanced solid-state and mechanical LiDAR sensors.51.4. Objectives and MethodologyFollowing the Robot Operating System protocols, software and computa-tional nodes are developed to implement the path planning and mapping al-gorithms such as Rapidly-exploring Random Tree (RRT) and Hector SLAM.Most of the computational nodes were developed using rospy, a Python clientlibrary for ROS. YOLOv5 and YOLOv8 models were trained using the Py-Torch and OpenCV libraries for the object detection model. The data werecollected from structural components in the structural engineering labora-tory at the University of British Columbia.Objective 2: Develop a new transformer-based method to achieve high accuracy in seg-menting high-resolution images. This research aims to explore the potentialof transformer models in the computer vision and segmentation of high-resolution images. Transformer models with the self-attention mechanismare to be investigated for their capability to capture long-range relation-ships between sequences through the unique implementation of a multi-headattention mechanism. The proposed methodology employs a three-modulepipeline, which includes the Segment Anything Model (SAM), the proposedRefine Anything Model (RAM), and the DEtection Transformer (DETR).This pipeline is designed to segment and label structural and non-structuralcomponents. The SAM module initially segments components based on colorand texture. Subsequently, the RAM module is introduced to refine the seg-mentation masks, enhancing their boundary accuracy. The final step involveslabeling the refined segmentation masks using DETR, a transformer-basedobject detection model. This comprehensive methodology aims to achieveaccurate panoptic segmentation results, particularly for identifying struc-tural components and quantifying damage in civil infrastructure.Objective 3: Extend the proposed segmentation method for processing 3D point clouds.One of the objectives of this research is to devise an unsupervised pointcloud segmentation framework, eliminating the dependency on manuallylabeled data and, thereby, enhancing the applicability of point cloud pro-cessing techniques. Specifically, the research aims to develop an algorithmcapable of efficiently segmenting point clouds by leveraging 2D segmenta-tion methodologies developed for high-resolution images. Additionally, forpre-processing, the Cloth Filter for Separation (CFS) algorithm is employedto separate point clouds into ground and non-ground measurements withoutrequiring intricate parameter tuning. To facilitate the automatic labelingprocess using 2D segmentation results, custom Python functions were de-veloped to convert the point cloud into an image representation and mapsegmentation results to the original point cloud.Objective 4: Develop a pipeline for integrating large language models for robotics with ap-plications in vision-based structural health monitoring. The overarching goalis to investigate the potential of Large Language Models (LLMs), specificallyChatGPT by OpenAI, in the context of human-robot interaction for gen-erative cyber-physical systems in structural health monitoring applications.61.5. Organization of the DissertationThe methodology includes three critical modules: defining task-specific APIswith clear descriptions to facilitate ChatGPT\u2019s understanding, constructingwell-engineered prompts task details, allowed functions, and response for-mats, and incorporating human-robot interaction for results evaluation andrefinement before deploying the final output on a real robot. This innova-tive approach seeks to enhance human-robot interaction, bridging the gapbetween natural language processing and code generation while retainingtraditional AI and high-level APIs.1.5 Organization of the DissertationThis dissertation is organized into the following chapters:Chapter 1: Introduction provides a general overview of the background, motivation,and primary goals, as well as the organization of the dissertation.Chapter 2: Literature Review provides a comprehensive review of vision-basedapproaches. Following an introduction, an evolutionary trajectory from shallow to deeplearning is presented, which offers a foundational understanding of the recent shift towarddata-driven methods. The chapter includes further discussions about classification, seg-mentation, object detection models such as YOLO (You Only Look Once), and their appli-cations in vision-based structural health monitoring. Furthermore, cyber-physical systemsand robotics, such as ground and aerial vehicles, are discussed along with multi-agent sys-tems. The role of large language models in human-robot interaction and the state-of-the-artmodels are reviewed as well.Chapter 3: Transformer-based Segmentation Framework for High-ResolutionImages proposes a new method for image segmentation of high-resolution images, wherea novel boundary accuracy improvement model is applied on top of the state-of-the-artsegmentation models. Application of the proposed method is presented for multi-class seg-mentation and tracking.Chapter 4: Multi-agent Cyber-Physical Systems discuss the design, hardware,and software components of the robots that are used for deployment of the vision-basedmethods. Specifically, details regarding the sensors and unmanned ground vehicles arediscussed. Furthermore, this chapter investigates the 3D scene reconstruction and mappingtechniques using LiDAR-based mapping, structure from motion, and neural radiance fieldmethods. This chapter presents a novel method for 3D point cloud segmentation using theproposed approach in Chapter 3.Chapter 5: Implementations of Multi-agent Cyber-physical System utilizesdeveloped aerial and ground robots through an integrated and wireless robot operatingsystem network. The proposed system is used for autonomous mapping, structural dam-age detection, and 3D scene reconstruction. Furthermore, this chapter proposes a novelframework for developing intelligent robotic systems using large language models. Severalexperiments were designed to utilize programmable robots.Chapter 6: Conclusions and Future Work provides a summary of the dissertationand the developed methods, their limitations, as well as the future work.7Chapter 2Literature Review2.1 Vision-Based SHM Using Deep LearningThe majority of the conventional infrastructure inspection techniques are based on visualassessments (i.e., crack existence, location, and width) that rely on experts\u2019 insight andexperience, which may not always be reliable [39, 40]. Besides, such techniques are costlyand time-consuming [41, 42]. The vision-based inspections can be performed by inspectingraw images (by humans and without post-processing), by image enhancement or by applyingbasic image processing filters to magnify and detect edges to accelerate the inspection, andby autonomous image processing tools that require computers and ML algorithms [43, 44].The main goal of the studies in the computer vision field is to extract information fromimage data to automatically recognize the real world (as visual cortex functions). Such ef-forts were initiated decades ago to detect edges, and it has been continuously developed forproblems with complex image patterns, such as facial recognition, vehicle, and pedestriandetection [45]. Jahanshahi and Masri [46] showed that morphological operations are not theonly image processing-based damage detection techniques. Other approaches include: bina-rization [47], image correlation [48, 49], edge detectors [23, 50\u201352], percolation model [53],fractal analysis [54], etc.Most of the efforts in computer vision are concentrated on developing end-to-end learningalgorithms while using artificial intelligence, mainly through DCNNs capable of achievingmore than 95% accuracy on image-based classification problems [55]. In addition, theapplication of CNNs has been extended to pixel-level labeling within an image to detectand localize different objects of interest while using nonlinear filters and feature maps [56].The recent advances in computer vision have brought more attention to such technologies inSHM as one of the most effective tools in the non-contact assessment of deflection [57, 58],corrosion [59], concrete spalling [60, 61], concrete and pavement cracks [21, 62, 63], fatiguedetection [64], and surface and subsurface damages [65, 66]. Dorafshan et al. [67] carriedout a comprehensive study to discuss the performance of vision-based image processingtechniques in SHM, which employ artificial intelligence.Infrastructures, particularly aging concrete structures, are prone to forming cracks dueto changing loading conditions, corrosion, etc. Cracks in concrete or road pavements usuallyappear as lines with random orientations and intensity. Usually, these lines are darker andconnected, and a simple crack detection can be carried out while using properly prescribedthresholds. Generally, two approaches have been used by researchers in vision-based SHM,particularly in crack detection: the image binarization method [68] and the sequentialimage processing method [53]. Binarization techniques for transforming images into blackand white pixels, or cracked and sound pixels, [69], as well as mathematical morphology [70]82.1. Vision-Based SHM Using Deep Learningcan facilitate and improve the accuracy of the detection process due to the nature of cracks.Prior to the introduction of DL in crack detection, traditional approaches used pixelgroups with similar color levels. The earlier generation of heuristic methods for vision-basedcrack detection in concrete structures was based on edge detection algorithms by applyingfilters, such as Roberts, Prewitt, Sobel, and LoG (in the spatial domain), or Butterworthand Gaussian (in the frequency domain) [71]. Figure 2.1 compares the results of applyingdifferent filters for detecting cracks on a concrete surface.Sensors 2020, 20, x FOR PEER REVIEW 5 of 36 approaches have been used by researchers in vision-based SHM, particularly in crack detection, the image binarization method [98], and the sequential image processing method [84]. Binarization techniques for transforming images into black and white pixels, or cracked and sound pixels, [99], as well as mathematical morphology [100] can facilitate and improve the accuracy of the detection process due to the nature of cracks. Prior to the introduction of DL in crack det ction, he traditional approach s have been using pixel groups with similar color levels. The earlier generation of heuristic methods for vision-based crack detection in concrete structures was based on edge detection algorithms by applying filters, such as Roberts, Prewitt, Sobel, and LoG (in the spatial domain), or Butterworth and Gaussian (in the frequency domain) [101]. Figure 4 compares the results of applying different filters for detecting cracks on a concret  surface.  Figure 4. Concrete surface crack detection using different edge detection filters. Abdel-Qader et al. [81] showed that the fast Haar transform has higher accuracy (86%) as compared to the other filters, such as Canny and Sobel, with 76% and 68% of accuracies, respectively. The image dataset that was used in this study, as well as the classification criteria, were further improved by Dorafshan et al. [97]. In general, major ML-based problems include three techniques: classification, localization, and segmentation. Figure 5 illustrates the frequent crack detection approaches: classification [25], object localization [23], and pixel-level segmentation [102]. Using the classification method, the dataset is labeled as cracked, non-cracked (sound). In the crack localization method, the cracks within each input image are labeled with bounding boxes. In the pixel-level segmentation method, the pixels are classified as cracked and non-cracked [103].  Figure 5. Illustration of (a) crack and non-crack image classification, (b) object localization, and (c) pixel-level segmentation (open-access images from Yang et al. [102]). Dorafshan et al. discussed a comparison of different edge detection methods and performance of different filters have been discussed by [97]. Based on ANN-based image processing methods, several researchers highlighted potential applications of autonomous crack detection techniques. Jahanshahi and Masri [104] proposed ML-based models using SVMs for concrete crack detection, based on morphological features. The crack width was measured by identifying the centerline of (a) (c) (b) Figure 2.1: Concrete surface crack detection using different edge detection filters.Abdel-Q der et al. [50] showed th t the fast Haar tra sform has higher ac uracy (86%)as compared to the other filters, such as Canny and Sobel, with 76% and 68% of accuracies,respectively. The image dataset used in this study and the classification criteria werefurther im roved by Dorafshan et al. [67]. Major ML based problems generally includethree techniques: classification, loc lization, and segmentatio . Figure 2.2 illustrates thefrequent crack detection approaches: classification [72], object localization [18], and pixel-level segmentation [73]. Using the classification method, the dataset is labeled as crackedor non-cracked (sound). The crack localization method labels the cracks within each inputimage with bounding boxes. In the pixel-level segmentation method, the pixels are classifiedas cracked and non-cracked [74].Sensors 2020, 20, x FOR PEER REVIEW 5 of 36 approaches have been used by researchers in vision-b sed SHM, particularly in crack detection, the image binarization meth d [98], and the sequential image pr cessing meth d [84]. Binarization techniques for transforming images into black and white pixels, or cracked and sound pixels, [99], as well as mathematical morphology [100] can facilitate and improve the accuracy of the detection process due to the nature of cracks. Prior to the introduction of DL in crack detection, the traditional approaches have been using pixel groups with similar color levels. The earlier generation of heuristic methods for vision-based crack det ction in concrete struct r s was b sed on dge detection algorithms by applying filters, such as Rob ts, Prewit , Sobel, and LoG (i  the spatial domain), or Butterworth and Gaussian (in the frequency domain) [101]. Figure 4 compares the results of applying different filters for detecting cracks on a concrete surface.  Figure 4. Concrete surface crack detection using different edge detection filters. Abdel-Qader et al. [81] showed that the fast Haar transform has higher accuracy (86%) as compared to the other filters, such as Canny and Sobel, with 76% and 68% of accuracies, respectively. The image dataset that was used in this study, as well as the classification criteria, were further improved by Dorafshan et al. [97]. In general, major ML-based problems include three techniques: classificatio , localization, and segment tion. Figure 5 illustrates the frequent crack detection approaches: classification [25], object localization [23], and pixel-level egmentation [102]. Using t e classification method, the dataset is labeled as cracked, non-cracked (sound). In the crack loc lization method, the cracks within each input image are labeled with bounding boxes. In the pixel-level segmentation method, the pixels are classified as cracked and non-cracked [103].  Figure 5. Illustration of (a) crack and non-crack image classification, (b) object localization, and (c) pixel-level segmentation (open-access images from Yang et al. [102]). Dorafshan et al. discussed a comparison of different edge detection methods and performance of different filters have been discussed by [97]. Based on ANN-based image processing methods, several researchers highlighted potential applications of autonomous crack detection techniques. Jahanshahi and Masri [104] proposed ML-based models using SVMs for concrete crack detection, based on morphological features. The crack width was measured by identifying the centerline of (a) (c) (b) Figure 2.2: Illustration of (a) crack and non-crack image classification, (b) object localiza-tion, and (c) pixel-level segmentation (open-access images from Yang et al. [73]).Dorafshan et al. [67] compared different edge detection methods and the perf rmanceof different filters. Based on ANN-based image processing methods, several researchers92.1. Vision-Based SHM Using Deep Learninghighlighted potential applications of autonomous crack detection techniques. Jahanshahiand Masri [75] proposed ML-based models using SVMs for concrete crack detection basedon morphological features. The crack width was measured by identifying the centerline ofcracks in their study. Using the abovementioned techniques, an automated vision-basedcrack detection framework was proposed by Yeum and Dyke [76] for bridge inspection.Only a limited number of studies have attempted to compare the performance of recentlydeveloped crack detection methods by other researchers [67, 68, 77]. In addition, mostrecent studies have yet to clearly describe the accuracy and classification criteria, includingtrue positive (TP) metrics for reproducibility of the results. Furthermore, a comparison ofseveral studies from a broadly different range of datasets [78], as well as comparisons usingsmall datasets or the idealized datasets that were collected in laboratory conditions [67]do not reflect the merits of one method over another. Dorafshan et al. [23] and Talab etal. [52] proposed an automatic crack detection using the OTSU threshold [79] and imagefiltering. Such methods were later improved by implementing terrestrial laser scanning,which has three main steps: shading correction, crack detection, and mapping [80], andcould be implemented in an automated manner using robotic systems yielding up to 95%accuracy [81].Researchers in the computer vision field have consistently developed deep CNNs; Rawatand Wang present more details regarding the background of the CNN developments in imageclassification [82]. Following such significant achievements, several studies adapted CNN todetect surface and subsurface cracks in pavement and concrete. Earlier studies used DCNNto classify concrete or pavement surfaces by sliding-window method, but, recently, semanticsegmentation through an end-to-end pipeline using fully convolutional networks (FCN) hasattracted attention [83, 84]. This approach has been employed to tackle challenging clas-sification problems in different fields, including SHM. Recently, Chen and Jahanshahi [85]developed an enhanced CNN-based crack detection method while using a Na\u00a8\u0131ve Bayes datafusion scheme for the extracted data from video frames. Kim et al. [41] proposed a fasterCNN-based model to determine pixel-wise location while using image binarization.2.1.1 DetectionFor real-time damage detection and localization using the UAV, the YOLOv5 algorithmis used. The YOLO-based models have at least eight successive versions and many othersubvariants. Each YOLO model also typically comes with various subvariants that mainlydiffer in the model size. YOLOv1 [86] was introduced in 2016 and followed by YOLOv2 [87],YOLOv3 [88], YOLOv4 [89]), YOLOv5 and YOLOv8 [90], and other new versions. TheYOLOv5 and YOLOv8 have three main components: a backbone for feature extraction, aneck to generate a feature pyramid, and a detection head for prediction from the anchorboxes. Different variants of YOLOv5 and YOLOv8 (n, s, m, l, and x) were introducedbased on size and accuracy. As the backbone, YOLOv5 uses a cross-stage partial networkwith Darknet (CSPDarknet); in this way, the problem with the gradient in large backbonesis solved, and by decreasing the model\u2019s parameters, inference speed increases. As theneck, YOLOv5 uses a path aggregation network (PANet) with a feature pyramid network(FPN) to improve the propagation of the low-level features, which enhances localization102.1. Vision-Based SHM Using Deep Learningaccuracy in general. As the head, YOLOv5 creates three different feature maps for multi-scale prediction for different object sizes. Due to the importance of the speed of inferenceon the limited hardware resources of the mobile robot, YOLOv5s are used in this study.FocusConvC3ConvC3ConvC3ConvSPPFBackboneConvC3ConcatUpsampleConcatUpsampleConvC3C3ConvConcatC3ConvConcatC3Conv2dConv2dConv2dNeck Detection Head  Figure 2.3: The YOLOv5 architecture for object detection.2.1.2 SegmentationDetecting cracks in tunnels is of vital importance. They might be a potential sign of ahidden danger that can pose severe threats to users or even trigger a catastrophic collapse.On the other hand, identifying tunnel cracks is challenging because the tunnel images havemany noise patterns. Therefore, developing automated, accurate methods for monitoringtheir surfaces can enhance safety and decrease potential costs. Li et al. [91] created a60,000 tunnel crack images database for training, testing, and comparing different cracksegmentation networks. According to their study, by introducing clique block and attentionmechanisms into U-net, it can significantly outperform basic U-net, fully convolutionalnetworks (FCN), SegNet, and multi-scale fusion crack detection (MFCD) for detectingcracks in tunnel noisy images.Soloviev et al. [92], Li et al. [93], Tong et al. [94], and Fan et al. [95] demonstratedthe use of DCNNs to detect and recognize cracks as defects with quantifiable properties inapplications for crack detection on pavement surfaces (e.g., crack length and size). Fan etal. [96] proposed a CNN-based multi-label classifier by improving the positive-to-negativeratio of samples. In another study by Wang et al. [97], they proposed a CNN model112.1. Vision-Based SHM Using Deep Learningwith three blocks of convolutional layers followed by two FC layers, consisting of 1,246,240trainable parameters in total, which could detect surface cracks from the subdivided imagesof asphalt pavement. Tong et al. [94] also developed another two-stage CNN-based model todetect asphalt pavement crack length. A fast pavement crack detection network (FPCNet)was developed by Liu et al. [98] using encoder-decoder configurations.Most studies in the literature validated the performance of DL models implemented inlaboratory conditions with image datasets of intact and cracked surfaces, which still haslimitations in addressing real-world conditions. The acquired surface images may be con-taminated with noise, shadow, dust, or extra brightness, which requires more robust andintelligent techniques for classification. Several studies have addressed such practical chal-lenges Depending on the applications. Kim and Cho [99] defined a five-class crack detectionproblem using a large volume of images collected from the Internet and their augmenta-tions. Their study considered field conditions to tackle real-world limitations associatedwith several uncertainty factors and the inability to employ contextual information, suchas the nature of materials, structural components, and the region of interest (ROI). Cha etal. [72] discussed the feasibility of autonomous DL-based methods for crack detection, andCha and Choi [100] proposed CNN-based classifiers and applied a sliding window methodon 256 \u00d7 256 RGB images of concrete surfaces to detect cracks. Their proposed methodsachieved an accuracy of 97% for concrete image datasets when considering different light in-tensities associated with variable weather conditions. An et al. [101] developed a DL-basedcrack detection method using hybrid images of combined vision and infrared thermogra-phy of macro- and micro-cracks. They observed that the hybrid images made the networkrobust against varying operational conditions such as shadow, dust on the surface, rust,etc. Moreover, they developed a sticking-type UAV that can be utilized to inspect largereinforced concrete civil infrastructures. Jang et al. [102] devised a ring-type robot for crackevaluation of circular bridge piers in a controlled manner. This robot provides fast scanningand high-quality raw images for crack detection. Feeding these images, they trained a CNNto segment the crack maps on the piers precisely. According to the experimental results,the proposed system could identify images with 97.47% recall and 90.92% precision.In typical region-based classification or object detection, a bounding box is createdaround the region of interest (e.g., cracks, spalling, components, etc.) [67]. For example, Aliet al. [103] proposed a modified cascade face detection method that uses the Viola\u2013Jonesalgorithm for crack detection on concrete walls while using bounding boxes around theregion of the crack. This method was modified by Ramana et al. [104, 105] to automaticallydetect loosened bolts in steel structures with higher efficiency when compared to the earlierstudies using hand-crafted features [106].Yeum et al. [107] used region-based CNNs (R-CNN) for post-event evaluation of build-ings with an accuracy of nearly 60%; however, this technique requires further developmentsalso to include multiple damage scenarios. Xu et al. [108] proposed a fast R-CNN approachto detect different damage types in concrete structures and damage locations using bound-ing boxes. Fast R-CNN and Faster R-CNN were developed by Girshick [109] and Ren etal. [110], respectively. Another newly developed region-based segmentation technique isMask R-CNN [111], which segments images into objects and can be used for crack detec-tion, concrete spalling, and rebar detection. Cha et al. [18] proposed a faster R-CNN for122.1. Vision-Based SHM Using Deep Learningdetecting multiple damage types and developed a method to localize various damage types,including steel and bolt corrosion and delamination. One of the main drawbacks of theregular CNN approaches for detecting cracks is their need to specify out-of-plane cracks.Deng et al. [112] recently embedded deformable modules into various R-CNN and fully con-volutional networks to overcome this drawback. When comparing the suggested techniquewith regular networks, they observed that the modified approach improves the detectionaccuracy of out-of-plane cracks and enhances the accuracy for other cases.Other studies proposed pixel-level classification methods [21] to provide more preciseinformation regarding the path and intensity of cracks. In most published research, the bi-nary classification problems include distinguishing \u2018crack\u2019 and \u2018non-crack\u2019 regions or pixels.For more precise classification, Dung and Anh [21] proposed semantic segmentation also toidentify path and density, The typical object detection models attempt to fit a boundingbox around the ROI [55], and semantic segmentation methods [113] or pixel-level classifi-cation [73], should be used to delineate damage level, shape, and location precisely. Forpavement crack detection problems, Zhang et al. [56, 114] proposed CrackNet, an efficientmodel based on R-CNNs. Xu et al. [64] developed a DL-based fatigue crack identificationtechnique for long-span steel box girder bridges using deep CNN, as well as a framework forsteel crack detection while using a restricted Boltzmann machine with high accuracy [115].Hoskere et al. [116, 117] proposed a pixel-wise DCNN with a parallel configuration anda fully CNN (FCN) to localize and classify different damages, including concrete cracks,spalling, exposed rebars, corrosion, fatigues cracks, and asphalt cracks.Figure 2.4 shows the typical architecture of UNet and Nested UNet models for semanticsegmentation tasks. In general, as the segmentation model has an encoder to extract featuresand pass them to a decoder, the segmentation and classification heads are used to determinethe pixel-level class.132.1. Vision-Based SHM Using Deep LearningI(a)                             \u2198                                \u2197                                   \u21e2                Encoder SEG Head CLS HeadCat.Cat. Cat.Input OutputDown-sampling Up-sampling Skip connection CNN-layers(b)Figure 2.4: The U-Net (a) and Nested UNet (b) architectures for image segmentation.2.1.3 Change DetectionIt is essential to perform a global-level inspection and the structural component recognitionprocess before moving closer to the details to understand the relationship between damageand safety of structures [57]. However, recent studies in DL-based SHM have not fullyaddressed this concern. Even the video-based crack detection models do not interpret theimpact of damage in a global context. Yeum et al. [118] proposed a CNN-based techniqueto classify civil infrastructure images by recognizing regions of interest. Gao and Mos-alam [119] used similar object-detection techniques to classify structural components anddamage types. A Faster R-CNN algorithm was used by Liang [120] to automatically detectstructural components of the RC bridge system using boundary boxes.Narazakia et al. [121] used global and close-up views to train two recurrent neural net-142.1. Vision-Based SHM Using Deep Learningworks (RNNs) while using a single image-based pre-trained FCN for structural componentrecognition from video image data. The simple RNN and ConvLSTM units in their modelscould learn memories of the focus region of the video. The ground-truth labels for thevideo frames of flying UAVs were synthetically created using a game engine [55]. Theiroverall goals were associated with the recognition, localization, and structural componentclassification from complex scenes. Each input image is automatically or semi-automaticallydown-sampled using convolutional layers and then up-sampled to generate the segmentedimage that is similar to the ground truth, as shown in Figure 2.4.Alcantarilla et al. [122] proposed street-view (ground-level) change detection using de-convolutional networks. Using a CNN model, Stent et al. [123] proposed a tunnel changedetection method. The main assumptions in these studies were that the cracks are con-nected to slender and darker lines on concrete surfaces [53].2.1.4 Change detection using satellite and aerial imageryOn February 6, 2023, two major earthquakes with magnitudes of 7.7 and 7.6 (Mw) strucknear southern and central Tu\u00a8rkiye and northern regions of Syria, the largest since the 1939Erzincan earthquake. The official death toll had exceeded 57,000, with countless othersinjured. The twin earthquakes and the aftershocks caused significant damage to the region,resulting in more than a million people being left homeless [124]. This disaster revealedseveral shortcomings in construction, infrastructure management, and emergency responsesystems, despite previous experiences with deadly earthquakes, such as the 1999 Go\u00a8lcu\u00a8k(Marmara) earthquake.Rapidly evaluating damage to civil infrastructure after a disaster is essential for promptemergency response and resource allocation [125]. The traditional methods for damageassessment using ground stations are time-consuming and labor-intensive, which can onlybe fully automated or scaled with having experts on-site [126]. Furthermore, Reconnaissanceafter an earthquake significantly impacts damage assessment, locating survivors, preventingsecondary hazards, accelerating the recovery process, and gathering experience about theevent [127]. However, current paradigms for reconnaissance are labor-intensive and slowbecause, in the early stages of the recovery, human resources are often involved in otherurgent priorities. In addition, manual methods for surveillance can cause further risksfor the teams gathering data from inaccessible or hazardous locations. Hence, automatedreconnaissance provides a more efficient, accurate, and safe way to assess damage and collectdata. Data science can make this process automated with the use of algorithms to analyzedata and develop tools for effective decision-making.The recent advances in machine learning (ML) and data science demonstrated the effi-ciency of ML-based methods in damage assessments before and after a disaster [17, 128\u2013133].Since most data-driven damage assessment approaches require the health state of structures,collecting pre-event and post-event data can enhance the process and provide a baseline forthe region of interest [134]. In this regard, remote sensing technologies, such as satellitesand unmanned aerial vehicles (UAVs), can be applied to collect data on structures andinfrastructures in their healthy states. The advances in remote sensing and aerial imageryenabled the obtaining of high-resolution images within days of a disaster. For instance,152.1. Vision-Based SHM Using Deep Learningthe WorldView-2 satellite, with 3.7 days, can accelerate regional damage assessment us-ing computer vision and AI for generating building footprints and damage levels after anevent [135]. The prevalent techniques for evaluating building damages with satellite imageryand computer vision are done mainly through segmentation and classification. Several stud-ies have used convolutional neural networks (CNNs) to develop desired performance levelsfor them [136, 137]. As a semantic segmentation task for change detection, building dam-age assessment can be modeled using a CNN-based network that takes in a concatenationof pre- and post-disaster satellite images and outputs the changes. Such models can befollowed by damage classification at the pixel level [138].Along with the advancement of machine learning techniques, autonomous approachesfor change detection (CD) using satellite imagery have been developed, which can reducethe time and effort required for manual on-ground interpretations [139]. In the context ofregional damage assessment, change detection can detect and classify changes in buildingconditions by comparing the images before and after a disaster, such as earthquakes, floods,and tornados. This approach can provide valuable information for emergency respondersand aid organizations to prioritize their efforts and allocate resources more efficiently. Ma-chine learning models such as convolutional neural networks (CNNs) have shown promisingresults in building damage assessment using satellite imagery for change detection [140].This study utilized a Bitemporal Image Transformer (BIT) model to detect changes usingsatellite images before and after a disaster. To maximize the performance of the changedetection task, the BIT uses the strength of transformers as well as the CNNs [139].The before-after satellite imagery, obtained from MAXAR [141], was used for changedetection to identify damaged buildings. The Bitemporal Image Transformer (BIT) modelwas employed to detect changes in building conditions before and after the 2023 Tu\u00a8rkiye-Syria earthquake. The BIT model utilized a combination of transformers and CNNs tomaximize the performance of the change detection task. The results in Figure 2.5 show thatthe model effectively detected changes in building conditions, and the damaged buildingswere successfully identified [38]. The change detection approach using satellite imagery canprovide valuable information for emergency responders and aid organizations in prioritizingtheir efforts and allocating resources more efficiently.162.1. Vision-Based SHM Using Deep Learning Damaged buildings \u0130slahiye, Gaziantep, T\u00fcrkiye Oct 4, 2022 \u0130slahiye, Gaziantep, T\u00fcrkiye February 7, 2023 Figure 2.5: Regional damage assessment using change detection based on before-after satel-lite images2.1.5 TransformersTraditional structural health monitoring (SHM) models rely on physics-based models withlimited capabilities and may not be suitable for processing large volumes of data. Onthe other, data-driven models using machine learning (ML) provide versatile solutions, andthus, they have been at the center of attention during the past few years [14]. Deep learning(DL) has been used for developing end-to-end classification, detection, and segmentationtasks [130, 131]. Most of the recent DL-based algorithms for structural health monitoring(SHM) utilize convolution neural networks (CNNs), which have unparalleled performancein extracting local information but are limited in capturing long-range relationships be-tween the features [142]. Therefore, CNN-based models perform poorly in extracting globalcontextual features from high-resolution images (e.g., damage detection under occlusion).On the other hand, the resolution of portable cameras has increased significantly in thelast few years. Due to the computational cost associated with processing high-resolutionimage data for deep learning, most recent computer vision models in SHM are designedand trained for low-resolution input data. Typical CNN-based models may not be suitablefor high-resolution data because larger receptive fields are needed to capture the entire seg-mented regions. One approach that has been used in recent years is training a deep learningmodel with down-sampled images, where the details are removed. Other approaches havetrained such models using cropped images, which damages the context of the images [130].Furthermore, fitting 4K image segmentation models into GPU memory is still challeng-ing for typical computers. Even with stat-of-the-art high-performance computers, properlyannotated high-resolution images are scarce for SHM tasks.172.2. Cyber-Physical SystemsThe earlier generation of CNN-based semantic segmentation models may achieve a highaccuracy level compared to non-CNN-based approaches. However, for pixel-level labeling, itis important to develop models to capture contextual information [143]. Image and featurepyramid methods use multi-level input images or feature maps. The proposed algorithmuses pyramid pooling [144] to extract features from the input images. Methods such asregion growth [145] have been proposed recently to create high-resolution segmentationrefinement. However, such models cannot process high-resolution images due to computa-tional limitations. On the other hand, larger models have more parameters that may leadto overfitting. Supervised and unsupervised encoder-decoder models have recently gainedattention for segmentation tasks [146, 147]. The encoder-decoder model captures high-levelsemantics by reducing the spatial dimension using an encoding module and reconstructsthe inputs using an encoder. Using skip connections [148] results in more accurate re-constructed models. Due to computational constraints for training semantic segmentationmodels, higher strides [149] is used, which leads to lower accuracies.The transformer models [150] have gained popularity due to their performance on var-ious sequence-based tasks, such as large language models (LLM). Such breakthroughs intransformer-based models for natural language processing (NLP) sparked attention in thecomputer vision domain to perform vision tasks. As a result, transformer-based modelshave been developed for classification [142], object detection [151], and segmentation [152].This dissertation introduces a new semi-supervised framework named R-SAM, specificallydesigned for high-resolution segmentation. R-SAM utilizes the Segment Anything Model(SAM) [153] model, as well as a novel multi-level refinement module and a transformer-basedobject detection model for labeling the segmentation masks, which significantly improvedthe segmentation accuracy of SAM.2.2 Cyber-Physical SystemsIn the past, evaluating infrastructure was usually considered a time-consuming and laboriousprocess [27, 130]. Despite the advancements in innovation and technology, the predominantapproach in infrastructure evaluation relies on human operators employing conventionalmethodologies for data acquisition [27\u201329]. In addition, most conventional tools have asingle sensor on the system, constraining the overall hardware and software capabilities.Nevertheless, it is noteworthy that these single-sensor tools and methodologies demandsubstantial human resources to assess structural integrity. Conversely, state-of-the-art toolsand methods demonstrate enhanced efficiency in utilizing a diverse range of sensors [26, 154].Over the past few years, there has been a growing interest in advancing and applying semi-autonomous and fully autonomous robotic systems for SHM. Various robotic platformsare tailored for specific tasks and depending on the type of infrastructure (e.g., bridge,building, wind turbine, and roads). Such robotic systems include climbing robots with legsor ground robots with wheels [30], unmanned aerial vehicles (UAVs), unmanned groundvehicles (UGVs), unmanned surface vehicles (USVs), and unmanned underwater vehicles(UUVs). Recently, hybrid robotics with multifunctional capabilities, such as UAVs withwall climbing capability, has attracted attention [155].182.2. Cyber-Physical Systems2.2.1 Ground RobotsThe predominant robotic system used for SHM is ground-based robots. For example, arobotic platform called ROCIM [156] specializes in bridge deck inspection. RABIT [30],a ground robot with wheels, featured several sensors, such as impact echo and GPR, andautonomous navigation capabilities, which make it an ideal option for surface defect detec-tion. ARA Lab Robot was developed to perform bridge deck inspections [28]. Charron etal. [157] used ground robots to inspect bridges. By utilizing data-driven methods and suchan array of sensors, La et al. [27] developed a semi-autonomous robotic system for surfacecrack detection, rebar exposure, and signs of deterioration in concrete components. An-other type of ground robot was developed with climbing capabilities, utilized for inspectionof unreachable regions of structures, such as under bridge decks [158], bridge cables [31],or vertical surfaces. Among these robotics systems are the vortex climbing and magneticwheeled robot [159]. Due to the nature of the applications, the climbing robots are generallysmall, and the inspections are limited to vision-based methods similar to the UAVs.Nasimi et al. [160] Ojha et al. [161]Clearpath [162] Phillips et al. [163] Halder et al. [164]Figure 2.6: Examples of ground robots utilized by researchers for SHM.192.2. Cyber-Physical Systems2.2.2 Aerial RobotsDeep learning networks facilitated the damage identification task by automating the processand achieving acceptable levels of accuracy. However, in some cases, inspectors can onlyaccess parts of structures to acquire image data (for vision-based approaches) or sensordata (for vibration-based methods). This is one of the main difficulties in structures, suchas tall buildings, bridges, and heritage structures [32]. Drones were proposed as tools forinspecting such structures to overcome these difficulties [33, 165, 166]. Drones, UnmannedAerial Vehicles (UAVs), or Unmanned Aerial Systems (UASs) are classified based on theirautomaticity, size, and other capabilities. They minimize the need for physical labor and aretime-saving, cost-effective, safe, available, and accurate. In recent years, different studieshave been conducted to provide a framework for using UAVs, showing their applicabilityand addressing some of their disadvantages. When considering the rapid developments indrone industries, nowadays, utilizing intelligent UAVs for wireless data acquisition is notconsidered a future technology [34].The leading developers of UAVs for bridge and other structural inspections are thedepartments of transportation (DOT) and the universities in the USA [55, 167\u2013170]. Alongwith the developments in wireless data transmission techniques, several studies have beenconducted that utilized UASs technologies to broaden vision-based inspection in SHM [171\u2013173], as well as vibration-based techniques [174].Since DL-based SHM is an emerging technology, the capability of UAVs for robust real-time evaluation is still a challenge. Kim and Cho [99] demonstrated the feasibility of UAV-based inspection of a concrete retaining wall. They analyzed videos (two frames\/second)recorded approximately 2m from the concrete surface. By embedding image processing andcomputer vision systems in UAVs, instant crack detection tasks could be safely done withminimum costs and maximum accuracy [175]. For example, Jang et al. [176] mounted ahybrid image scanning system (HIS) combining the laser thermography cameras and visionsensors to detect concrete cracks.For bridge inspections, Kim and Cho [177] deployed a high-resolution camera on acommercial UAV to collect images for crack detection and generating damage maps of aconcrete bridge. Kang and Cha [171, 178] developed an autonomous UAV system for SHMwhile using ultrasonic beacons to replace the role of GPS that performs poorly in partiallycovered places, such as under bridge decks. In addition, Huynh et al. [179] used a UAV forthe quasi-real-time inspection of connection bolts on a full-scale girder bridge. Dorafshan etal. [44] examined the performance of different UASs for detecting cracks in steel bridges andconcluded that instability in GPS-denied and windy environments might pose significantchallenges for UAS-assisted inspections.202.2. Cyber-Physical SystemsSaleem et al. [180] Nasimi et al. [181]Figure 2.7: Examples of aerial robots utilized by researchers for SHM.UAVs were also employed to create image datasets for masonry heritage structures [182].The images collected by UAVs are sometimes noisy or have relatively low contrast; in addi-tion, the unavailability of GPS signals in indoor environments or under bridges interruptstheir performance [183]. Duarte et al. [184] discussed the performance of the networkswhen considering multi-resolution images derived from satellite, manned, and unmannedaerial vehicles. Hoskerre et al. [117] proposed a framework to convert UAV data to DL-basedcondition-aware models for automating and accelerating post-earthquake inspections. Theytrained three networks for building information, the presence of damage, and the types ofdamage. Moreover, they suggested an approach for modal identification of the structuresusing videos recorded from different parts of a structure through a divide and conquer strat-egy [185]. Because of the growing attention to smartphone applications, several studies usedthem as inexpensive tools for SHM [74]. Images taken by smartphones have been utilizedfor the identification of various damages, such as pavements [186], bolt loosening [179, 187],volumetric damages [188], and concrete cracks. Zhao et al. [189] developed a mobile-basedmethod for measuring the forces in cables of cable-stayed bridges.Using the framework of Core ML as well as the Xcode, Li, and Zhao [190, 191] integrateda trained CNN model into a developed smartphone application to detect the presence ofconcrete surface cracks on a bridge with 99% accuracy. Furthermore, using CNN, Wanget al. [192] developed a real-time efflorescence and spalling detection of historic brick ma-sonry buildings. Based on 99 \u00d7 99 RGB images that were acquired by low-cost smartphonesensors, Zhang et al. [22] developed a deep CNN model with six convolution layers for au-tomatic crack detection on road surfaces, which was a binary classification task. Pauly etal. [193] proposed a deeper CNN model to enhance the performance of CNNs based onthe 99 \u00d7 99 RGB images. Maeda et al. [194] prepared a large-scale dataset while using asmartphone that was installed on a car dashboard, which used images to develop an end-to-end public application so-called \u2018RoadDamageDetector,\u2019 which classifies different typesof road damages.212.2. Cyber-Physical SystemsTable 2.1 provides an overview of key characteristics of selected robotic platforms used inSHM by researchers, along with their corresponding sensor types. These modalities includeRADAR, LiDAR, RGB, RGB-D, time-of-flight, and thermal sensors.Table 2.1: Robotic Platforms, Type, and Sensors [30].Robotic Platforms Type SensorsRABIT [195, 196] UGV GPR Array, RGB, IEROCIM [156, 197] UGV RGBARA Lab Robot [28, 198] UGV GPR Array, RGB, 2 ER ArraysETH Zurich Climbing Robot [158]Climbing Potential MapperBridgeBot [199] Climbing 1 RGBSteel Bridge Climbing Robot[200] Climbing 2 RGB, Eddy Current SensorCaterpillar robot [201] ClimbingSkySweeper [202] ClimbingCable Robot [203] Sliding 4 CCDCable Inspector [31] Climbing 3 RGBCCRobot-II [204] ClimbingMRC2 IN-II [205] Sliding RGBQuadrotor platform [206] UAVManipulator robot [207] UAVContact prism robot [208] UAVFlying\/walking platform [155] HybridOcto-rotor platform [209] UAVQuadrotor platform [209] UAV RGBHammering platform [210] UAV IE3D Mapper Robot [211] UAV 3D LIDAR, RGBDJI\u00aePhantom [212] UAV RGBOmni-Wheel Robot [213] UAV 2 RGBInfrared Imagery UAV [214] UAV IR , RGBUnderwater ROV [215] USV RGBSea-RAI [216] USV 4 RGBMuddy Waters [217] USV Stereo RGB-D,SonarVideoray ROV [216] USV RGB, Sonar2.2.3 Multi-agent Cyber-physical SystemsRecent breakthroughs in artificial intelligence (AI) for computer vision, as well as improve-ments in hardware and computational capabilities, have opened up the possibility of devel-oping fully automated cyber-physical systems (CPS) for accurate and efficient SHM [218].The recent shift in AI-based applications towards edge computing has enabled data pro-cessing closer to the source, reducing the need for costly data transmission and extendedoperations to remote regions. Autonomous unmanned aerial vehicles (UAVs) and unmannedground vehicles (UGVs) have been explored for a range of applications, such as construc-tion [219], transportation [220], mapping [221], and SHM [222]. The majority of SHMstudies using UAVs focused on map-based applications, where global positioning system(GPS) or building information modeling (BIM) is available [223]. However, GPS may notbe available in indoor environments. In addition, in post-disaster inspections or new con-222.2. Cyber-Physical Systemsstruction, where the map is unavailable, the traditional SHM using UAV may need to befixed.Goal  Figure 2.8: Heterogeneous UAV-UGV system in a grid world with obstacles. The UAV canfly over the obstacles to reach the goal directly, while the UGV needs to circumnavigate theobstacles.Previous studies have attempted to design autonomous systems utilizing single-agentCPS for data collection and damage detection [130]. However, these approaches have pri-marily been conducted in a controlled environment and may need to be more efficient whenfaced with obstacles or occluded objects. UAVs and UGVs have different strengths and lim-itations. For example, UAVs have limitations on the payload and flight time, while UGVshave limitations in speed and crossing obstacles.Table 2.2: Comparison of UAVs, UGVs, and collaborative UAV-UGV.Aspect UAVUGVUAV-UGVProsAbility to quickly cover large areas X XReduced risks for human inspectors X X XCan navigate narrow or confined spaces X XCan be equipped with a variety sensors X XCan operate for longer periods of time X XMore cost-effective for certain tasks XCan potentially carry heavier payloads X XConsLimited payload capacity X XLimited flight time X XHigh cost of equipment and trained personnel X XNot suitable for narrow pathways, pipes, etc. X XLimited ability to pass rough terrain or climb stairs X XLimited ability to capture images from different angles X XLimited ability to travel long distances X XMore susceptible to interference from obstacles X232.2. Cyber-Physical SystemsAn integrated multi-agent CPS (i.e., collaborative UAV-UGV system) can overcome thelimitations of single-agent CPS and provide an efficient and robust solution for SHM [224].Table 2.2 includes a list of the advantages and drawbacks of each system. For instance,the UGVs can serve as portable charging stations for UAVs and carry high-performanceprocessing units, which may relieve the payload demand for the UAVs for real-time decision-making. Collaborative robots have been investigated in different fields, such as warehouseoperations, transportation, and surveillance [225, 226]. In most previous studies, fullyautonomous navigation is not considered, where the robots are either controlled by anoperator [227, 228] or navigated through pre-defined paths [229, 230].Figure 2.9 presents examples of UAV-UGV collaborative systems that have gained sig-nificant attention recently. Yu et al. [231] proposed algorithms for routing UAVs using amobile charging station (Figure 2.9(a)). In a study by Palafox et al. [232], autonomoustake-off and landing were investigated for cooperative UAV-UGV systems (Figure 2.9(b)).Cantieri et al. [233] proposed a cooperative UAV\u2013UGV system for the inspection of powerpylons autonomously (Figure 2.9(c)). The leader-follower concept was used by Bacheti etal. [234] for delivering packages (Figure 2.9(d)). The idea of integrated UGV-UAV was usedby Asadi et al. [219] for data collection purposes in construction sites, however, they useda blimp as the aerial vehicle (Figure 2.9(e)).(a) (b) (c)(d) (e)Figure 2.9: Examples of cooperative UAV-UGV systems [219, 231\u2013234].242.3. Simultaneous Localization and Mapping2.3 Simultaneous Localization and MappingNumerous studies have aimed to enhance the autonomous exploration capabilities of robotswithin an unknown environment [235] by improving estimation accuracy and reducing explo-ration errors. Various approaches have been proposed to address challenges in autonomousrobotics, with most techniques focused on mapping and localization. The introductionof Simultaneous Localization and Mapping (SLAM) has since become the most widelyacknowledged fundamental approach. In the following section, the most popular SLAMalgorithms are briefly discussed.2.3.1 LiDAR-based SLAMGmapping [236] and TinySLAM [237] are among the SLAM methods that utilize particlefilers for the robot\u2019s trajectory. In the case of having a loop closure, the particle filterconverges effectively. GMapping is widely used in robotics applications, and by default,ROS uses GMapping to generate the 2D occupancy grid map by utilizing 360-degree LiDARsensors. The generated map can be used for autonomous path planning and navigation.Hector SLAM [238] is another 2D SLAM algorithm that optimizes the computationalcost and is robust to drift in localization. Hector SLAM can efficiently utilize external IMUsensors for more accurate mapping in a 3D environment. The drawback of Hector SLAM isthat it lacks loop closure detection, and therefore, the map cannot be corrected when therobot scans the previously visited locations. This problem may affect the performance ofthis algorithm for environments with fewer geometry constraints, with many robot turns,and with limited sensor range. A similar approach is used by ETH Zurich AutonomousSystems Lab (ETHZASL)-ICP-Mappe [239], which is prone to errors for more extendedoperations.Graph-based SLAM algorithms, such as Karto SLAM [240], Lago SLAM [241], GoogleCartographer [242], and Berkeley localization and mapping (BLAM) [243], stores linkedsub-maps. Among these approaches, Google Cartographer can utilize 3D LiDAR sensors togenerate 3D point clouds. In contrast to Hector SLAM, these methods can update the mapafter a loop closure. In addition, they can be paired with external odometry to improveaccuracy and robustness. BLAM, available only for 3D LiDAR mapping, optimizes the mapby utilizing Georgia tech smoothing and mapping [244] whenever a loop closure is detected.The main disadvantage of BLAM is its limitation in detecting larger loops, as it relies onlocal scan matching. SegMatch [245] is another graph-based SLAM algorithm capable ofmatching 3D point clouds, and it can be done in multiple sessions using a single robot orby utilizing multiple robots [246].2.3.2 Visual SLAMSeveral visual SLAM algorithms have been developed recently [247]. However, few are suit-able alternatives for robotic applications [248]. Significant challenges with visual SLAMmethods (e.g., PTAM [249], SVO [250], LSD-SLAM [251], and ORB-SLAM [252]). Ad-vanced visual SLAM algorithms have recently been developed to eliminate scale drift while252.3. Simultaneous Localization and Mappingmapping. The VINS-Mono [253] and The maplab [254] are graph-based SLAM algorithmsthat utilize IMU measurements and a camera to generate a map. Unlike maplab, VINS-Mono processes maps in real time. Among the stereo camera-based visual slam algorithms,they perform well using RGB-D data.An open-source library known as Real-time Appearance-based Mapping (RTAB-Map),developed by Labbe\u00b4 and Michaud [247, 255], employs a memory management strategyfor loop closure detection. This approach ensures that the map size remains constrained.Consequently, loop closure detections are consistently processed within a predeterminedtime frame for online prolonged and large-scale environment mapping. The block diagramof the ROS node for RTAB-MAP is shown in Figure 2.10.Figure 2.10: Block diagram of RTAB-MAP ROS node [247]Model NameInputs LiDAR OutputsStereo RGB-D IMU 2D 3D OccupancyGMapping [236] X 2DTinySLAM [237] X 2DHector SLAM [238] X 2DETHZASL-ICP [239] X X 2DKarto SLAM [240] X 2DLago SLAM [241] X 2DCartographer [242] X X 2DBLAM [243] X 3DSegMatch [245] X 3DVINS-Mono [253] RGB X Pose onlyRTAB-Map [255] X X X X X 2D\/3DTable 2.3: Popular SLAM algorithms available in ROS262.4. Large Language Models for Robotics2.4 Large Language Models for RoboticsRecently, the development and deployment of artificial intelligence (AI)-based applicationshave undergone a paradigm shift driven by edge and cloud computing advances. The newapproaches of using highly efficient edge computers and accessible could-based AI modelshave reduced the reliance on expensive and time-consuming robotic-based inspections forinfrastructures [161, 256, 257]. Despite the rigorous efforts and successful implementationof AI models in structural health monitoring (SHM) applications, they are limited to single,structured, well-defined tasks. Thus, they may perform poorly in a dynamic environmentor for zero-shot tasks [130, 258, 259].Since the introduction of transformers [150], the exponential developments in the field ofnatural language processing (NLP) have recently shifted toward developing language models(LMs), such as Bidirectional Encoder Representations from Transformers (BERT) [260],Generative Pre-trained Transformer (GPT) [261], OpenAI Codex (Codex) [262], and themost recently Llama-2 [263]. Successful applications of such models in various research andapplied fields and the remarkable results indicate their revolutionizing and promising rolein engineering.Most previous LLMs were based on a single prompt or input [264]. Recently, OpenAIintruded the pre-trained model ChatGPT [265], significantly different from the previousmodels. One of the particular features of ChatGPT is its ability to interact through adialog and remember previous conversations. This feature is ideal for multimodal applica-tions, such as robotics, that require controlling physical components and utilizing softwarecomponents for real-time decision-making and action, where LLMs can combine the outputtext with code synthesis.In contrast to text-only applications, using LLMs for robotics systems demands a thor-ough comprehension of the environment\u2019s physics and the operating software for takingactual physical actions. Therefore, a robust generative robotics system for engineering ap-plications must have a deep understating of the environment and the ability to efficientlyinteract with users to receive prompts, process, interpret, and execute reasonable actions.This dissertation seeks to build upon earlier studies and enhance human-machine in-teraction with the ability to engage in human-level natural dialogs, allowing flexibility andcorrection in different tasks. Due to diverse applications in robotics, the current researchfocused on unlocking the capabilities of LLMs to solve SHM problems using robotics. There-fore, a library of high-level functions was created for ChatGPT to utilize based on the re-ceived prompts. Such a library can be linked to certain APIs from appropriate platformsin the backend to perform specific low-level tasks. With this approach, it is possible touse LLMs to parse users\u2019 input from human-level natural language and convert the promptinto callable functions. Furthermore, this study presents a pipeline to design the library ofhigh-level functions to assist LLMs in understanding the problem and achieving higher per-formance. Therefore, ChatGPT can perform a range of tasks without explicit programmingor the need for training on specific tasks, which is also adaptable for a broader researchfield.The key contributions of this study are (a) introducing a general pipeline for prompt272.4. Large Language Models for Roboticsengineering and creating a high-level function library for using ChatGPT for robotics, (b)evaluating the proposed pipeline through experimental studies using a costume-built groundrobot, and (c) providing a pipeline for prompt engineering for generative robotics for deeplearning-based SHM applications. Therefore, this study aims to inspire ongoing and futureresearch and introduce new opportunities for integrating LLMs with cyber-physical systemsfor solving engineering problems using the concept of \u201cend-user in the loop\u201d instead of\u201cexpert in the loop\u201d (Figure 2.11).  Deploy and Validate - Scan the surface for cracks - Go 2m closer and take RGBD images Prompt, API, \u2026 LLM Robot User Python (b) End-user in the loop Deploy and Validate Robot Python (a) Expert in the loop  Expert Figure 2.11: Expert in the loop vs. user in the loop.28Chapter 3Transformer-based SegmentationFramework for High-ResolutionImages3.1 OverviewThe utilization of transformers in the computer vision domain has the potential to bridgethe gap between language processing and visual reasoning. The foundation of transformermodels is the self-attention mechanism trained to comprehend the interdependencies amongsequences. From this point of view, transformers are similar to recurrent neural networks(RNNs) [266]. RNNs can capture short-term context using the recursive procedure. How-ever, transformers can learn long-range relationships between sequences through the atten-tion mechanism. Though such a mechanism has been used in RNNs [267, 268], transformersrely on the unique implementation of a multi-head attention mechanism that allows fasterand parallel computation; this allows transformers to be scalable to complex models andoutperform when dealing with larger datasets [132, 269]. The results of this research havebeen accepted for publication in Computer-Aided Civil and Infrastructure Engineering [270].3.2 Vision TransformersThe Vision Transformer (ViT) concept was initially presented by [142]; it can potentiallyreplace standard CNNs for large datasets. ViT was based on the original Transformer [150],where they used sequences of image patches instead of directly processing the pixel values ofthe entire image. It is worth emphasizing that their model was pre-trained and encoded priorknowledge about the images using the JFT-300M dataset [271], which contains 300 millionimages. Pre-training ViT on a smaller dataset would result in lower accuracy. The Data-efficient image Transformers (DeiT) [272] model demonstrated that vision transformers canbe trained faster using mid-range datasets while maintaining high accuracy. The distillationapproach in DeiT uses CNNs (teacher) to a transformer model (student). The standardViT models had a fixed number of tokens through the network, which was a limitation forcapturing spatial information.293.2. Vision Transformers3.2.1 Attention Mechanism in Vision TransformersTransformers are built on two core components: self-attention and pre-training. Utilizingself-attention enables transformer models to capture long-range dependencies among se-quences of features effectively, a challenge that traditional RNNs struggle to overcome.Meanwhile, pre-training involves training a network by leveraging a large, labeled, orunlabeled dataset and then fine-tuning it on the target dataset [260, 273]. The self-attention mechanism aims to capture the relationships among all n entities within a se-quence X \u2208 Rn\u00d7d by encoding them in terms of global information. In other words, itestimates the interactions between the elements in the sequence, such as identifying whichtypes of damage tend to co-occur in a given structure image. The embedding dimension dspecifies the size of the vector space in which the sequence is represented, allowing for morecomplex and informative representations of the data. To accomplish this, the input sequenceX = (x1, x2, . . . , xn) is projected onto a triplet of learnable matrices (i.e., weights), QueriesWQ \u2208 Rdmodel\u00d7dk , Keys WK \u2208 Rdmodel\u00d7dk , and Values WV \u2208 Rd\u00d7dv . These matrices arethen fed into a normalized dot-product attention mechanism, as described by [274]:Z = Softmax(QKT\u221adk)V (3.1)where, Q = XWQ, K = XWK , V = XWV , and Z \u2208 Rn\u00d7dv .A multi-head self-attention mechanism (see Figure 3.1) utilizes h parallel attention lay-ers, or heads, to project the input sequence X onto representation subspaces, each with itslearnable query, key, and value matrices {WQi ,WKi ,WVi}hi=1 (e.g., h = 8).303.3. Proposed MethodSoftmaxScaleMatMulMatMuliViQ iKLinearConcatenatehQZiScaled Dot-Product AttentionLinear Linear LinearK VXWQ WVWK Figure 3.1: Parallel attention mechanism (adapted from [150])This approach is used to overcome the limitations of the standard attention mechanismin capturing multiple important relationships simultaneously. To elaborate further, when aparticular input is considered:Z = concat(Z0,Z1, . . . ,Zh\u22121)Wo (2)where, Wo is the additional projection weight and Wo \u2208 Rhdv\u00d7dmodel ( [132]). The \u2018linear\u2019block in the figure refers to a linear transformation applied to project the feature map intodifferent subspaces [150]. The term \u2018MatMul\u2019 refers to matrix multiplication, and beforethe softmax function, the attention values are scaled by the square root of the total numberof keys ( 1\u221adk).3.3 Proposed Method3.3.1 Overview and AbbreviationsFigure 3.2 provides a visual representation and an overview of the proposed pipeline, whichconsists of three main modules: Segment Anything Model (SAM), Refine Anything Model(RAM), and DEtection Transformer (DETR). Depending on color and texture, the inputRGB images are fed into the SAM module [153] to segment all the components, whetherstructural or non-structural. The RAM module is used to enhance the performance of SAMby refining the edges of the generated masks. The refined segmentation masks are finally313.3. Proposed Methodlabeled using the DETR, an object detection model based on transformers. The pipelineoutput is a panoptic segmentation, where structural components or damage are detectedand segmented. Each of the abovementioned components is explained in the followingsections.                 Panoptic Segmentation RGB SAM \u20db \u20db Column \u20db Bridge R-SAM RAM DETR Figure 3.2: The overview of the proposed R-SAM pipeline.3.3.2 Segment Anything Model (SAM)In this study, the Segment Anything Model (SAM) [153] is used to generate the initial seg-mentation for R-SAM. As shown in Figure 3.3, the original SAM has three main modules:the image and prompt encoder modules, which take input RGB images, as well as promptsand output encoded embeddings, and the mask generator. For scalability, a pre-trainedvision transformer adapts the model for high-resolution images. The image encoder\u2019s out-put, a downscaled embedding, is a 256\u00d7 64\u00d7 64 feature matrix fed into the mask decoderhead that outputs the final masks. While the main intent of the SAM is to be used forprompt-based segmentation [153], and it is mentioned that the SAM accepts masks, points,and text prompts, the text prompting has not yet been released with the model. In thisstudy, the pre-trained SAM is implemented, and the combination of Dice loss [275] andFocal loss [276] functions were used in training the original SAM (L = LDice + 20LFocal).Additional information about SAM, including the pre-trained models, can be found in theoriginal paper [153].                 Image encoder Conv Mask decoder  Prompt encoder Image embeddings Image text box points mask Valid masks , IoU score , IoU score , IoU score Figure 3.3: The overview of the Segment Anything Model (SAM) (based on [153]).323.3. Proposed Method3.3.3 Refine Anything Model (RAM)In this section, the proposed Refine Anything Model (RAM) module of R-SAM is explainedin detail, inspired by the models using cascade features model [277, 278]. This researchuses a stack of multiple RAM modules to achieve high-resolution segmentation results.The refinement module is based on a PSP-Net [144, 278] architecture with three keycomponents: 1) the ResNet34 as the backend pre-trained model for feature extraction, 2)the pyramid pooling module (PPM) as the mask decoder, and 3) the upsampling module.As depicted in Figure 3.4, the RAM module receives a batch of input RGB images alongwith the multi-resolution segmentation masks (i.e., with strides of 1, 2, 4, and 8) and passesthem through the backend [279] to extract the feature map. Then, the acquired featuremap is passed through the PPM to perform adaptive average pooling at varying scales,resulting in an output that captures a representation of the input at multiple scales [144].A 1\u00d71 convolution layer, with ReLU activation [280], takes the concatenated outcomes todecrease the depth of feature map [278].   CONVCONVCONVCONVPOOL CONVBackendUPUPFeature MapPyramid Pooling ModuleSkip Connections UPOutputsInputs            Figure 3.4: The proposed Refine Anything Model (RAM) module.Using the feature maps derived from the preceding layer, the upsampling module per-forms its operations by leveraging bilinear interpolation and incorporating concatenationwith the feature maps from the skip connections from the backend. The obtained featuremaps are passed through a sequence of convolution layers, with a sigmoid activation func-tion, to generate the segmentation mask. The outputs are upscaled to create the mask forthe next iteration. The skip connections are incorporated into the design to let the networklearn mask features at the pixel level by retaining the lost information from different steps.The multi-resolution segmentation mask generation helps to fix the overall structure andboundary of the imperfect segmentation masks [278].The cross-entropy (CE) loss function is employed for the coarser stride eight outputto detect the global features. However, for finer stride 1, the L1+L2 loss functions pushthe model to pay attention to the local pixel-level features. These loss functions are pro-vided by the PyTorch library [281]. The intermediate strides 2 and 4, the average of the333.3. Proposed Methodaforementioned losses is used.L8 = LCE (3.2)L1 = LL1+L2 + \u03b2Lgrad (3.3)L2,4 = 12(LL1+L2 + LCE) (3.4)where Lgrad is the gradient loss that is used for the finer stride 1, which is defined asfollows [278]:Lgrad = 1n\u2211i\u2016\u2207(fm(xi))\u2212\u2207(fm(yi))\u20161 (3.5)where \u2207 denotes the gradient or edge detector operator, and fm(.) is the mean filter,and [282]. The overall loss function is defined as L = L1 + L2 + L4 + L8. MultipleRAM modules are recursively used to implement the trained model for high-resolutionsegmentation of imperfect input segmentation and to capture pixel-level details of structuralcomponents or damages accurately. The input segmentation is replicated, resized, andpassed through the refinement model to maintain consistency in the RAM module. Eachiteration replaces the one output mask as illustrated in Figure 3.5. In this study, the RAMmodule consists of five iterations that could be reduced depending on the complexity of theproblem. This particular approach has been previously employed by [278] to achieve fasterrefinement at the cost of reduced accuracy. Input RGB & mask RAM RAM RAM RAM RAM Level 1 outputs Level 2 outputs Level 3 outputs Level 4 outputs Level 5 outputs Final output Figure 3.5: Implementation pipeline of the RAM module in this study.343.3. Proposed Method3.3.4 DEtection TRansformer (DETR)This section provides an elaborate explanation of the detection module of R-SAM, whichis inspired by the research projects that used the advantages of transformers in computervision tasks [132, 142, 151]. Transformers are used for end-to-end detection tasks: CNNbackbone with a transformer head, transformer backbone with an R-CNN head, and puretransformer. DETR, the End-to-End Object Detection with Transformers [151], developedby Facebook AI Research (FAIR), uses the first approach to predict bounding boxes aroundobjects. This way, it is possible to predict multiple objects from a single shot and theirrelationships by performing bipartite matching. One key feature of DETR is that it doesnot have the hand-designed components to encode prior knowledge, such as region-proposal-network (RPN), the main module of modern object detection methods. This allows DETRto perform object detection tasks without knowledge of complex detection tasks.The detailed architecture of the DETR-based object detection model is given in Fig-ure 3.6 and Figure 3.7. The CNN backbone of the DETR model creates spatial featuremaps that are flattened to create a set of features (f \u2208 R(C\u00d7H\u00d7W )). During this step, everyinput image is first zero-padded to match the shape of the largest image in the dataset. Aconvolution layer is utilized to decrease the channel dimension of the input spatial featuremap, transforming it into a one-dimensional sequence of features. The transformer\u2019s en-coder and decoder modules incorporate multi-head attention and a feed-forward network(FFN). Unlike the previous transformers [150], DETR can decode multiple objects in par-allel. The final block of the model has multi-layer FFN with ReLU and Softmax activationfunctions to predict the bounding boxes and the labels. Transformer encoder \u2026 CNN Positional encoding \u2026 Transformer decoder FFN class, bbox FFN no object FFN class, bbox FFN no object Figure 3.6: The high-level architecture of DETR (adapted from [151]).In Figure 3.7, \u2019Object Queries\u2019 are learnable vectors that are passed through the at-tention mechanism to attend to specific locations (of objects of interest). Therefore, anobject query in DETR represents a potential object in the image, and the model is trainedto associate the object queries with the corresponding bounding box. Positional encodingis used in transformer-based models to embed information regarding the position of theelements within input sequences. The key reasons for incorporating positional encodingare: a) transformers, unlike RNNs, are permutation-invariant and they cannot understandthe sequential order of the inputs inherently, and it helps to consider the order of elements;b) To capture spatial relationships, such as proximity, distance, and orientation, which iscrucial for object detection; and c) without using positional encoding, a transformer-basedmodel may treat two similar inputs (i.e., symmetry) at the different locations as equivalent.353.3. Proposed MethodFigure 3.7: Detailed architecture of the DETR model [151].In the context of transformer-based detection models, the term \u2018Add & Norm\u201d refersto addition, where the output of the multi-head self-attention mechanism and the skipconnection from the input are added element-wise, and layer normalization, where thevalues across the output are normalized. The former helps the model preserve and retaininformation from the original input, and the latter helps in stabilizing the learning process.The DETR generates predictions per iteration and achieves optimal bipartite matchingby associating the predicted bounding boxes with the corresponding ground truth boundingboxes. For N predictions per a single pass, y\u02c6 = {y\u02c6i}Ni=1, since the number of objects thatcan potentially exist in an input image is smaller than N, several extra padded \u2018no object\u2019 isconsidered in the input. To get the optimal bipartite matching, the total pair-wise matchingcosts, Lmatch(yi, y\u02c6\u03c3(i)), for a permutation of N elements is minimized as follows [151]:\u03c3\u02c6 = arg min\u03c3\u2208SNN\u2211i=1Lmatch(yi, y\u02c6\u03c3(i)) (3.6)where, yi = (ci, bboxi) is the ground truth, and the predictions are denoted as y\u02c6\u03c3(i). cirepresents the target class, including the no-object class, denoted as \u2205, and bboxi repre-sents the bounding box. Therefore, the matching cost includes both class predictions andbounding boxes. Lmatch is defined as follows:363.3. Proposed MethodLmatch(yi, y\u02c6\u03c3(i)) = \u22121ci 6=\u2205p\u02c6\u03c3(i)(ci) + 1ci 6=\u2205Lbox(bi, b\u02c6\u03c3(i)) (3.7)where, p\u02c6\u03c3(i)(ci) and b\u02c6\u03c3(i) are the probability of class ci with the predicted bounding boxof b\u02c6\u03c3(i). The key distinction of DETR from other models is its ability to enable direct setprediction. The Hungarian algorithm [283] is used to calculate the loss.LHungarian(y, y\u02c6) =N\u2211i=1[\u2212 log p\u02c6\u03c3(\u02c6i)(ci) + 1ci 6=\u2205Lbox(bi, b\u02c6\u03c3(\u02c6i))](3.8)where, p\u02c6\u03c3(\u02c6i) is the probability of the optimal assignment, Eqn. (9), and Lbox(\u00b7) the boundingbox loss that is calculated by linearly combining two losses, the generalized Intersection-over-Union (IoU) loss and the `1 loss.Lbox(bi, b\u02c6\u03c3(i)) = \u03bbiouLiou(bi, b\u02c6\u03c3(i)) + \u03bbL1\u2016bi \u2212 b\u02c6\u03c3(i)\u20161 (3.9)The generalized IoU loss can be calculated as follows:Liou(bi, b\u02c6\u03c3(i)) = 1\u2212(|b\u03c3(i) \u2229 b\u02c6i||b\u03c3(i) \u222a b\u02c6i|\u2212 |B(b\u03c3(i), b\u02c6i)\\(b\u03c3(i) \u222a b\u02c6i)||B(b\u03c3(i), b\u02c6i)|)(3.10)where, | \u00b7 | denotes the area, and B(b\u03c3(i), b\u02c6i) is the largest box that has b\u03c3(i) and b\u02c6i.Figure 3.8 illustrates the implementation pipeline of the object detection module of R-SAM. For instance, in this Figure, the trained DETR-based model detects the componentbounding box, (label, bbox)i, with it matched with one of the generated masks by the SAMmodule, (mask, bbox)j , which has the maximum IoU. The matched outputs of both modulesare fused to generate the final output mask and the label.  \u22ee Max(IoU) (mask, bbox)i (label, bbox)j SAM DETR IoU Input Figure 3.8: Implementation pipeline of the object detection.373.4. Results3.4 ResultsAll three modules of the proposed R-SAM were developed or implemented utilizing thePyTorch framework. Details about the training of the refinement and object detectionmodels are provided below in this section.3.4.1 Training Refinement ModelThe RAM module was trained using a diverse segmentation dataset that includes Crack-Forest [284], Structural Material Semantic Segmentation Dataset [285], and ECSSD [286],MSRA Salient Object Database [287], FSS-1000 [288], and DUT-OMRON pixel-wise dataset [289].To ensure that the RAM module can be applied to various scenarios, existing pre-trainedmodels, such as SAM, were not used to generate additional segmentation masks. Dataaugmentation techniques, including image flipping and random cropping, were consideredin this study to add variety. Furthermore, the RAM module was trained using perturbedground truth, as suggested by [278], to improve the robustness of the R-SAM. Adam opti-mizer [290] was used with a total of 10,000 iterations and batch size of 8.Figure 3.9 shows the segmentation improvement using the 5-level refinement. Figure3.10 shows the performance of the trained RAM module for different image segmentationtasks. The first case, Figure 3.10-(a), is segmentation mask improvement that is applied tothe upsampled output of a pre-trained Mask R-CNN [291]. This application indicates thepotential of the proposed RAM module in the segmentation of high-resolution images usingpre-trained light models that output down-sampled masks.  Input RGB Input mask Output mask Level 1 Level 2 Level 3 Level 4 Level 5 Figure 3.9: Multi-level refinement.The second case, Figure 3.10-(b), shows the segmentation refinement for a noisy maskgenerated using a LiDAR camera by thresholding the distance from the camera position.Since this case does not require pre-training of a segmentation model, it is an ideal optionfor segmenting 3D structural components from the background. Finally, Figure 3.10-(c)shows a multi-class segmentation refinement that can be achieved through the fusion of383.4. Resultsmultiple refined masks, which highlights the potential of the proposed RAM module for thecomponent and damage-level segmentation tasks for damage assessment. Input images  Input masks Output masks (a) (b) (c) \u25a0 Component \u25a0 Damage Figure 3.10: RAM module performance on different imperfect input masks.3.4.2 Training Object Detection ModelThis study trained the object detection model to generate the bounding boxes for eachobject detection task (e.g., crack detection, component detection, and damage detection).The image dataset for structural components and damage detection of concrete columnswere created using experimental reinforced column tests that have been done at the Struc-tural Laboratory at The University of British Columbia (UBC). The images were taken indifferent locations and lighting conditions, and additional augmentation techniques such asrandom crops, resizing, rotating, blurring, and added noise during the training were used togeneralize the trained model. 250 images were annotated with bounding boxes around theconcrete column and the damaged regions. The dataset was split into training, validation,and testing subsets with 70:15:15 ratios. The training was done with the following keyhyperparameters: learning rate=0.00005, batch size = 8, weight decay = 0.0001, epochs =393.5. Discussions50, backbone=ResNet34, position embedding = sine, encoder layers = 6, decoder layers =6, hidden layer dimension = 256, dropout = 0.2, number of queries = 5. The test resultsare shown in Figure 3.11 for a batch of images from the test subset. This figure shows thatthe model can effectively detect the object of interest (i.e., component) and the regions ofinterest for damage detection. These bounding boxes are used in R-SAM to identify thesegmentation region accurately. Figure 3.11: Visualization of object detection results on the dataset. Detected objects arehighlighted with bounding boxes and confidence scores.3.5 Discussions3.5.1 Evaluation MetricsTwo accuracy metrics were employed to evaluate the performance of the segmentationmodels: mean Intersection-over-Union (mIoU) score across all classes and mean BoundaryAccuracy (mBA). The mIoU is calculated by dividing the pixel intersection by the pixelunion,mIoU =1ncnc\u2211i=1TPiTPi + FPi + FNi(3.11)where nc is the number of classes, TPi, FPi, and FNi are true positive, false positive, andfalse negative for class i, respectively.To calculate the boundary accuracy of the refined masks, the binary map of boundaryregions for the ground truth masks was generated using the morphological gradient methodimplemented in the OpenCV library [292]. Different disk-shaped structuring elements ofvarying radii (e.g., r = 1, 3, 5, . . .) within the range [1, max(h,w)200 ] (Figure 3.12). The meanbounding accuracy is calculated as the mean of the ratio of the number of accuratelyidentified boundary pixels to the sum of boundary pixels.mBA =1Mm\u2211j=11Njn\u2211i=1Ii,j (3.12)403.5. Discussions Boundary Region \ud835\udc5f = 3 Boundary Region \ud835\udc5f = 5 Boundary Region \ud835\udc5f = 7 Boundary Region \ud835\udc5f = 1 Ground Truth Image Figure 3.12: The ground truth mask and boundary regions with different radiuses.3.5.2 Comparative Analysis of Segmentation ModelsTwenty-seven segmentation models were developed and trained with different configura-tions to investigate performance improvement using the RAM module. These modelsare based on the U-Net [293], U-Net++ [294], MA-Net [295], LinkNet [296], FPN [297,298], and DeepLabV3+ [143, 299], and with different encoders including EfficientNet [300],ResNet [279], MobileNet [301, 302], MiT [303], and VGG [304]. The results are presentedin TABLE 1 for mean IoU (mIoU) and mean BA (mBA) metrics. In addition, the resultswere compared with the same models refined using the proposed RAM module to achievemore accurate segmentation results. The proposed R-SAM model achieved a mean IoU of97.23% with a mean BA of 87.06%, which exhibits superior performance compared to theother models studied in this research. Figure 3.13 shows the visual comparison betweendifferent segmentation models and compares them with the results obtained from SAM andR-SAM. Except for the U-Net and U-Net++ with EfficientNet-b0 encoders, the rest of themodel achieved a high IoU after refinement. However, R-SAM has the highest boundaryaccuracy due to the initial segmentation results with a higher IoU. Considering the segmen-tation task performed by the proposed R-SAM framework is unsupervised and requires onlybounding boxes to train the object detection module, it has promising potential for real-world applications where segmentation data may not be available and rapid assessmentsare required.413.5. DiscussionsTable 3.1: Comparative analysis of various semantic segmentation models with and withoutrefinement.Model Encoder mIoUmIoURAM\u2206IoUmBAmBARAM\u2206BA(%) (%) (%) (%) (%) (%)DeepLabV3+EfficientNet-b0 76.10 92.25 16.15 65.61 80.21 14.61DeepLabV3+ ResNet34 90.92 96.58 5.66 78.27 81.12 2.84DeepLabV3+ ResNet50 88.75 96.18 7.43 76.89 81.04 4.15FPN EfficientNet-b0 68.14 94.83 26.69 57.35 80.63 23.28FPN MobileNetV2 91.64 96.81 5.17 75.80 81.27 5.47FPN ResNet18 94.29 96.84 2.56 77.25 81.30 4.05FPN ResNet34 91.52 96.84 5.32 72.29 81.24 8.95FPN ResNet50 89.78 96.37 6.58 72.14 80.86 8.73LinkNet EfficientNet-b0 71.84 77.95 6.11 71.14 74.64 3.50LinkNet MobileNetV2 90.73 96.65 5.92 80.71 81.13 0.42LinkNet ResNet18 91.63 96.64 5.00 81.55 81.00 -0.55LinkNet ResNet34 92.68 96.79 4.11 83.85 83.26 -0.59MA-Net EfficientNet-b0 78.78 91.51 12.73 76.20 79.95 3.75MA-Net MiT-b0 81.67 96.39 14.72 73.75 81.11 7.36MA-Net MobileNetV2 92.36 96.52 4.16 84.12 82.98 -1.14MA-Net ResNet18 92.56 96.72 4.16 84.81 82.75 -2.07MA-Net ResNet34 88.82 95.75 6.93 79.06 81.17 2.11U-Net EfficientNet-b1 55.21 70.11 14.90 68.88 74.62 5.74U-Net MobileNetV2 86.17 96.83 10.66 82.16 81.25 -0.91U-Net ResNet34 90.86 96.83 5.97 83.11 82.28 -0.83U-Net ResNet50 86.50 95.39 8.89 82.95 82.74 -0.21U-Net VGG16 85.17 96.63 11.47 78.51 81.22 2.72U-Net VGG19 85.28 96.81 11.53 78.99 81.31 2.31U-Net++ EfficientNet-b1 56.64 66.29 9.65 69.48 74.31 4.83U-Net++ MobileNetV2 86.41 96.83 10.41 83.40 83.26 -0.15U-Net++ ResNet34 81.44 96.84 15.41 77.82 81.26 3.44U-Net++ ResNet50 87.41 95.05 7.64 83.35 81.92 -1.43SAM+DETR 91.24 79.61R-SAM 97.23 87.06423.5. Discussions R-SAM  DeepLabV3+ (1)  DeepLabV3+ (2)  DeepLabV3+ (3)  FPN (1)   FPN (2)   FPN (3)   FPN (4)   FPN (5)   LinkNet (1)   LinkNet (2)   LinkNet (3)   LinkNet (4)   MA-Net (1)   MA-Net (2)   MA-Net (3)   MA-Net (4)   MA-Net (5)   U-Net (1)   U-Net (2)   U-Net (3)   U-Net (4)   U-Net (5)   U-Net (6)   U-Net++ (1)   U-Net++ (2)   U-Net++ (3)   U-Net++ (4)   (c) SAM  DeepLabV3+ (1)  DeepLabV3+ (2)  DeepLabV3+ (3)  FPN (1)   FPN (2)   FPN (3)   FPN (4)   FPN (5)   LinkNet (1)   LinkNet (2)   LinkNet (3)   LinkNet (4)   MA-Net (1)   MA-Net (2)   MA-Net (3)   MA-Net (4)   MA-Net (5)   U-Net (1)   U-Net (2)   U-Net (3)   U-Net (4)   U-Net (5)   U-Net (6)   U-Net++ (1)   U-Net++ (2)   U-Net++ (3)   U-Net++ (4)   (b) (a) Figure 3.13: Visual comparison of different segmentation methods: (a) selected image andthe region of interest, (b) before applying refinement, and (c) after applying refinement.433.6. Case Studies3.5.3 Performance of The Refinement Module with Different InputResolutionsAs recommended by [278], to segment ultra-high resolution images without resizing thedimensions, one option is to refine the segmentation using the cropped regions from theoriginal images without allocating more GPU memory. Therefore, square L\u00d7L patches ofthe images are obtained at specific strides, where L=max(height, width), and the obtainedimages are fed into the RAM module. The average of the output masks is used as the finalrefined mask to eliminate the disagreement among the outputs from different overlappingpatches. Figure 3.14 shows the performance of the RAM module with different L values. Alower L produces segmentation with lower accuracies, while a higher L performs better. Fora typical GPU with 8GB of memory, the maximum L value would be 1,300 pixels. It hasbeen observed that a minimum value of L=600 is required to prevent any negative impacton the accuracy improvement.Figure 3.14: Segmentation accuracy improvement using larger crop size.3.6 Case StudiesThree applications for R-SAM are presented in this section, highlighting its versatility andeffectiveness in various aspects of structural health monitoring.3.6.1 Multi-class SegmentationTo demonstrate the effectiveness of R-SAM in the accurate segmentation of multiple classes,the publicly available DTU\u2014Drone inspection images of the wind turbine [305] were usedto train the DETR model. For this purpose, the dataset (5280\u00d72970 images) was annotatedusing a multiclass annotation approach, where each instance was labeled with a bounding443.6. Case Studiesbox. The results of this case study are presented in Figure 3.15, which shows that R-SAMaccurately identifies and segments various components such as blades, nacelles, towers, andpotential damages such as corrosion. Furthermore, compared to the SAM+DETR model,R-SAM significantly improves the refining of the boundaries of each segmented region. Thisenhancement greatly enhances the accuracy of understanding and analyzing the structuralcomponents and damages, thereby significantly strengthening R-SAM\u2019s overall performanceand capabilities within the framework of structural health monitoring applications. Thiscapability holds significant value as it enables timely maintenance interventions and effectivemitigation of potential risks, ensuring the efficient operation of wind energy systems andmaximizing their lifespan. By providing precise and reliable information about the conditionof wind turbines, R-SAM empowers operators and maintenance teams to proactively addressissues, optimize performance, and safeguard the long-term sustainability of these criticalenergy assets. Input (RGB) \u2588 Blade     \u2588 Nacelle     \u2588 Tower     \u2588 Corrosion  CascadePSP (fast) CascadePSP (slow)  SAM-HQ R-SAM MobileSAM SAM Figure 3.15: Multi-class segmentation and damage detection of wind turbine using SAM-based models.453.6. Case Studies3.6.2 Marker-free segmentation and trackingStructural vibration measurement plays a crucial role in structural health monitoring. Re-cently, a growing interest has been in employing cost-effective computer vision approachesto measure structural vibrations [130]. Optical flow-based algorithms have garnered signif-icant attention among the various computer vision algorithms explored for motion detec-tion. One widely used optical flow method is Kanade-Lucas-Tomasi (KLT) [306], which hasdemonstrated high precision in motion tracking, particularly in a controlled environment.However, despite the effectiveness of the KLT method in structural vibration measurements,it requires manual specification of the region of interest (ROI) in the initial video frame.Although recent studies utilized deep learning for marker detection for structural vibrationmeasurement, areas can still be improved to enhance their effectiveness and reliability [307].To exhibit the capability of R-SAM in accurate segmentation and real-time marker-freetracking of structural components, a case study is conducted on publicly available videorecordings [308] from a multi-story frame subjected to dynamic loading on a shake table.The results of this case study are shown in Figure 3.16. R-SAM can significantly refinethe boundaries of the regions of interest (full-frame in this example), which can be usedfor accurate measurement. By feeding the generated initial mask into the next frame\u2019ssegmentation, it is possible to generate a highly accurate segmentation mask for the regionof interest for tracking. Furthermore, R-SAM provides a marker-free approach to trackingthe entire frame using a single binary mask and a multi-class mask for tracking multiplecomponents simultaneously. This approach allows extending the applications of R-SAM tomodel updating and system identification.463.6. Case Studies \ud835\udc61 = \ud835\udc610 Video Frame SAM+DETR R-SAM \ud835\udc61 = \ud835\udc61\ud835\udc5b  Figure 3.16: Segmentation and tracking using R-SAM.473.7. Conclusions3.7 ConclusionsThis chapter proposes a novel deep learning-based framework called R-SAM to address theproblem of automated damage assessment in civil infrastructure. The proposed frameworkcomprises three modules: segmentation, refinement, and annotation. The pre-trained Seg-ment Anything Model (SAM) was utilized to generate the initial segmentation, and a novelrefinement model was trained to improve the accuracy. RAM module aims to refine thesegmentation mask obtained by pre-trained models by applying a multi-stage refinementprocess. Finally, the refined masks were labeled using a transformer-based object detectionmodel, DETR.The proposed refinement module was implemented and trained using diverse datasets.The performance of the trained refinement module was evaluated by comparing it with 27various segmentation techniques. The findings indicate that R-SAM outperforms all themodels with a mean IoU of 97.23% and a mean BA of 87.06%. The results indicate thatR-SAM has promising potential for real-world applications where segmentation data maynot be available and rapid assessments are required. The proposed framework was examinedusing concrete columns tested in the laboratory. The results showed that R-SAM can detectand segment components and damages accurately.In conclusion, the proposed R-SAM framework provides an effective and efficient solutionfor automated visual damage assessment in civil infrastructure. The proposed frameworkcan be used for real-time monitoring of the infrastructure health condition, making it avaluable tool for infrastructure maintenance and management.48Chapter 4Multi-agent Cyber-PhysicalSystems4.1 OverviewWith the rise of modern technology and engineering, Cyber-Physical Systems (CPS) haveemerged as a domain that seamlessly intertwines the digital and physical components. Asthe name suggests, CPS merges cyber and physical components with applications in var-ious fields, including Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles(UGVs) and beyond. These components fall into two primary categories: sensing compo-nents and controlling components. Sensing components encompass sensors, the cornerstoneof data collection and recording for data-driven techniques. Aggregators strategically posi-tioned in the transmission layer to analyze information for decision-making processes andactuators that bridge the digital-physical divide by processing input data and translating itinto physical actions. On the other hand, the controlling components include programmablemicrocontrollers, remote terminals, and control. In this chapter, details of the designed CPSand the sensors are explained in detail.  System Control Compute Communicate Information Figure 4.1: Components of a cyber-physical system.494.2. Unmanned Aerial Vehicle (UAV)4.2 Unmanned Aerial Vehicle (UAV)The UAVs in this dissertation (Chapters 4) were selected from the affordable and pro-grammable consumer-grade UAVs listed in Table 4.1.Table 4.1: Comparison of FeaturesFeature DJI Robomaster Parrot Bebop2 Parrot AnafiPrice $ $$$ $$$$Camera 5 MP 14 MP 21 MPVideo 720 \u00d7 30fps 1080 \u00d7 30fps 2160 \u00d7 30fpsFlight time 10 min 25 min 26 minWeight 80 g 500 g 320 gProgrammable X X XIMU X X XGPS x X XGimbal x x XMemory Slot x x XTo further expand the capabilities of the Parrot Anafi, which is small and ideal for indoorinspections, it was equipped with additional sensors to avoid obstacles in all directions. A1-D LiDAR distance sensor and three ultra-sound distance sensors were installed to keepthe UAV at a safe distance from obstacles. The sensors were installed on a customized PCBboard (provided by Sina Tavasoli) with an ESP-32 to transmit real-time data. A 5-voltrechargeable battery powers these sensors and the ESP 32. Figure 4.2 shows the model indetail, and the corresponding weights are given in Table 4.2.Figure 4.2: Customized PCB for collision avoidance module for the UAV.504.3. Unmanned Ground Vehicle (UGV)Table 4.2: Payload design for the UAVModules Weight (g)3\u00d7Ultra-sound sensor 151\u00d7Lidar ToF sensor 85-volt battery 301\u00d7ESP 32 10PCB 15SUM 784.3 Unmanned Ground Vehicle (UGV)4.3.1 Design and AssemblyThe proposed cyber-physical system includes a cost-effective custom-built ground robotwith different modules (Figure 4.3). To measure the distance data points between the robotand the objects (i.e., laser scanning), a mechanical LiDAR sensor (RPLiDAR A1M8) wasutilized. This sensor can scan and generate 2D point sets (360\u00b0) to measure distance fromobjects within 12 meters [31], which is critical for collision-free navigation and mapping. Inaddition, a solid-state LiDAR camera (RealSense L515) is utilized to fuse color and depth(RGB-D) frames for accurate detection and localization of objects using 3D point clouds[32]. During the operation, this camera streams 1920\u00d71080-pixels RGB and 1024\u00d7768-pixels depth frames with 30 frames\/seconds to the Robot Operating System (ROS) network.Figure 4.3: The custom-built unmanned ground vehicle (UGV).514.3. Unmanned Ground Vehicle (UGV)4.3.2 Hardware ComponentsOmni-directional mecanum wheels chassisThe UGV platform in this study features a mecanum wheel chassis for enhanced maneuver-ability and versatility. This chassis design incorporates omnidirectional mecanum wheels,allowing the UGV to move seamlessly in any direction, making it highly agile and adapt-able to complex environments. As shown in Figure 4.4, this omnidirectional robot chassisprovides the foundation for the UGV\u2019s mobility. Figure 4.5 illustrates the working principleand motion characteristics of the 4WD mecanum wheel UGV, highlighting its capability tomove forward, backward, sideways, and rotate. This unique chassis configuration signifi-cantly enhances the UGV\u2019s navigation and control capabilities, making it an ideal choicefor applications that demand movement in various directions.Figure 4.4: The omnidirectional robot chassis.Figure 4.6 shows the ground robot platform considered a rigid body equipped with 4-mecanum wheels for omnidirectional motions. It is assumed that the wheels do not slip andthat the robot operates within a horizontal plane. In this figure, the global coordinates ofthe robot with respect to the origin, O = (XI , YI), are defined by (XR, YR) with respect tothe robot\u2019s center P . Therefore, the robot\u2019s pose \u03beI can be defined as the relative positionof the point P from the reference frame I as well as the rotation \u03b8 [309].524.3. Unmanned Ground Vehicle (UGV)Figure 4.5: The working principle and motion of the 4WD mecanum wheel UGV.YXYxpyp\u03a9ab\u03b8XIIRR2V1V3V4V234P{I}45or2 Izr2 Iz r2 Izr2 Izr2 Iz r2 IzFigure 4.6: Kinematic representation of the mecanum wheels.534.3. Unmanned Ground Vehicle (UGV)\u03beI =\uf8eb\uf8edxpyp\u03b8\uf8f6\uf8f8 (4.1)The robot\u2019s motion with respect to the reference frame I, can be represented as follows:\u03be\u02d9I = R\u03b8\u03be\u02d9R (4.2)where R\u03b8 is a transformation matrix to map the robot\u2019s motion from the local coordinateto the global and \u03be\u02d9R is the velocity of the robot in local reference, and they are defined asfollows [310]:R\u03b8 =\uf8ee\uf8f0 cos(\u03b8) sin(\u03b8) 0\u2212 sin(\u03b8) cos(\u03b8) 00 0 1\uf8f9\uf8fb , and \u03be\u02d9R =\uf8ee\uf8f0uvw\uf8f9\uf8fb (4.3)Since the body velocities, u and v, can be obtained using the rotational velocities ofthe four wheels, w = [w1, w2, w3, w4], \u03be\u02d9R = Jw, where J is the Jacobian matrix. It can besimplified as follows for the 45\u25e6 orientation of rollers:J =r4\uf8ee\uf8f0 1 1 \u22121 \u221211 \u22121 \u22121 11a+b1a+b1a+b1a+b\uf8f9\uf8fb (4.4)where a and b are shown in Figure 4.6. The dynamics of the UGV can be written as:M\u03c9\u02d9 +D\u03c9\u03c9 = \u03c4 (4.5)where \u03c4 is the DC motors torque vector and \u03c9\u02d9 is a 4\u00d71 acceleration vector of wheels. Themass matrix, M , can be obtained as follows [309]:M =\uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0r2Iz16(a+b) +mr28 + I\u03c9 \u2212 r2Iz16(a+b)r2Iz16(a+b)mr28 \u2212 r2Iz16(a+b)\u2212 r2Iz16(a+b) r2Iz16(a+b) +mr28 + I\u03c9mr28 \u2212 r2Iz16(a+b)r2Iz16(a+b)r2Iz16(a+b)mr28 \u2212 r2Iz16(a+b)r2Iz16(a+b) +mr28 + I\u03c9 \u2212 r2Iz16(a+b)mr28 \u2212 r2Iz16(a+b)r2Iz16(a+b) \u2212 r2Iz16(a+b)r2Iz16(a+b) +mr28 + I\u03c9\uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb (4.6)where m is the mass of the robot, and Iz is the moment of inertia for the robot and I\u03c9 isthe moment of inertia for the wheels. The D\u03c9 is a diagonal 4\u00d71 matrix to consider thefriction between the ground and the wheels. The C++ and Python codes for controllingthe robot are provided in Appendix B.NVIDIA Jetson Nano 4GBThe Jetson Nano (4GB), a powerful and low-power consumption single-board computer withUbuntu 18.04 LTS and ROS Melodic, was used to connect the UGV to the ROS networkfor efficient navigation and data collection. This compact edge computer has a quad-coreARM Cortex-A57 CPU, 4GB of RAM, and NVIDIA Maxwell GPU with 128 NVIDIA544.3. Unmanned Ground Vehicle (UGV)CUDA cores, which is ideal for running light-weight computer vision, AI, and roboticsapplications, including OpenCV [292]. To establish a ROS network, the Intel-AC8265 dual-mode wireless networking card module was used to provide a reliable Wi-Fi and Bluetoothconnection compared to simpler single-board computers such as Raspberry Pi 4. To deploythe low-level C++ codes for parsing the navigational commands, an ATmega2560-basedmicrocontroller (Arduino Mega) was used.Figure 4.7: NVIDIA Jetson Nano 4GB Development Kit.Arduino\/ESP32The Arduino Mega 2560 is a popular and versatile microcontroller widely utilized in variousengineering applications. This board incorporates an ATmega2560 microcontroller featuring54 digital input\/output pins, 16 analog inputs, and a 16 MHz quartz crystal oscillator. Italso includes four hardware serial ports, a USB connection, and a power jack for enhancedconnectivity. Due to its user-friendly programming interface and extensive capabilities, theArduino Mega 2560 has been adopted in various projects ranging from basic robotics toadvanced automation systems. For wireless data communications and multi-agent systems,ESP-32 modules were used.Figure 4.8: The Arduino Mega and its sensor hat (left) and ESP32 (right).554.4. Sensors and CamerasPower SupplyTwo rechargeable power units powered the UGV system. An uninterruptible power supply(UPS) module provided the Jetson Nano\u2019s stable 5V power output to ensure reliable anduninterrupted operation. The Arduino module and 12V speed encoder motors were poweredby a 3S 5100 mAh 11.1 V Lithium Polymer (LiPo) battery. The combined use of thesepower sources improved the overall reliability of the UGV for extended periods of operationwithout frequent recharging or dismounting of the batteries, which is critical for large-scaleinspections.(a) (b) (c)Figure 4.9: Power supply units: uninterruptible power supply (a) and 3S 5100 mAh 11.1 VLithium Polymer (b), and LiPo alarm.WiFi and Bluetooth ModuleIntel-AC8265 dual-mode wireless networking card module was used to provide a reliable Wi-Fi and Bluetooth connection compared to simpler single-board computers such as RaspberryPi 4.Figure 4.10: Wifi and Bluetooth module.4.4 Sensors and Cameras4.4.1 GPS ModuleThe GPS module is an essential component of the ground robot, designed to obtain preciseglobal positioning with efficiency and accuracy, which can be used for path planning anddata collection for 3D reconstruction. This HAT, L76X GPS HAT, is robust in receivingsatellite signals and compatible with Raspberry Pi and Jetson Nano.564.4. Sensors and CamerasFigure 4.11: GPS module (L76X).4.4.2 RGB CameraThe camera utilized for robotics and vision-based recording purposes is the IFROO 1080PHD Video Webcam. It was chosen due to its capacity to deliver 1080p video at 30 frames persecond (fps), ensuring the transmission of high-quality visual features. It has an 82-degreewide-angle view with minimum image distortion, even in environments with limited lighting.It is convenient for diverse vision-related tasks using OpenCV and USB 2.0 plug-and-playfunctionality.Figure 4.12: The RGB camera.4.4.3 RGB-D CamerasFigure C.1 shows the two types of RGB-D cameras that researchers and developers havewidely used in the field of robotics [311, 312]. The two presented cameras used differenttechnologies to estimate the depth frame, stereo vision, and time of flight (ToF).The RealSense D435i utilizes stereo vision technology to create a depth map by measur-ing the disparity between two images captured from both the left and right perspectives.Therefore, stereo cameras may face challenges when objects need more texture for accuratedepth perception. Given its compact size and lightweight design, this camera is ideal forapplication in outdoor environments and with drone technology.On the other hand, the ToF sensors emit infrared light pulses and measure the time574.4. Sensors and Camerasit takes for reflected pulses to reach the sensor. ToF is more accurate than stereo vision,particularly in low-light and indoor conditions. The Realsense L515 LiDAR camera is astate-of-the-art LiDAR sensor designed to meet the demands of robotics, mapping, andaugmented reality. This camera can measure distance within a range of 9 meters and witha 70-degree horizontal field of view. The depth frame of this sensor is essential for exact andaccurate 3D point cloud generation, particularly in low-light conditions. Furthermore, L515is ideal for real-time 3D mapping because it has a vision processing unit that can matchdepth and color frames and generate the point cloud in real time. This sensor has a built-inIMU that can be used for odometry and tracking algorithms. It connects to the computer(Jetson Nano, RaspberyPi, or laptop) using USB 3.2 for efficient data transmission. L515 isa solid-state LiDAR, which means that it has no mechanical system, and the entire system,including the RGB camera and IMU sensors, is run on a single chip, which makes it an idealsolution with low power consumption for small-scale applications using ground and aerialrobots. Stereo Vision Time of Flight R image L image Figure 4.13: Stereo vision vs. time of flight.4.4.4 360\u25e6 LiDAR SensorTo measure distance data points between the robot and surrounding objects, the RPLiDARA1M8 LiDAR sensor was utilized. This sensor has 360\u00b0 angular measurement capabilitieswithin a range of 12 meters, an 8,000-Hz sampling rate, and a 5.5-Hz angular scan rate basedon 360\u00b0 per round. The sensor operates by emitting infrared laser signals and receivingthe signals reflected by objects in its vicinity, providing accurate and reliable distancemeasurements. By leveraging these advanced capabilities, the RPLiDAR A1M8 sensorrepresents a powerful tool for various applications, including mapping, navigation, andobstacle detection in diverse environments.584.4. Sensors and Cameras Figure 4.14: Visualization of the generated point cloud data using RGB-D cameras   Right Left Forward    90\u00b0 0\u00b0 180\u00b0 270\u00b0 Sensor Range Obstacle   (a) (b)Figure 4.15: The mechanical 360\u25e6 LiDAR sensor (a), and visualization of the generateddata (b).594.5. Robot Operating SystemTable 4.3 compares two cost-efficient mechanical or solid-state LiDAR sensors suitablefor small-scale robotic applications. Both sensors provide a balance between performanceand cost.Table 4.3: Comparison of solid-state and mechanical LiDAR.Realsense L515 RPLiDAR A1M8Frequency 30 Hz 10 HzFoV 70\u00b0 \u00d7 55\u00b0 360\u00b0Range 0.25-9 m 0.1-12 mDimension 61 mm\u00d7 26 mm 46 mm\u00d7 33 mmWeight 100 g 100 gPower Consumption 3.5 W 5 WRGB X \u00d7IMU X \u00d7Interface USB UART\/USB4.5 Robot Operating System4.5.1 ROS NetworkThe Robot Operating System (ROS) is an open-source system for controlling robotic andsensor components from an onboard or remote computer. ROS provides a flexible platformfor communication between various components of a robot. The ROS system starts withthe ROS Master, which acts as a lookup table for all the nodes in the system. Nodes (eachcomputational instance) can register with the Master and state their capabilities, such aspublishing, subscribing to topics, and offering services.Nodes communicate with each other by publishing and subscribing to topics. For ex-ample, a sensor driver is a node that publishes the sensor data as a continuous stream ofmessages. This allows other nodes to find and communicate with each other without havingto specify the IP address or port. Each computational node can receive and publish datato the network for further processing and decision-making. ROS also provides a mechanismfor nodes to request data from other nodes, known as services. A key feature of ROS is itsflexibility in designing a distributed network, where nodes can run on different computersconnected to the same network, facilitating seamless communication among nodes.Figure 4.16 shows a multi-sensor multi-node ROS network, where several nodes canrun simultaneously and efficiently. In this diagram, a camera node generates vision datapublished to the network by the camera node. Then, the generated data can be accessedfrom the remote computer for image processing, AI-based applications, and monitoring.Similarly, point cloud data is generated and published using the LiDAR node and processedby another computer for mapping or localization.604.5. Robot Operating SystemROS MasterCamera Node LiDAR NodeCamera LiDARRegistration RegistrationImage Processing Node Mapping NodeRegistrationData DataRegistration\/point_cloud_dataMessagePublish\/image_dataMessagePublishSubscribe SubscribeFigure 4.16: A distributed ROS network with multiple sensors and computation nodes.4.5.2 Simulation and Visualization Tools for ROSGazebo software is a 3D physical simulation platform that creates a virtual simulation en-vironment in ROS. The simulation environment shown in Figure 4.17(a) is the publiclyavailable TurtleBot3 House, which includes connected small and large rooms and obsta-cles that are ideal for testing exploration and mapping algorithms that are described inSection 4.6.3.Rviz is a 3D visualization tool with many plugins that can be used to display cameraimages, robotic models, robot movement trajectories, and the generated data using com-putational nodes. Figure 4.17(b) shows the RViz environment with the generated 3D mapusing the LiDAR sensor.614.6. Multi-agent Aerial-ground Systems(a) TurtleBot3 House environment in Gazebo(b) 3D SLAM visualization in RVizFigure 4.17: The Gazebo (simulation environment) and RViz (visualization environment).4.6 Multi-agent Aerial-ground SystemsThe proposed multi-agent CPS utilizes an integrated UAV and UGV for SHM. The goal ofthe system is to autonomously navigate in an environment and create a 2D map, identify624.6. Multi-agent Aerial-ground Systemsstructural components, and tag the locations. The locations of the labeled componentsare used to generate the navigation path for the UAV to collect high-resolution imagesfrom different view angles for 3D reconstruction. To prolong the inspection time and fullyutilize the power sources on the UGV, the UAV is carried by the UGV during the initialexplorations and mapping. Figure 4.18 shows the configuration of the components and thedata transmission methods. All the units are connected to a Wi-Fi network with distinctIP addresses. The data transmission is done either through general-purpose input\/output(GPIO) pins or USB between the parts of the robot and through the Wi-Fi network betweenthe robots and the ground control station computer, where the autonomous exploration andmapping algorithms along with vision-based object detection models are deployed for real-time processing and decision making.WiFi Router(192.168.1.1)Gear MotorsRGB CameraGPIOUAV (192.168.1.122)ESP32 (192.168.1.123)GPIOUGVUAVRGB-D Camera360\u00b0  LiDARJetson Nano(192.168.1.121)ROS Nodes: camera, lidar, arduino,...UGVMicrocontrollerGround Control Station(192.168.1.120)ROS Nodes: rviz, slam, controller1D LiDAR1D UltrasoundGPIO Figure 4.18: The configuration of the proposed multi-agent cyber-physical system.4.6.1 Component-level 3D Reconstruction Using Structure-from-MotionAfter detecting and localizing the object of interest (i.e., damaged component) using theRGB-D camera on the UGV, The UAV takes off, and with the collision-avoidance modulemounted, the UAV starts scanning and rotating around the component. As demonstratedin Figure 4.19, video frames from different angles and elevations are captured and streamedto the ROS network. The safe obstacle-free zone around the UAV is shown in green dashedlines, the blue dot line shows the scanning path, and the red arrows represent the viewpointsfor video recording. If the UAV detects an obstacle during the scanning, it will stop and startrotating counter-clockwise movements with the same strategy. If the UAV faces anotherobstacle in the counter-clockwise direction, the scanning mission will be terminated, andthe UAV will return to the UGV. Once the UAV completes the recording, a new \u2018return-to-home\u2019 path is generated to return and land. The pseudocode for UAV scanning is shownbelow.634.6. Multi-agent Aerial-ground SystemsAlgorithm 1 UAV-based Scanning Pseudo CodeDetect the objectInitiate video recordingGet LiDAR and ultra-sound sensor data.while Right side ultra-sound sensor feedback = False doClockwise rotation around the objectwhile Left side ultra-sound sensor feedback = False doCounter-Clockwise rotation around the objectReturn to UGVLand \u25a0 Camera orientation    \u25a0 Flight path     \u25a0 Obstacle-free zone Figure 4.19: Component-level data collection.Over the past few years, there has been remarkable development in Structure-from-Motion (SfM), among which the incremental SfM is the most popular approach [35]. In thisstudy, the incremental SfM is used for the 3D reconstruction of structural components. Thispipeline comprises three stages: feature extraction, feature matching, geometric verification,and structure and motion reconstruction [35]. The input to this pipeline is a set of imagestaken by the UAV of the object of interest from different viewpoints, and the output is a 3Dreconstruction of the objects of interest. Figure 4.20 shows an example of the incrementalSfM of a concrete column tested in the laboratory.644.6. Multi-agent Aerial-ground Systems   Feature Extraction Feature Matching Geometric Estimation   Sparse Reconstruction           Dense Reconstruction           Figure 4.20: Component-level 3D reconstruction using Structure-from-Motion (SfM).4.6.2 Scene-level 3D Reconstruction Using Neural Radiance FieldsOnce the UGV covers the intended floor area, using the generated 2D occupancy grid mapof the surroundings, navigation routes were developed for the UAV using the GeneralizedVoronoi Diagram (GVD) [313]. As shown in Figure 4.21, the GVD algorithm partitions thegrid map based on the location of obstacles and constructs the navigation path using theA* algorithm.   Voronoi vertices       Obstacles       Navigation path  Figure 4.21: Generalized Voronoi Diagram for collision-free path planning for UAV.4.6.3 Software Components of The CPSCollision-free Path Planning and Navigation Using RRT and GVDTo solve collision-free path-planning problems, the Rapidly-exploring Random Tree (RRT)algorithm [314] is used, which is a sampling-based path-planning algorithm that efficientlysearches non-convex high-dimensional spaces to create a graph of routes until it finds apath between the \u2018start\u2019 and \u2018goal\u2019 points. An example of RRT-based path planning isshown in Figure 4.22, and more details about the algorithm are available from Karamanand Frazzoli [315]. The pseudocode for a simple RRT algorithm is provided in Algorithm 2:654.6. Multi-agent Aerial-ground SystemsAlgorithm 2 RRT (Rapidly-Exploring Random Tree)procedure RRT(Start state, Goal state, Obstacle map)Initialize: Open list, Closed list, Current state = Start statewhile Current state 6= Goal state doGenerate random stateFind the nearest state in the Open list to the random stateExtend the nearest state toward the random stateif the extended state does not collide with any obstacles thenAdd the extended state to the Open listSet the parent of the extended state as the nearest stateMark the nearest state as visited by adding it to the Closed listSet the Current state as the state in the Open list with the lowest costOutput: Path from Start state to Goal state   \u25a0 Start   \u25a0 Goal    \u25a0 Obstacles    \u25a0 Tree nodes    \u25a0 Generate Path Figure 4.22: Collision-free path-planning using the RRT algorithmThe Voronoi diagram is a mathematical construct that partitions a map into differentregions based on the proximity of the points. In the context of path planning for an au-tonomous UAV-based data collection, Voronoi diagrams can be used to construct a roadmapof the environment, with each Voronoi region representing the set of points that are closestto a specific obstacle or feature in the environment. In the presented studies in this disserta-tion, the Voronoi diagram is utilized to generate a roadmap of the environment based on theSLAM-generated occupancy grid map. The roadmap is then used to plan the UAV\u2019s flightpath through the environment, to minimize the distance traveled while avoiding collisionswith obstacles.664.6. Multi-agent Aerial-ground Systems (a)  (b)   Obstacle Obstacle Figure 4.23: Path planning using Voronoi diagram: (a) the Voronoi regions, (b) finding asafe flight path based on the Voronoi diagram.The proposed algorithm follows a specific flowchart shown in Figure 4.24, starting withreceiving the laser scanning data from the LiDAR sensor and storing it in a buffer memory.The point cloud data is then extracted from the buffer queue and down-sampled to removeredundant or noisy data. Feature extraction, including the extraction of edges and corners,is performed on the down-sampled point cloud. The extracted edges are then used toupdate the map and estimate the robot\u2019s current pose in the global coordinate system. Theestimated odometry is then used to update the global map, where a local map is built usingthe edge from the nearby frames.674.6. Multi-agent Aerial-ground SystemsRoslaunch Feature ExtractionGlobal MapOdometry EstimationLocal MapGet LiDAR DataRobotFilterAssignMoveFrontier Dectection  Figure 4.24: Collision-free path-planning using the RRT algorithmThe \u2018Filter\u2019 node is defined to filter detected frontiers based on the received globalmap. It used the ROS Transformer Library to transform the frontier from the robot\u2019scoordinate frame to the global frame. The mean-shift algorithm [316] clusters the closefrontier together. The final list of frontiers with the highest information gain is publishedto be used by the \u2018Assigner\u2019 node. The \u2018Assigner\u2019 node takes the occupancy map andthe frontiers and assigns the goal node for the robot. The final map is then publishedto another ROS node for further processing and visualization. Figure 4.25 illustrates thefrontier detection and map-generating procedure.             \u25a0 Robot    \u25a0 Trajectory    \u25a0 Obstacle     \u25a0 Frontiers     \u25a0 Free Space Figure 4.25: Collision-free navigation and mapping.684.6. Multi-agent Aerial-ground SystemsHector SLAM Algorithm for Indoor Localization and MappingThe Simultaneous Localization and Mapping (SLAM) algorithm aims to construct a 2Dmap of the environment by combining and integrating sensor data. This map, known as anoccupancy grid map, represents the environment and allows for localization and navigation.One of the challenging problems in robotics is the Kidnapped Robot Problem (KRP), whichoccurs when a robot is placed in an unknown environment and is forced to move instantlyto create a map of its surroundings [317]. With BIM-based path planning, robots aregiven the navigation path before the mission, inefficient for dynamic environments whereobstacles may block the routes. There are three most popular SLAM algorithms to utilizethe 2D 360\u00b0 LiDAR sensor for SLAM, such as gMapping [236], Hector SLAM [238], andGoogle Cartographer [242]. Considering the hardware of the robot and the dynamic natureof the project, Hector SLAM is used for occupancy grid mapping, which is based on theleast square optimization method, and compared to other methods, it would not lead toloop-closure issues as highlighted by Yagfarov et al. [318] and Turnage [319].The process of scan matching for the Hector SLAM algorithm involves optimizing thealignment of the 2D laser scan endpoints at the current time with the global map. AGausspNewton optimization approach is used to predict the robot pose. Hector SLAMutilizes a rigid body transformation:\u03be = (pxpy\u03c8)T (4.7)where px, py, and \u03c8 are the coordinates and the yaw angle of the robot in the robotcoordinate system, and it is minimized to achieve the best alignment between the laser scanand the map.\u03be\u2217 = arg min\u03ben\u2211i=1[1\u2212M(Si(\u03be))]2 (4.8)where Si represents the coordinates of the scan endpoints as described by Wang [320] asfollows:Si(\u03be) =[Si,x(\u03be)Si,y(\u03be)]=[cos(\u03c8) \u2212 sin(\u03c8)sin(\u03c8) cos(\u03c8)] [si,xsi,y]+[pxpy](4.9)where Si,x(\u03be) and Si,y(\u03be) are the ground coordinates of the i-th endpoint scanned by theLiDAR sensor. The derivative of Si(\u03be) for the i-th scanned point, based on the robotcoordinate, can be obtained as follows:\u2202Si(\u03be)\u2202\u03be=[1 0 \u2212 sin(\u03c8)si,x \u2212 cos(\u03c8)si,y0 1 cos(\u03c8)si,x sin(\u03c8)si,y](4.10)The Gauss-Newton optimization approach addresses the nonlinear least square problemabove. By utilizing the Gauss-Newton method and given the starting estimate of \u03be, therecurrence relation for minimizing the objective function is:\u03bet = \u03bet\u22121 \u2212H\u22121G (4.11)694.6. Multi-agent Aerial-ground Systemswhere G represents the gradient vector, and H represents the Hessian matrix, which isderived by neglecting second-order derivative terms. Assuming a small movement of therobot, \u2206\u03be, we have:\u2207\u03be = H\u22121n\u2211i=1[\u2207M(Si(\u03be))\u2202Si(\u03be)\u2202\u03be]T[1\u2212MSi(\u03be)] (4.12)where H is the Hessian matrix given by:H =n\u2211i=1(\u2202ri\u2202\u03be)T (\u2202ri\u2202\u03be)=n\u2211i=1[\u2207M(Si(\u03be))\u2202Si(\u03be)\u2202\u03be]T [\u2207M(Si(\u03be))\u2202Si(\u03be)\u2202\u03be](4.13)where \u2207M(Si(\u03be)) is the gradient of the current map. A comprehensive examinationof the occupancy grid approximation process can be found in the work of Kohlbrecher etal. [238].The nodes graph of the Hector SLAM implementation in ROS are shown in Figure 4.26.In this figure, the input scan data are provided by utilization of RPLiDAR sensor (Sec-tion 4.4.4).Figure 4.26: Visualization of the Hector SLAM node graph.Figure 4.27 illustrates the 2D SLAM procedure, employing the RRT path planningalgorithm detailed in Section 4.6.3. As the robot navigates the simulation environment,TurtleBot3 House, it continuously generates and updates the map autonomously.704.7. From 2D to 3D VisionFigure 4.27: Visualization of different stages of autonomous exploration and mapping.4.7 From 2D to 3D VisionIn the following sections a thorough exploration of 3D reconstruction and point cloud seg-mentation techniques is presented, providing a holistic view of state-of-the-art method-ologies for capturing, processing, and analyzing spatial data. An in-depth discussion ofStructure from Motion (SfM), a robust technique of photogrammetry. SfM facilitates theextraction of intricate 3D information from 2D images, thereby revolutionizing fields suchas 3D reconstruction, navigation, and path planning. The workflow of SfM is meticulouslyinvestigated, with particular emphasis on camera calibration and positioning in ensuringprecise 3D model generation.Building upon the foundations laid by SfM, attention then turns to the exploration ofNeural Radiance Fields (NeRF), the state-of-the-art approach that effectively addresses thelimitations of SfM when dealing with complex scene reconstruction. NeRF leverages neuralnetworks to directly model a scene\u2019s visual characteristics, offering high fidelity in capturingintricate lighting effects. NeRF\u2019s efficiency and scalability are illustrated by utilizing themulti-resolution hash encoding approach, introduced by Instant Neural Graphics Primitives(Instant-NGP).Furthermore, an innovative unsupervised point cloud segmentation technique is intro-duced, bridging the gap between 2D and 3D vision data analysis. By leveraging 2D seg-mentation methods and the Cloth Simulation Filtering (CSF) algorithm, this approachefficiently segments point clouds. The process includes converting point clouds into im-age representations and mapping segment IDs back to the original data demonstrated forclassifying spatial data.714.8. LiDAR-derived Point Cloud Data4.8 LiDAR-derived Point Cloud DataPoint Clouds, 3D Vision, and LiDAR are closely related technologies widely used in variousapplications such as mapping, surveying, and autonomous navigation. LiDAR, which standsfor Light Detection and Ranging, is a remote sensing technology that uses laser pulsesto measure objects\u2019 distance and depth range. The principle behind LiDAR is based onmeasuring the time it takes for a small light on the surface to return to its source. The systemuses a laser beam to send light pulses toward the target object and measures the reflectedpulses with a sensor. The object\u2019s distance is determined using the laser wavelength andthe return time. This technology has numerous applications in terrestrial mapping, whereit is used to generate high-resolution depth maps. Point Clouds and 3D vision are relatedtechnologies that capture and process large amounts of data to generate a 3D representationof the world. Combined with LiDAR, these technologies offer a powerful real-time toolsetfor capturing and processing 3D data.Figure 4.28: 3D point cloud data acquisition using LiDAR [321]Stereo vision and Time-of-Flight are two different methods for depth measurement in3D vision systems. Stereo vision, also known as binocular vision, relies on two camerasthat capture images of the same scene from slightly different perspectives. These imagesare then processed to calculate the depth of objects in the scene based on the differencein the position of the objects in each image. Stereo vision is a passive method that relieson existing light in the environment and is suitable for applications that require low- tomedium-resolution depth data. On the other hand, Time-of-Flight (ToF) is a technologyLiDAR cameras use to measure depth. ToF uses a light source, such as a laser or LED, toilluminate the target and a sensor to measure the time it takes for the light to travel to theobject and back. The distance of an object is calculated based on the known speed of lightand the time it took for the light to make the round trip. ToF is an active method thatprovides more accurate depth measurements, especially in environments where the existinglight is low or absent. It is commonly used in industrial and scientific applications thatrequire high-precision depth data. Both stereo vision and ToF have their strengths andweaknesses, and the choice between the two methods depends on the application\u2019s specificrequirements. Figure 4.29 illustrates the point cloud data acquisition using a solid-state724.9. LiDAR-based 3D Localization and MappingLiDAR and by fusion of RGB and Depth frames in real-time. Fusion RGB Depth LiDAR  Figure 4.29: 3D point cloud data acquisition using solid-state RealSense L515 LiDAR4.9 LiDAR-based 3D Localization and MappingThe three-dimensional Simultaneous Localization and Mapping (SLAM) techniques providemore detailed representations of an environment and allow for detecting higher-level featuressuch as structural and non-structural components. The solid-state LiDAR in this studydiffers from the mechanical LiDAR sensors because it has no mechanical moving parts, andall the components are integrated into a single board. Such sensors generate high-frequencypoint cloud data that cannot be processed using conventional mapping algorithms. LiDAR-based mapping algorithms such as Lidar Odometry and Mapping (LOAM) [322] and SSL-SLAM [311] use edge and plane detection to improve efficiency. A similar technique isdeveloped in this study to utilize a wireless ROS network for real-time mapping and objectdetection tasks.The solid-state LiDAR sensor records depth and color frames to the ROS network.The color frames are used for object detection using YOLOv8 [90] and Segment Any-thing [153] deep learning models, and the depth frames are transformed into point clouddata for 3D mapping. The point cloud data from the sensor is processed to extract edgeand plane features. Each point from the scan data, Pt, is represented with a 5D input,pi = {xi, yi, zi, \u03b1i, \u03b8i} \u2208 Pt, where xi, yi, and zi are the coordinates of the i-th point fromthe t-th scan with the corresponding vertical angle of \u03b1i and horizontal angle of \u03b8i. Followingthe methodology described by Wang et al. [311], for each scan Pt, the point cloud is seg-mented based on the vertical and horizontal angles, and the smoothness index is calculatedfor each segment of the t-th scan, \u03c3segj ,t, as follows:734.9. LiDAR-based 3D Localization and Mapping\u03c3segj ,t =1r\u2211pi\u2208St(\u2016pt\u2016 \u2212 \u2016pi\u2016) (4.14)where St is the nearby point set (in Euclidean space), and r is the search radius. Segmentswith higher smoothness indices are considered edges, and the segments with lower smooth-ness indices are considered surfaces during the feature extraction. In order to estimate therobot\u2019s position, the extracted edges and surfaces, pt \u2208 Pt, are matched with the local mapby searching the nearest feature points, p\u000f1 and p\u000f2, and calculating the distance between ptand the edge line between the two points as follows [311]:f\u000f(p\u02c6t) =\u2223\u2223\u2223\u2223(p\u02c6t \u2212 p\u000f2)\u00d7 (p\u02c6t \u2212 p\u000f1)|p\u000f1 \u2212 p\u000f2|\u2223\u2223\u2223\u2223 (4.15)where p\u02c6t = Ttpt is the transform of the edge point in the local coordinate. For matchingthe surfaces, three points are selected as the feature point,fS(p\u02c6t) =\u2223\u2223\u2223\u2223(p\u02c6t \u2212 pS1 )T \u00b7 ((pS1 \u2212 pS2 )\u00d7 (pS1 \u2212 pS3 ))|(pS1 \u2212 pS2 )\u00d7 (pS1 \u2212 pS3 )|\u2223\u2223\u2223\u2223 (4.16)The final odometry estimation, T \u2217, and map update is done by minimizing the two costfunctions above [311],T \u2217 = arg min\u2211\u000ff\u000f(p\u02c6t) +\u2211SfS(p\u02c6t) (4.17)An experiment conducted to showcase the efficacy of the real-time 3D mapping algo-rithm in Section 4.9 involved the utilization of a custom-built ground robot, as detailedin Chapter 4, equipped with a solid-state LiDAR sensor and Jetson Nano as the process-ing unit. This robot semi-autonomously navigated along a predefined route, capturing 3Dpoint cloud data of the environment. Figure 4.30 illustrates the generated point cloud data,showcasing the final map of the scanned area within the structural laboratory. Given thelimited physical space, the robot operated within the constraints of the predefined routehighlighted in red. The navigation and mapping procedure started from the entrance door,with the robot traversing the concrete specimens and making turns to refine the global mapby aligning subsequent scans. Real-time point cloud processing and map generation wereexecuted through a portable computer, as part of the wireless ROS network. The map\u2019sprogressive completion during the scanning process is shown in Figure 4.30(b), illustratingthe transition from the initial scan to a more comprehensive representation achieved byintegrating new scans with the existing map.744.9. LiDAR-based 3D Localization and MappingRobot(a) Final map and robot path(b) Real-time mappingFigure 4.30: Real-time 3D mapping using LiDAR sensor.754.10. Structure from Motion (SfM)4.10 Structure from Motion (SfM)Structure from Motion (SfM) is a three-dimensional method that is rooted in the principlesof photogrammetry [323], where depth is estimated from the two-dimensional images inorder to measure the surface point with high accuracy. In other words, SfM aims to recoverthe camera motion using the captured images. SfM has been used in a variety of fields incomputer vision including 3D reconstruction, navigation, and path planning. The use ofSfM techniques has revolutionized the field of photogrammetry by providing a cost-effectivemethod for 3D reconstruction using RGB images [324]. Figure 4.31 shows 3D point cloudgeneration using a set of 2D images taken from different angles.Figure 4.31: 3D Reconstruction using SfM.The workflow of 3D reconstruction using SfM is shown in Figure 4.32. For the 3Dreconstruction, SfM can be accomplished through two methods, using prior knowledge aboutthe scene and the degrees of freedom and using feature extraction from multiple views [325,326]. The key factor in accurate 3D reconstruction is to accurately obtain the cameracalibration and camera position in the scene.764.10. Structure from Motion (SfM)1. Data Collection (2D image set)2. Feature Detection (SIFT)3. Feature Matching4. Correspondence Estimation5. Bundle AdjustmentFigure 4.32: SfM pipeline.4.10.1 Feature Extraction Using SIFTFeature detection for SfM is a process in which the algorithm detects interesting pointswithin an image and identifies similar features by comparing all the images one by one.Therefore all the common features, such as edges and corners are detected and saved forlater matching process. Figure 4.33 shows matching pixels from two different views of thesame object.Figure 4.33: Matching features.The Scale Invariant Feature Transform (SIFT) is an algorithm that was developed byLowe in 1999 [327], and has gained popularity due to its robustness to transformations suchas scale, rotation, distortion, and noise. The SIFT algorithm extracts distinctive featuresfrom each image and stores them in a database. Then, using the Euclidean distance betweenthe features, images are matched. The SIFT algorithm can be divided into the followingsteps:774.11. Neural Radiance Fields (NeRF)1. Scale-Space Extrema Detection2. Keypoint Localization and Filtering3. Orientation Assignment4. Keypoint Descriptor5. Keypoint MatchingFigure 4.34: Five steps of the SIFT algorithm [35, 327].The first step involves the computation of the Difference of Gaussians (DoG) by apply-ing Gaussian blurring to the image with varying scaling parameters. Therefore, the localextreme pixels, as a potential key point, are identified by comparing with its neighboringpixels within the current scale and those within its adjacent scales. The second step filtersout low-contrast points using a threshold. The third step, Orientation Assignment, assignsdistinct orientations to the filtered points to make the point scale-variant. Then, a distinc-tive representation for each key point is generated, which encodes information about theregion surrounding the key point (i.e., image patch). Such representation is robust to rota-tion and scale, as well as lighting and noise. The descriptor is computed by considering theHistogram of Gradients (HoG) of the path, which encodes information such as edges andtexture. Finally, key points are matched by comparing the descriptors from the previousstep.4.11 Neural Radiance Fields (NeRF)While SfM has demonstrated its efficacy in reconstructing smaller objects and componentswithin a less complex scene, it has limitations in realistic reconstruction of a large-scalescene, which includes: a) computational complexity: processing the UAV-based videorecordings from large-scale scenes with a significant number of feature points is intensive.b) Memory limitations: storing and processing many images and feature correspondencesand the temporary variables during the 3D reconstruction demands substantial memoryresources. c) Occlusion, reflection, and limited distinct patterns: In an indoor environment,such as a construction site, SfM may fail to identify distinctive patterns accurately due tothe repetition of objects, occlusions, or limited camera viewpoints. d) Image quality: withinlarge-scale indoor scenes, the same object may appear in several images with different noiselevels, leading to performance degradation and unreliable depth estimation of SfM.Considering these limitations, integrating alternative methods such as Neural Fields(NF) [36] for 3D detailed 3D model reconstruction of large-scale indoor scenes can addressmany of the challenges mentioned above of SfM and provide enhanced scene fidelity. Neural784.11. Neural Radiance Fields (NeRF)Radiance Field (NeRF), state-of-the-art NFs, was introduced by Mildenhall et al. [37] forgenerating realistic 3D models using images taken from varying viewpoints, employing prin-ciples similar to those used in SfM. Unlike the previous approaches for 3D reconstruction,NeRF employs a trained neural network to represent the visual characteristics of the scenedirectly. Additionally, NeRF is more robust in capturing lighting effects (e.g., transparency,reflection) [328]. As a result, NeRFs have been at the center of attention as a promisingtechnique for generating realistic 3D scenes and have been used for different applications[133, 328].Figure 4.35: The Neural Radiance Fields [37]As shown in Figure 4.35, a NeRF model represents a scene using a radiance field. NeRFutilizes a multi-layer perceptron (MLP) network, F\u0398, to map a 5D input of spatial locationand viewing angle of the camera, (x, y, z, \u03b8, \u03c6), to the color and density of the emitted lighttowards the camera (r, g, b, \u03c3), for a point in the scene [133]. The volume density in NeRFmodels indicates the significance of a point within the overall scene, and the likelihoodof including the predicted color. Unlike voxel grid or mesh models, NeRFs are capableof generating different representations of the same viewpoint, which makes them ideal forconsidering lighting reflection as well as transparencies.4.11.1 Multiresolution Hash EncodingOne of the most popular NeRF models is Instant Neural Graphics Primitives (Instant-NGP), proposed by Muller et al. [329]. They used a novel multiresolution hash encodingmethod to represent the inputs in order to speed up the training of NeRF by utilizingsmaller neural networks. In Instant-NGP, the encoding parameters are also trained alongwith the network weights.As shown in Figure 4.36, for a point X from a 2D image, the surrounding grids areassigned indices using their coordinates. Therefore, it is possible to construct look-uptables for the indices that correspond to trainable feature vectors. By linearly interpolatingfeature vectors and concatenating them, the final vector can be obtained, which is passed toa neural network for training (Figure 4.36). Further details regarding the implementationof NeRFs are available from the original research paper [37] and in the literature [133, 328].Using image data collected from a steel frame, the Instant-NGP model was trained toreconstruct the 3D scene. As it can be seen from Figure 4.37, the final result is highly794.11. Neural Radiance Fields (NeRF)accurate and the details are preserved, which indicates that NeRF models are capable ofgenerating novel views from a scene with minim information loss. Such a clean reconstruc-tion approach can be used in construction monitoring, structural health monitoring, as wellas post-disaster response (Section 4.11.2).Figure 4.36: Illustration of the multi-resolution hash encoding [329](a) Input images (280 frames with 3840\u00d7 2160 resolution)(b) ResultFigure 4.37: 3D scene reconstruction using NeRF.The NeRF results from the 3D construction of the scene, have been subjected to a visual804.11. Neural Radiance Fields (NeRF)comparison with corresponding views captured by the camera. This evaluation highlightsthe ability of NeRF to preserve details and maintain high-quality reconstructions. The visualanalysis reveals that NeRF effectively captures intricate features and reproduces scenes,demonstrating its proficiency in generating accurate and realistic 3D representations fromthe given input data. Such a powerful tool can be used for post-disaster assessment and fordeveloping advanced virtual scenes for training robotics and machine learning models.(a) Captured views (b) Reconstructed viewsFigure 4.38: Quality assessment of NeRF results.4.11.2 Application of NeRF in Post-disaster ResponseTu\u00a8rkiye has modern building codes, and many buildings in the southeastern province ofMalatya\u2019s Bostanbas\u00b8\u0131 neighborhood (newly constructed luxury residences) did not collapseduring the first earthquakes. However, many buildings suffered severe damage that led to thecollapse during the second earthquake and the aftershocks. In this section, the Instant-NGPalgorithm [329] was used to reconstruct a 3D scene using a video captured by a UAV [330].The 3D reconstruction using NeRF is shown in Figure 4.39, which highlights the capabilitiesof NeRF-based models in 3D scene reconstruction over the other traditional methods suchas photogrammetry (Figure 4.39-(a)). The algorithm was able to capture the complex814.12. Conclusionsgeometry of the scene with fine details while maintaining the texture of the buildings. Thereconstructed scene also demonstrates accurate lighting and shading that results in photo-realistic, coherent, and consistent results. It should be noted that NeRF-based models arealso computationally expensive and require further research in optimizing the models formore efficient 3D reconstruction in real-world applications such as post-earthquake damageassessment and planning.  (a) (b)   (c) (d)  Figure 4.39: Results of the 3D reconstruction: (a) SfM method (drone trajectory in red),(b)-(d) NeRF.4.12 ConclusionsThis chapter proposes a novel, affordable aerial-ground cyber-physical system (CPS) forrapid structural health monitoring applications. The proposed CPS utilizes advanced simul-taneous localization and mapping (SLAM) and collision-free path-planning algorithms tomap an unknown environment through a custom-built unmanned ground vehicle (UGV). Atthe same time, high-resolution images are collected by the unmanned aerial vehicle (UAV)for component-level and scene-level 3D reconstruction. The framework also employed a dis-tributed ROS network for communication between multiple sensors, robots, and computingunits, allowing for efficient resource management and monitoring. Furthermore, 3D dataprocessing techniques were developed for analyzing the 3D point clouds using supervisedand unsupervised deep learning models. This chapter has investigated 3D scene recon-824.12. Conclusionsstruction and point cloud segmentation. Structure from Motion (SfM) was discussed as afoundational method in 3D model generation, and the transition to Neural Radiance Fields(NeRF) addressed the limitations of SfM in complex scene reconstruction, offering photo-realistic representations. The integration of innovative segmentation techniques bridged thegap between 2D and 3D data analysis, expanding the horizons of spatial data processing.These insights provide a robust foundation for diverse applications, including constructionmonitoring, structural damage assessment, and post-disaster response.83Chapter 5Implementations of Multi-agentCyber-physical SystemThis chapter explores the design, development, and integration of a collaborative Cyber-Physical System (CPS) tailored for autonomous exploration, mapping, structural compo-nent recognition, and 3D reconstruction in indoor environments. The chapter begins withthe system\u2019s conceptual framework, outlining its robotic agents, hardware components,and software architecture, as well as the synergistic integration of ground-based UnmannedGround Vehicles (UGVs) and aerial Unmanned Aerial Vehicles (UAVs) configured withLiDAR sensors and advanced algorithms for path planning, navigation, and SimultaneousLocalization and Mapping (SLAM).The hardware components of the CPS include the NVIDIA Jetson, Arduino, mobilityplatform, and battery system. The CPS software components include integrating rapidlyexploring random Tree (RRT) and Generalized Voronoi Diagrams (GVD) path planningalgorithms and Hector SLAM for indoor localization and environment mapping. Theseintertwined software modules facilitate autonomous exploration and mapping in dynamicindoor settings. Moreover, the chapter compares ground-based and aerial exploration andhighlights the superior efficiency of a system equipped with 360\u00b0 LiDAR sensors coveringunknown environments. The chapter concludes with the implementation of autonomousexploration and mapping in the context of large indoor spaces, indicating the method\u2019ssuitability for real-world applications. Moreover, it explains the process of structural com-ponent recognition and 3D reconstruction using object detection and the NeRF models inidentifying and reconstructing damaged components and entire scenes with photo-realisticprecision.5.1 Experimental StudiesThis section presents the two experiments for testing and validating the capabilities of theproposed collaborative UAV-UGV CPS.5.1.1 Ground-based vs. Aerial ExplorationAlthough several methods exist for enhancing a UAV or UGV capabilities, adding sensorsor actuators can increase cost and computational complexity. The authors highlight theproposed system\u2019s affordability and the balance between cost and performance as its keyadvantage. Figure 5.1 illustrates the information gain (covered area) of an unknown indoorenvironment using a UGV with 360\u00b0 LiDAR and a UAV with 1D LiDAR, showcasing the845.1. Experimental Studiestrajectory of both robots with autonomous navigation and exploration. Mounting a 2DLiDAR on micro-aerial vehicles (MAVs) is not feasible and may require upgrading to moreadvanced, large, and expensive UAVs.  (a) UAV  (b) UGV  \u25cf- Robot Trajectory   \u25a0Obstacle    \u25a0Free Space \u25cf Start   \u25cf Current  Figure 5.1: Comparing UAV vs. UGV for indoor mapping.The results demonstrate that the UGV with 360\u00b0 LiDAR is more efficient for a full scanof an unknown environment because the exploration module of the system incorporates atravel distance minimization algorithm. Additionally, the UGV\u2019s accurate mapping andlocalization reduce the time required to cover an unknown scene fully. In contrast, eventhough UAVs are more maneuverable and faster, a UAV equipped with a 1D LiDAR sensortakes a long time to explore the same environment, which hinders the use of advanced SLAMalgorithms for simultaneous exploration and mapping with minimized travel distance.5.1.2 Remote Monitoring of UGV Hardware UtilizationThe implementation of a distributed ROS network allows for optimal utilization of theembedded devices with limited capacities, which is essential for running multiple modulesand components in real time. To monitor the hardware usage remotely, the \u2018jetson-stats\u2019utility is used, a command-line tool that can be used to monitor the utilization of the varioushardware components on NVIDIA Jetson platforms.). The results, shown in Figure 5.2,depict the utilization of the quad-core ARM processor, RAM, and GPU usage for the first200 seconds of the operation. The results indicate that all four CPU cores are fully utilizedduring the SLAM and data collection. Jetson Nano has 128 NVIDIA CUDA cores, whichwere not fully utilized in this study since all the computer vision tasks are accomplishedusing a powerful computer within the network. Last but not least, RAM usage was alsologged, which varies within the range of 60% to 80%.855.1. Experimental Studies Figure 5.2: Jetson Nano utilization during the 2D-SLAM.5.1.3 Autonomous Exploration and MappingThe exploration and mapping algorithm was tested on the custom-built UGV in a largeroom with connected partitions (Figure 5.3). The visual representation of the explorationand mapping process demonstrates the algorithm\u2019s ability to navigate in an unknown en-vironment and generate a map of the surroundings. The results also indicate that thealgorithm is suitable for real-world applications. In this study, the Hausdorff distance (HD)metric is used to assess the accuracy of the generated SLAM map with respect to theground truth map [319, 331]. The normalized directed Hausdorff distance (NDHD) wascomputed between the SLAM-generated map and the ground truth map, with 0 indicatingidentical images and 1 indicating entirely dissimilar images. With an NDHD of 0.0321, thegenerated SLAM map is considered to have a high degree of similarity to the ground truthmap, thereby increasing the reliability of the SLAM-generated map. Further details aboutthe calculation and interpretation of the HD metric were provided by Torres-Torriti andGuesalaga [332]. Since the robot navigates autonomously, small and sharp corners are notscanned to avoid collision with obstacles. In addition, a slight drift in the final map canbe minimized using advanced optimization algorithms [311]. In Figure 5.3, the locationsof the placed objects of interest are shown with yellow square colors and the take-off\/landlocations. The trajectory of the robot navigation during the exploration and mapping isdemonstrated in blue-red colors.865.1. Experimental Studies (c) (b) (a) \u25a0 Obstacle    \u25a0 Free space  \u25a0 Detected columns    Take-off\/Landing locations -\u25cf UGV trajectory Figure 5.3: The exploration and mapping results: (a) The office room, (b) 2D SLAM results,and (c) the ground truth map.The results demonstrate that the UGV with 360\u00b0 LiDAR is more efficient for a full scanof an unknown environment because the exploration module of the system incorporates atravel distance minimization algorithm. Additionally, the UGV\u2019s accurate mapping andlocalization reduce the time required to cover an unknown scene fully. In contrast, eventhough UAVs are more maneuverable and faster, a UAV equipped with a 1D LiDAR sensortakes a long time to explore the same environment, which hinders the use of advanced SLAMalgorithms for simultaneous exploration and mapping with minimized travel distance.5.1.4 Structural Component Recognition and 3D ReconstructionFigure 5.4 shows the training history and performance of the YOLOv5 algorithm. The figureincludes the plots of box loss (train and validation) and object loss (train and validation).The box loss measures the error in predicting the bounding boxes of the component anddamage in the images, and the object loss measures the error in predicting the presence875.1. Experimental Studiesof object-of-interest in the input image frames. As the model\u2019s training progresses withmore epochs, the loss decreases and accuracy increases, indicating that a healthy trainingapproach and the algorithm are becoming more accurate in predicting the bounding boxesand the presence of objects in the images. The precision and recall plots are also provided toevaluate the performance of the detection algorithm. Precision measures the proportion oftrue positive detections to the total number of positive detections, and recall measures theproportion of true positive detections to the total number of actual objects. Precision andrecall balance the model\u2019s ability to correctly identify and detect all objects in an image.Mean Average Precision (mAP) measures the model\u2019s overall performance across multipleobject classes and is calculated by taking the average of the model\u2019s precision at variousrecall levels.    Figure 5.4: Training history and YOLOv5 performance.885.1. Experimental Studies  Figure 5.5: Component and damage localization in different environments.The recorded video files are stored in the host computer after completing the UAV-based component-level data collection. The post-processing step on the host computer iscrucial for extracting more accurate and detailed information about the object of interestand for damage quantification. For the component-level 3D reconstruction, the recordedvideo frames by the UAV are extracted and processed using SfM to generate the 3D pointclouds of the object of interest. Figure 5.6 shows the results of the 3D reconstruction oftwo damaged concrete columns using the recorded video by UAV, which provides a detailedrepresentation of the actual RC column. During the post-processing, a significant numberof points in 3D are generated, making it possible to identify and quantify volumetric defectsand deformations.              Figure 5.6: The 3D reconstruction of two damaged columns using the video frames capturedby the UAV.895.1. Experimental Studies5.1.5 Efficeint-SfM Using R-SAMDespite the effectiveness of SfM methods, inaccuracies in the generated 3D models mayoccur due to the presence of reflections, lighting, occlusions, and repeated patterns in thescene [333]. One approach to address these issues and improve the accuracy of SfM is usingmasked images. Masking can remove unwanted objects, such as moving people or robots,which may cause artifacts in the 3D model. Limiting the feature extraction to specificregions of interest allows a faster and more accurate 3D reconstruction using high-resolutionimages (Figure 5.7). This is particularly important when using limited computational power,such as low-RAM GPUs, as it reduces computation costs and data storage requirements.FusionFigure 5.7: Multi-class segmentation using R-SAM.The application of the proposed R-SAM, Chapter 3, in the 3D reconstruction of struc-tural components using masked images was investigated. For this purpose, COLMAP [35]was used. The results for multi-level segmentation (i.e., component-level and damage-level)masks were generated automatically, and the background pixels were removed from the com-putation, leaving power for more accurate and detailed 3D reconstruction of the objects ofinterest. The results of this experiment are shown in Figure 5.8. The results demonstratethat the proposed R-SAM method can produce finer and denser point clouds of damagedregions while requiring fewer points and achieving faster results than the original images.905.1. Experimental Studies (a) Without mask 2,513,854 points 1,996,808 points (b) With mask (component) 1,017, 864 points (c) With mask (damage) Figure 5.8: 3D reconstruction using original images and multi-class masked images.5.1.6 3D Reconstruction of Scene Using NeRFThis experiment was carried out to generate a large-scale 3D representation of a constructionscene using an optimized navigation path and NeRF. The first step involves creating the2D occupancy grid map of the surroundings by utilizing the 360\u00b0 mechanical LiDAR sensorinstalled on the custom-built UGV and the exploration and mapping algorithms illustratedabove. Similar to component-level data collection, only ROS nodes were deployed on therobots to allocate the resources efficiently, and navigation and mapping algorithms andvisualization tools were initiated from the portable computer station on the ground (e.g.,ROS core, RViz, Hector SLAM). The generated grid map of an indoor construction site forthis experiment is shown in Figure 5.9(a). The GVD-based diagram of the scanned area isshown in Figure 5.9(b). The safest navigation routes are the edges of the Voronoi regions,and the optimal locations for recording the 360\u00b0 videos are the Voronoi vertices. For thisstudy, the 2D map was used for 3D navigation planning using obstacle avoidance, assumingthat the grid map is the same for the specified elevation.   Figure 5.9: GVD-based path planning: (a) experimental setup, (b) navigation routes.915.1. Experimental StudiesThe UAV in this experiment was programmed to take 360\u00b0 recordings at the proximityof the Voronoi vertices, the intersection of the Voronoi region edges, to capture multipleviews of the objects within the scene. The recorded video frames were sent to the hostcomputer, where the NeRF algorithm was utilized to generate the full 3D photorealisticrepresentation of the construction site. In total, 400 video frames with 3840\u00d72160 resolutionwere used to generate the 3D map representation of the scene. The COLMAP v3.8 [35] wasused to extract the camera information for training the NeRF model. The rendered finaloutput of the trained NeRF model is presented in Figure 5.10. As can be seen from thevisual results, the entire scene is reconstructed with high accuracy and preserved details,which is not feasible using conventional methods. A closer look at the reconstructed scene,Figure 5.10(b), demonstrates the high-fidelity maintained details and high accuracy of theNeRF model for site inspections, particularly in construction management and post-disasterresponse. The developed model was validated for its explainability using the state-of-the-art segmentation model, Segment Anything (SAM) [153]. For this purpose, a frame fromthe recorded video by the UAV was compared with a similar but new viewpoint from thereconstructed NeRF-based model. The segmentation results are presented in Figure 5.11 forboth frames. Based on the obtained visual results, there is a great agreement between thenovel reconstructed viewpoints and the input images (i.e., photorealistic). Furthermore, thesegmentation results using R-SAM show its effectiveness in segmenting the object of interestfrom the reconstructed synthetic views, highlighting its potential for SHM applications.925.1. Experimental Studies  (a) (b) (c) Figure 5.10: 3D scene reconstruction using NeRF: (a) structural laboratory site, (b) regionof interest, and (c) extracted point clouds of the region of interest.935.2. Point Cloud Processing  (a) Input video frame and the segmentation results for the detected components: wall, beam, support   (b) New viewpoint from NeRF-based reconstructed model and the corresponding segmentation results  Figure 5.11: Segmentation of (a) recorded video frame and (b) the new synthetic frameusing NeRF.5.2 Point Cloud Processing5.2.1 Segmentation of 3D Point Clouds Using RANSACA novel algorithm is introduced in this section for autonomous segmentation of 3D pointclouds, clustering the data in an unsupervised way. The initial phase involves thoroughlyassessing the acquired 3D point clouds to ensure their integrity and removing any detectedoutliers. Subsequent processing of the point clouds is facilitated through the utilization ofthe Random Sampling Algorithm Consensus (RANSAC) [334] and Density-Based SpatialClustering (DBSCAN) [335, 336] algorithms, which play critical roles in partitioning thegenerated point clouds into distinct planes and objects.RANSAC, illustrated in Algorithm 3, is a robust framework for detecting shapes withinnoisy data. It iteratively identifies potential models based on randomly selected data pointsand computes the number of inliers (data points consistent with the model) within a certainthreshold distance. The model with the highest number of inliers is the best fit. Conversely,DBSCAN, illustrated in Algorithm 4, excels in uncovering clusters based on point density.It labels data points as noise, border points, or core points while effectively clustering byconnecting core points within a specified distance. The resulting clusters offer valuableinsights into the environment\u2019s structural composition and aid in ground floor detectionand obstacle avoidance tasks in robotics applications.945.2. Point Cloud ProcessingRead point clouds (*.ply)Remove global outliersDown-samplingAssign labels to each point Perform 3D shape detection on the point cloud using RANSACEstimate the normals using KDTreeSearchHybrid Remove local outlierPerform clustering on the point cloud using the DBSCANVisualize segmented regions in 3D Figure 5.12: Flowchart of the proposed method for automatic segmentation and clustering of3D point clouds using RANSAC-based 3D shape detection and DBSCAN-based clustering.The following pseudocodes provide an overview of both algorithms, implemented inOpen3D [337].Algorithm 3 RANSAC (Random Sample Consensus)procedure RANSAC(Data points, Number of iterations n, Threshold distance d)Initialize: Best model, Best scorefor i in range(n) doSelect m random data points to form a possible modelfor each data point doMeasure the distance between the point and the modelif distance \u00a1 d thenInclude the point as an inlierif number of inliers is highest so far thenUpdate Best model and Best scoreReturn: Best model with the highest number of inliers955.2. Point Cloud ProcessingAlgorithm 4 DBSCAN (Density-Based Spatial Clustering)procedure DBSCAN(Dataset D, Epsilon \u03b5, Minimum number of points minPts)Initialize: Cluster C, Visited Points V = {}for each point P in D doif P is not in V thenAdd P to VNeighbors N = points in D within distance \u03b5 of Pif size of N \u00a1 minPts thenP is considered as noiseelseC = new clusterAdd P to Cfor each point P \u2032 in N doif P \u2032 is not in V thenAdd P \u2032 to VNeighbors N \u2032 = points in D within distance \u03b5 of P \u2032if size of N \u2032 >= minPts thenAdd N \u2032 to NAdd C to the set of clustersReturn: Set of clusters CFigure 5.13 provides a visual representation of the outcomes of the clustering algorithmapplied to 3D point clouds from a reconstructed reinforced concrete column. In this figure,the rendered point clouds show regions of interest, highlighting exposed rebar and damagedareas within the scanned column. This approach enhances the processing and segmentationof point cloud data. It holds promise for many structural health monitoring applications,where identifying distinct planes is crucial in decision-making processes, such as obstacleavoidance and ground floor detection.965.2. Point Cloud Processing  >50 mm 5 mm   >50 mm 5 mm Figure 5.13: Concrete damage quantification using the proposed algorithm.5.2.2 Segmentation of 3D Point Clouds Using TransformersThe point cloud data generated by SfM or NeRF can be used for autonomous detection andsegmentation. Several deep learning models have been recently developed for processingpoint cloud data. For example, PointNet [338] and PointNet++ [339] were used for clas-sification and segmentation, processing point clouds without voxelization. However, mostsegmentation approaches are supervised learning-based models requiring labeled data [340].A new algorithm is proposed in this section to achieve high-accuracy point cloud segmen-tation in a single-shot way. This algorithm, shown in Figure 5.15, can efficiently segmentpoint clouds by utilizing 2D segmentation methods, such as R-SAM (Chapter 3).In the subsequent step, the (Cloth Filter for Separation) algorithm(CFS) [341] was used,which separates point clouds into ground and non-ground measurements without requiringintricate parameter tuning for high accuracy. The CFS algorithm inverts the point cloudand applies a flexible cloth to cover the surface. The algorithm determines the groundsurface points by analyzing the interactions between the cloth nodes and the correspondingpoints (Figure 5.14).975.2. Point Cloud ProcessingFigure 5.14: Overview of the CFS algorithm [341] .A custom Python function was developed to convert a point cloud into an image repre-sentation, which takes the input array of the points cloud, where each point is representedin (x, y, z, r, g, b) format. The RGB color values are extracted from the points in the lattercase. The function then maps these points to pixel coordinates based on the bounding boxdimensions. Subsequently, it creates an image array where each pixel corresponds to a pointin the cloud, with the pixel\u2019s color representing the RGB values of the corresponding point.This approach facilitates the visualization of the point cloud data as an image, making iteasier to segment using 2D segmentation methods. Finally, after segmenting the 2D im-age, the segment IDs were mapped back to the point cloud. Figure 5.16 shows the pointcloud segmentation for the data generated from Section 4.10 using the method developedin Chapter 3.985.2. Point Cloud ProcessingInitialize segmentation modelRead point cloudDetect & remove outliersApply Cloth Simulation Filtering (CSF)Convert point cloud to an imageSegment the image (R-SAM)Map segment IDs to point cloudExport segmented point cloudEndFigure 5.15: Point cloud segmentation using R-SAM995.3. Conclusions  (a) Input (b) Output Figure 5.16: Point cloud segmentation using the proposed method5.3 ConclusionsThis chapter implemented the proposed affordable multi-agent cyber-physical system (CPS)for rapid structural health monitoring applications. The CPS utilizes advanced simultane-ous localization and mapping (SLAM) and collision-free path-planning algorithms to mapan unknown environment through a custom-built unmanned ground vehicle (UGV). At thesame time, high-resolution images are collected by the unmanned aerial vehicle (UAV) forcomponent-level and scene-level 3D reconstruction. The framework employed a distributedROS network for communication between multiple sensors, robots, and computing units.A lightweight deep-learning model was trained for structural component recognition anddamage detection, and state-of-the-art Segment Anything (SAM) was utilized for segmen-tation. The 3D dense point cloud of the detected damaged components was generatedusing the structure-from-motion (SfM) technique, and the scene-level photo-realistic 3Drepresentation was carried out using the Neural Radiance Fields (NeRF) method. Thisstudy\u2019s results demonstrate the proposed method\u2019s efficiency, high accuracy, and potentialfor use in real-world applications.100Chapter 6Conclusions and Future WorkIn this concluding chapter, a summary of the core elements of this research endeavor isprovided. The chapter is structured into three key sections: Summary, wherein the majorfindings are revisited, and their significance is highlighted; Contributions, where the novelinsights generated by this dissertation are reflected upon, along with their implications forthe broader field; and finally, Limitations and Future Works, where the constraints ofthe research are acknowledged, and future directions are highlighted.6.1 SummaryThis dissertation provides a comprehensive literature review of deep learning-based com-puter vision methods for structural health monitoring (SHM). With growing concerns aboutaging infrastructure, this research aimed to leverage cutting-edge technologies to expeditethe assessment of critical infrastructure and ensure its long-term integrity. To bridge thegap between 2D and 3D vision analysis, this dissertation introduced innovative techniquesfor 2D image segmentation, 3D scene reconstruction, and point cloud segmentation. Itutilizes Structure from Motion (SfM) and unveils the potential of Neural Radiance Fields(NeRF) for capturing complex scenes with photo-realistic precision. Additionally, it pro-poses a novel unsupervised point cloud segmentation method for spatial data analysis andquantification.Furthermore, this research investigates multi-agent Cyber-Physical Systems (CPS) forautonomous exploration, mapping, and structural component recognition in indoor envi-ronments. By synergistically integrating unmanned ground vehicles (UGVs) and aerialdrones equipped with color and LiDAR sensors, it demonstrates the efficiency of such acollaborative CPS in covering unknown indoor spaces. Finally, a new framework is pro-posed to design intelligent robotic systems powered by large language models (LLMs). Anovel pipeline showcases the potential of LLMs, such as ChatGPT and Llama-2, to serveas conversational interfaces, enabling natural and intuitive human-robot interaction.In summary, this dissertation introduces new research directions toward vision-basedstructural health monitoring by utilizing the capabilities of generative artificial intelligence,computer vision, and collaborative robotics.6.2 ContributionsThe primary contributions of the research lie in the innovation and advancement of novelalgorithms and frameworks within the domains of computer vision and robotics with ap-1016.2. Contributionsplications in structural health monitoring, such as automated exploration and scanningof structural components and accurate measurements using the proposed R-SAM throughhigh-resolution image segmentation. At the juncture of composing this dissertation, no doc-umented research endeavor exists focused on creating generative robotics tailored explicitlyfor structural health monitoring. The main contributions of the research are revisited below:1. In this dissertation, a novel segmentation pipeline has been developed for multi-classsegmentation of high-resolution images. This pipeline leverages the state-of-the-artSegment-Anything Model (SAM) and a novel mask refinement method, which exhibitssuperior performance and high boundary accuracy. The results obtained through thisproposed method will facilitate the generation of high-resolution segmentation masksusing modern cameras and robotics, all while maintaining segmentation accuracy.This approach is significant in structural health monitoring applications, where precisemeasurements are paramount for informed decision-making.2. Significant strides have been made in the field of 3D vision-based structural healthmonitoring and damage assessment. A comprehensive exploration of 3D scene recon-struction and point cloud segmentation techniques is presented. Traditional methods,as well as cutting-edge approaches, are covered to highlight the strengths and limita-tions of each. SfM has been used as a foundational technique for 3D model generationfrom UAV-based collected video frames. Furthermore, the field has been advanced byan in-depth investigation of NeRF, a state-of-the-art method addressing SfM\u2019s limi-tations, offering photo-realistic scene representations with preserved lighting effects.A new technique is proposed for real-time 3D scanning of structural components andindoor scenes using a lightweight solid-state LiDAR sensor and an affordable groundrobot. Furthermore, an innovative unsupervised segmentation technique has also beenproposed, bridging the gap between 2D and 3D vision data analysis. The proposedmethod enhances spatial data processing and opens opportunities for diverse appli-cations in construction monitoring, structural damage assessment, and post-disasterresponse.3. This dissertation introduces, for the first time, the concept of \u2019generative robotics\u2019and \u2019integrated multi-agent cyber-physical systems\u2019 in structural health monitoringapplications. The proposed pipeline for multi-agent aerial-ground CPS presents aseamless integration of Unmanned Ground Vehicles (UGVs) and Unmanned AerialVehicles (UAVs), each equipped with different sensors and distributed computationalnodes. This integration enables autonomous exploration, mapping, structural com-ponent recognition, and 3D reconstruction in complex and dynamic environments.Its lightweight and cost-effectiveness set this system apart, balancing affordabilityand high performance. The pipeline\u2019s contributions extend to real-world applications,demonstrating its suitability for large indoor spaces.The integration of LLMs into robotics is proposed within the scope of this disserta-tion. In the proposed framework, users interact with robots using natural languageinstead of low-level codes, where voice commands are converted to text and pro-cessed by LLMs. The LLMs\u2019 output is parsed, and the machine extracts and executes1026.3. Limitations and Future Workexecutable codes in a semi-supervised manner. The results of this research unveilopportunities for future investigations into developing advanced multi-agent roboticssystems for structural health monitoring and damage assessment without the need forprogramming. This approach allows robots to upgrade themselves without necessi-tating a redesign of the underlying core framework.6.3 Limitations and Future Work6.3.1 Limitations and Future Work for Segmentation ModelsWhile the effectiveness of the implementation of the proposed pipeline for R-SAM meetsacceptable requirements for the conducted experiments and considering large objects in theinput images, it could be better. The SAM module may not perfectly segment or detectsmall components or disconnected and occluded objects. In addition, the RAM module mayrequire high GPU memory for very large images to generate high-resolution segmentationwith higher accuracy. As a solution, future studies would optimize the proposed R-SAMfor typical GPUs by segmenting and refining the detected regions of interest (i.e., boundingboxes) by applying object detection before SAM. Furthermore, training an object detectionmodel still requires supervised training. Both the dataset and the selected object detectionmodels would affect the performance of R-SAM. The proposed framework would be ideal forgenerating panoptic segmentation datasets to train other segmentation models. Similarly,it would generate multi-level segmentation masks for scene parsing or 3D reconstruction(i.e., component-level and damage-level).6.3.2 Challenges and Prospects in 3D Vision TechnologiesWhile the methods explored in this dissertation have demonstrated their efficacy in variousaspects of 3D scene reconstruction and point cloud segmentation, several limitations stillneed to be considered. One notable limitation is handling occluded objects within the unsu-pervised segmentation methods, mainly using SAM. In cases where objects are partially orfully obscured from view, a 2D image segmentation model may struggle to assign segmenta-tion IDs to point clouds accurately. Future work should investigate techniques for handlingoccluded objects by incorporating multi-viewpoint segmentation approaches. Moreover, thecomputational demands of Neural Radiance Fields (NeRF) could be a potential bottleneckwhen processing large-scale scenes. Addressing the scalability and optimization of NeRFsremains challenging for real-world applications in structural health monitoring.Moving forward, future research will focus on enhancing the robustness of the 3D vi-sion models to diverse environmental conditions, which may lead to improved segmentationaccuracy and damage quantification. Additionally, investigating the fusion of sensor modal-ities, such as LiDAR and RGB imagery, could offer comprehensive representations for moreaccurate scene reconstruction or segmentation. Further studies would explore the applica-tions of such technologies in disaster response scenarios, focusing on optimizing algorithmsfor rapid decision-making. Ultimately, using various intelligent robots for 3D vision appli-cations remains promising, with potential applications in structural health monitoring andautonomous navigation.1036.3. Limitations and Future Work6.3.3 Limitations of Multi-agent CPS and Future HorizonsThere are several noteworthy limitations and potential for multi-agent CPS for efficientexploration, mapping, and structural assessment. While the proposed CPS framework rep-resents a significant step forward, it may only suit specific applications with improvements.For example, there is a trade-off between the efficiency and cost-effectiveness of differentexploration methods. The designed ground-based unmanned vehicles equipped with 360\u00b0LiDAR sensors outperform their aerial counterparts within the exact cost or size range re-garding mapping efficiency. However, addressing the computational complexities and thesoftware of the system systems remains an ongoing challenge. Future work would exploreinnovative ways to optimize performance by integrating advanced machine learning modelsand efficient data processing techniques.In this dissertation, only limited structural components were trained to be detectedusing object detection models and 3D reconstruction, and there is room for enhancing theversatility of object detection models using more class labels. Moreover, extending themethodology to real-world scenarios with dynamic environments and unforeseen challengesremains an avenue for pushing the applications beyond the laboratory setting.6.3.4 Toward Generative Robotics: Limitations and Future Work forLLMsThe proposed generative cyber-physical system has several limitations: a) The performanceof the proposed system is dependent on the accuracy and effectiveness of LLM. Therefore,any limitations of LLMs may impact the overall performance. For instance, a pre-filtermodule may be needed for long queries due to the limitation on the maximum token lengthin ChatGPT. b) Prompt engineering is a key system component that enables ChatGPT tounderstand the problem; however, designing a robust and comprehensive library of promptscan be time-consuming and require expertise. Equally important, it is necessary to becautious regarding the security and privacy of the APIs for cloud-based LLMs. c) Dependingon the complexity of the problem, the proposed model may perform slower than the othermodels trained for a specific task. d) There is a trade-off between LLMs\u2019 performance andresponsiveness. Therefore, more responsive models such as Llama-2 [263] do not generatecodes to fly a drone based on the rationale that flying a drone may require a license. f)Finally, like any machine learning models, LLMs are prone to hallucination, where generatedresponses are plausible but incorrect.As a recommendation for future research endeavors, further investigations should be con-ducted into the applications of ChatGPT, Llama-2, and Code Llama in advanced roboticssystems, such as using visual prompts and reward design for autonomous agents. Text-based LLM poses limitations in processing and generating images, as addressed in [342].Integrating visual capabilities in conversational language models holds significant poten-tial for SHM and robotics applications. Such interactions facilitate complex visual tasksin SHM, such as visual questioning and multiagent collaborative AI models and robots.Building upon the concept of reward design with language models [343], researchers canexplore the potential of using LLM-based models to simplify the process of reward designin reinforcement learning (RL) for robotics. Such research may evaluate the effectiveness of1046.3. Limitations and Future WorkLLMs in generating reward signals and compare them with the other traditional approachesin terms of accuracy, robustness, and reliability. Such investigations may contribute to de-veloping more efficient approaches or training autonomous robotic systems using advancedRL methods.105Bibliography[1] ASCE. ASCE\u2019s 2017 Infrastructure Report Card, 2018. Accessed on 17 September2018.[2] FHWA. Bridge Condition by Highway System 2019, 2019. Accessed on 11 May 2019.[3] H. Sohn, C.R. Farrar, F.M. Hemez, D.D. Shunk, D.W. Stinemates, B.R. Nadler, andJ.J. Czarnecki. A review of structural health monitoring literature: 1996\u20132001. LosAlamos Natl. Lab., 20:34\u201345, 2003.[4] O. Salawu. Detection of structural damage through changes in frequency: A review.Eng. Struct., 19:718\u2013723, 1997.[5] Y. An, E. Chatzi, S.H. Sim, S. Laflamme, B. Blachowski, and J. Ou. Recent progressand future trends on damage identification methods for bridge structures. Struct.Control Health Monit., 12:e2416, 2019.[6] M. Mashayekhi and E. Santini-Bell. Three-dimensional multiscale finite element mod-els for in-service performance assessment of bridges. Comput.-Aided Civ. Infrastruct.Eng., 34:385\u2013401, 2019.[7] M. Mashayekhizadeh. Fatigue assessment of complex structural components of steelbridges integrating finite element models and field-collected data. Bridge Struct.,15:75\u201386, 2018.[8] T. Cicero, P. Cawley, F. Simonetti, and S.I. Rokhlin. Potential and limitations of adeconvolution approach for guided wave structural health monitoring. Struct. HealthMonit., 8:381\u2013395, 2009.[9] Mohsen Azimi, Akbar Rasoulnia, Zhibin Lin, and Hong Pan. Improved semi-activecontrol algorithm for hydraulic damper-based braced buildings. Structural Controland Health Monitoring, 24(11):e1991, 2017.[10] Mohsen Azimi and Asghar Molaei Yeznabad. Swarm-based parallel control of adja-cent irregular buildings considering soil\u2013structure interaction. Journal of Sensor andActuator Networks, 9(2):18, 2020.[11] T. Rakha and A. Gorodetsky. Review of unmanned aerial system (uas) applicationsin the built environment: Towards automated building inspection procedures usingdrones. Autom. Constr., 93:252\u2013264, 2018.106Bibliography[12] H. Pan, M. Azimi, G. Gui, F. Yan, and Z. Lin. Vibration-based support vector ma-chine for structural health monitoring. In International Conference on ExperimentalVibration Analysis for Civil Engineering Structures, pages 167\u2013178, 2017.[13] H. Pan, M. Azimi, F. Yan, and Z. Lin. Time-frequency-based data-driven structuraldiagnosis and damage detection for cable-stayed bridges. J. Bridge Eng., 23:56\u201370,2018.[14] R. Zhao, R. Yan, Z. Chen, K. Mao, P. Wang, and R.X. Gao. Deep learning and itsapplications to machine health monitoring. Mech. Syst. Signal Process., 115:213\u2013237,2019.[15] H. Salehi and R. Burgueno. Emerging artificial intelligence methods in structuralengineering. Eng. Struct., 171:170\u2013189, 2018.[16] A Dadras Eslamlou, A Kaveh, M Azimi, and TY Yang. Structural health monitoringvia a group-theoretic wsa for optimal feature selection and data fusion. In Structures,volume 57, page 105280. Elsevier, 2023.[17] Y. Bao, Z. Tang, H. Li, and Y. Zhang. Computer vision and deep learning\u2013based dataanomaly detection method for structural health monitoring. Struct. Health Monit.,18:401\u2013421, 2019.[18] Y.J. Cha, W. Choi, G. Suh, S. Mahmoudkhani, and O. Bu\u00a8yu\u00a8ko\u00a8ztu\u00a8rk. Autonomousstructural visual inspection using region-based deep learning for detecting multipledamage types. Comput.-Aided Civ. Infrastruct. Eng., 33:731\u2013747, 2018.[19] D.J. Atha and M.R. Jahanshahi. Evaluation of deep learning approaches based onconvolutional neural networks for corrosion detection. Struct. Health Monit., 17:1110\u20131128, 2018.[20] F. Wei, G. Yao, Y. Yang, and Y. Sun. Instance-level recognition and quantificationfor concrete surface bughole based on deep learning. Autom. Constr., 107:102920,2019.[21] C.V. Dung and L.D. Anh. Autonomous concrete crack detection using deep fullyconvolutional neural network. Autom. Constr., 99:52\u201358, 2019.[22] L. Zhang, F. Yang, Y.D. Zhang, and Y.J. Zhu. Road crack detection using deep convo-lutional neural network. In 2016 IEEE International Conference on Image Processing(ICIP), pages 3708\u20133712, 2016.[23] S. Dorafshan, M. Maguire, and X. Qi. Automatic surface crack detection in concretestructures using otsu thresholding and morphological operations. UTC Rep., 1:2\u20138,2016.[24] S. Kaneko, S. Oka, and N. Matsumiya. Detection of cracks in concrete structuresfrom digital camera images. NTT Tech. Rev., 10:23\u201328, 2012.107Bibliography[25] L. Wu, S. Mokhtari, A. Nazef, B. Nam, and H.-B. Yun. Improvement of crack-detection accuracy using a novel crack defragmentation technique in image-basedroad assessment. J. Comput. Civ. Eng., 30:04014118, 2014.[26] Yifei Xiao, Xiao Pan, Sina Tavasoli, Mohsen Azimi, Yingnan Bao, Ehsan Noroozine-jad Farsangi, and Tony TY Yang. Autonomous inspection and construction of civilinfrastructure using robots. In Automation in Construction toward Resilience, pages1\u201326. CRC Press, 2024.[27] Hung M La, Nenad Gucunski, Kristin Dana, and Seong-Hoon Kee. Developmentof an autonomous bridge deck inspection robotic system. Journal of Field Robotics,34(8):1489\u20131504, 2017.[28] Spencer Gibb, Hung Manh La, Tuan Le, Luan Nguyen, Ryan Schmid, and Huy Pham.Nondestructive evaluation sensor fusion with autonomous robotic system for civilinfrastructure inspection. Journal of Field Robotics, 35(6):988\u20131004, 2018.[29] Parneet Kaur, Kristin J Dana, Francisco A Romero, and Nenad Gucunski. Auto-mated gpr rebar analysis for robotic bridge deck evaluation. IEEE transactions oncybernetics, 46(10):2265\u20132276, 2015.[30] Habib Ahmed, Hung Manh La, and Nenad Gucunski. Review of non-destructive civilinfrastructure evaluation for bridges: State-of-the-art robotic platforms, sensors andalgorithms. Sensors, 20(14):3954, 2020.[31] Kyeong Ho Cho, Young Hoon Jin, Ho Moon Kim, and Hyouk Ryeol Choi. Devel-opment of novel multifunctional robotic crawler for inspection of hanger cables insuspension bridges. In 2014 IEEE international conference on robotics and automa-tion (ICRA), pages 2673\u20132678. IEEE, 2014.[32] Sattar Dorafshan and Marc Maguire. Bridge inspection: Human performance, un-manned aerial systems and automation. Journal of Civil Structural Health Monitoring,8:443\u2013476, 2018.[33] Sattar Dorafshan, Robert J Thomas, and Marc Maguire. Benchmarking image pro-cessing algorithms for unmanned aerial system-assisted crack detection in concretestructures. Infrastructures, 4(2):19, 2019.[34] Sina Tavasoli, Xiao Pan, TY Yang, Saudah Gazi, and Mohsen Azimi. Autonomousdamage assessment of structural columns using low-cost micro aerial vehicles andmulti-view computer vision. In Canadian Conference - Pacific Conference on Earth-quake Engineering, 2023.[35] Johannes L Schonberger and Jan-Michael Frahm. Structure-from-motion revisited.In Proceedings of the IEEE conference on computer vision and pattern recognition,pages 4104\u20134113, 2016.[36] Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan,Federico Tombari, James Tompkin, Vincent Sitzmann, and Srinath Sridhar. Neural108Bibliographyfields in visual computing and beyond. In Computer Graphics Forum, volume 41,pages 641\u2013676. Wiley Online Library, 2022.[37] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, RaviRamamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields forview synthesis. Communications of the ACM, 65(1):99\u2013106, 2021.[38] Mohsen Azimi, Armin Dadras Eslamlou, T.Y. Yang, and Shiping Huang. Remote sens-ing and machine learning for enhanced post-disaster response: Insights from the 2023tu\u00a8rkiye\u2013syria earthquake. In Canadian Conference - Pacific Conference on Earth-quake Engineering, 2023.[39] M. Gatti. Structural health monitoring of an operational bridge: A case study. Eng.Struct., 195:200\u2013209, 2019.[40] E Vaghefi and S Serajzadeh. Deformation behavior and cavitation of aa2017 at ele-vated temperatures. Metals and Materials International, 27:4368\u20134382, 2021.[41] H. Kim, E. Ahn, M. Shin, and S.-H. Sim. Crack and noncrack classification fromconcrete surface images using machine learning. Struct. Health Monit., 18:725\u2013738,2019.[42] Mohsen Azimi. Design of Structural Vibration Control Using Smart Materials andDevices for Earthquake-Resistant and Resilient Buildings. PhD thesis, North DakotaState University, 2017.[43] S. Dorafshan, M. Maguire, N.V. Hoffer, and C. Coopmans. Fatigue crack detectionusing unmanned aerial systems in under-bridge inspection. Ida. Transp. Dep., 2:1\u2013120,2017.[44] S. Dorafshan, R.J. Thomas, and M. Maguire. Fatigue crack detection using un-manned aerial systems in fracture critical inspection of steel bridges. J. Bridge Eng.,23:04018078, 2018.[45] E.P. Carden and P. Fanning. Vibration based condition monitoring: A review. Struct.Health Monit., 3:355\u2013377, 2004.[46] M.R. Jahanshahi and S.F. Masri. Adaptive vision-based crack detection using 3d scenereconstruction for condition assessment of structures. Autom. Constr., 22:567\u2013576,2012.[47] L. Li, Q. Wang, G. Zhang, L. Shi, J. Dong, and P. Jia. A method of detecting thecracks of concrete undergo high-temperature. Constr. Build. Mater., 162:345\u2013358,2018.[48] M. Hamrat, B. Boulekbache, M. Chemrouk, and S. Amziane. Flexural cracking be-havior of normal strength, high strength and high strength fiber concrete beams, usingdigital image correlation technique. Constr. Build. Mater., 106:678\u2013692, 2016.109Bibliography[49] A. Rimkus, A. Podviezko, and V. Gribniak. Processing digital images for crack local-ization in reinforced concrete members. Procedia Eng., 122:239\u2013243, 2015.[50] I. Abdel-Qader, O. Abudayyeh, and M.E. Kelly. Analysis of edge-detection techniquesfor crack identification in bridges. J. Comput. Civ. Eng., 17:255\u2013263, 2003.[51] R.S. Lim, H.M. La, and W. Sheng. A robotic crack inspection and mapping systemfor bridge deck maintenance. IEEE Trans. Autom. Sci. Eng., 11:367\u2013378, 2014.[52] A.M.A. Talab, Z. Huang, F. Xi, and L. HaiMing. Detection crack in image using otsumethod and multiple filtering in image processing techniques. Optimacal, 127:1030\u20131033, 2016.[53] T. Yamaguchi, S. Nakamura, R. Saegusa, and S. Hashimoto. Image-based crackdetection for real concrete surfaces. IEEJ Trans. Electr. Electron. Eng., 3:128\u2013135,2008.[54] A. Ebrahimkhanlou, A. Farhidzadeh, and S. Salamone. Multifractal analysis of crackpatterns in reinforced concrete shear walls. Struct. Health Monit., 15:81\u201392, 2016.[55] Jr. Spencer, B.F., V. Hoskere, and Y. Narazaki. Advances in computer vision-basedcivil infrastructure inspection and monitoring. Engineering, 3:12\u201322, 2019.[56] A. Zhang, K.C. Wang, Y. Fei, Y. Liu, C. Chen, G. Yang, J.Q. Li, E. Yang, andS. Qiu. Automated pixel-level pavement crack detection on 3d asphalt surfaces witha recurrent neural network. Comput.-Aided Civ. Infrastruct. Eng., 34:213\u2013229, 2019.[57] D. Feng and M.Q. Feng. Vision-based multipoint displacement measurement for struc-tural health monitoring. Struct. Control. Health Monit., 23:876\u2013890, 2016.[58] D. Lecompte, J. Vantomme, and H. Sol. Crack detection in a concrete beam usingtwo different camera techniques. Struct. Health Monit., 5:59\u201368, 2006.[59] M.R. Jahanshahi and S.F. Masri. Parametric performance evaluation of wavelet-basedcorrosion detection algorithms for condition assessment of civil infrastructure systems.J. Comput. Civ. Eng., 27:345\u2013357, 2012.[60] T. Dawood, Z. Zhu, and T. Zayed. Machine vision-based model for spalling detectionand quantification in subway networks. Autom. Constr., 81:149\u2013160, 2017.[61] S. German, I. Brilakis, and R. DesRoches. Rapid entropy-based detection and proper-ties measurement of concrete spalling with machine vision for post-earthquake safetyassessments. Adv. Eng. Inform., 26:846\u2013858, 2012.[62] K. Vaghefi, T.M. Ahlborn, D.K. Harris, and C.N. Brooks. Combined imaging tech-nologies for concrete bridge deck condition assessment. J. Perform. Constr. Facil.,29:04014102, 2013.[63] Q. Mei, M. Gu\u00a8l, and M.R. Azim. Densely connected deep neural network consideringconnectivity of pixels for automatic crack detection. Autom. Constr., 110:103018,2020.110Bibliography[64] Y. Xu, Y. Bao, J. Chen, W. Zuo, and H. Li. Surface fatigue crack identification insteel box girder of bridges by a deep fusion convolutional neural network based onconsumer-grade camera images. Struct. Health Monit., 18:653\u2013674, 2018.[65] R. Ali and Y.J. Cha. Subsurface damage detection of a steel bridge using deep learningand uncooled micro-bolometer. Constr. Build. Mater., 226:376\u2013387, 2019.[66] G. Fu, P. Sun, W. Zhu, J. Yang, Y. Cao, M.Y. Yang, and Y. Cao. A deep-learning-based approach for fast and robust steel surface defects classification. Opt. LasersEng., 121:397\u2013405, 2019.[67] S. Dorafshan, R.J. Thomas, and M. Maguire. Comparison of deep convolutional neuralnetworks and edge detectors for image-based crack detection in concrete. Constr.Build. Mater., 186:1031\u20131045, 2018.[68] H. Kim, E. Ahn, S. Cho, M. Shin, and S.-H. Sim. Comparative analysis of imagebinarization methods for crack identification in concrete structures. Cem. Concr.Res., 99:53\u201361, 2017.[69] Y. Liu, S. Cho, B.F. Spencer Jr, and J. Fan. Automated assessment of cracks onconcrete surfaces using adaptive digital image processing. Smart Struct. Syst., 14:719\u2013741, 2014.[70] I. Giakoumis, N. Nikolaidis, and I. Pitas. Digital image processing techniques for thedetection and removal of cracks in digitized paintings. IEEE Trans. Image Process.,15:178\u2013188, 2006.[71] M.R. Jahanshahi, J.S. Kelly, S.F. Masri, and G.S. Sukhatme. A survey and evalu-ation of promising approaches for automatic image-based defect detection of bridgestructures. Struct. Infrastruct. Eng., 5:455\u2013486, 2009.[72] Y.J. Cha, W. Choi, and O. Bu\u00a8yu\u00a8ko\u00a8ztu\u00a8rk. Deep learning-based crack damage detectionusing convolutional neural networks. Comput.-Aided Civ. Infrastruct. Eng., 32:361\u2013378, 2017.[73] Xincong Yang, Heng Li, Yantao Yu, Xiaochun Luo, Ting Huang, and Xu Yang. Auto-matic pixel-level crack detection and measurement using fully convolutional network.Computer-Aided Civil and Infrastructure Engineering, 33(12):1090\u20131109, 2018.[74] Qipei Mei and Mustafa Gu\u00a8l. Multi-level feature fusion in densely connected deep-learning architecture and depth-first search for crack segmentation on images collectedwith smartphones. Structural Health Monitoring, 19(6):1726\u20131744, 2020.[75] Mohammad R Jahanshahi and Sami F Masri. A new methodology for non-contactaccurate crack width measurement through photogrammetry for automated structuralsafety evaluation. Smart materials and structures, 22(3):035019, 2013.[76] Chul Min Yeum and Shirley J Dyke. Vision-based automated crack detection forbridge inspection. Computer-Aided Civil and Infrastructure Engineering, 30(10):759\u2013770, 2015.111Bibliography[77] Soheil Bahrampour, Naveen Ramakrishnan, Lukas Schott, and Mohak Shah. Compar-ative study of deep learning software frameworks. arXiv preprint arXiv:1511.06435,2015.[78] Soheil Bahrampour, Naveen Ramakrishnan, Lukas Schott, and Mohak Shah. Compar-ative study of deep learning software frameworks. arXiv preprint arXiv:1511.06435,2015.[79] Nobuyuki Otsu. A threshold selection method from gray-level histograms. IEEEtransactions on systems, man, and cybernetics, 9(1):62\u201366, 1979.[80] Mostafa Rabah, Ahmed Elhattab, and Atef Fayad. Automatic concrete cracks detec-tion and mapping of terrestrial laser scan data. NRIAG Journal of Astronomy andGeophysics, 2(2):250\u2013255, 2013.[81] Prateek Prasanna, Kristin J Dana, Nenad Gucunski, Basily B Basily, Hung M La,Ronny Salim Lim, and Hooman Parvardeh. Automated crack detection on concretebridges. IEEE Transactions on automation science and engineering, 13(2):591\u2013599,2014.[82] Waseem Rawat and Zenghui Wang. Deep convolutional neural networks for imageclassification: A comprehensive review. Neural computation, 29(9):2352\u20132449, 2017.[83] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convo-lutional encoder-decoder architecture for image segmentation. IEEE transactions onpattern analysis and machine intelligence, 39(12):2481\u20132495, 2017.[84] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networksfor semantic segmentation. In Proceedings of the IEEE conference on computer visionand pattern recognition, pages 3431\u20133440, 2015.[85] Fu-Chen Chen and Mohammad R Jahanshahi. Nb-cnn: Deep learning-based crackdetection using convolutional neural network and na\u00a8\u0131ve bayes data fusion. IEEETransactions on Industrial Electronics, 65(5):4392\u20134400, 2017.[86] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only lookonce: Unified, real-time object detection. In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 779\u2013788, 2016.[87] Joseph Redmon and Ali Farhadi. Yolo9000: better, faster, stronger. In Proceedingsof the IEEE conference on computer vision and pattern recognition, pages 7263\u20137271,2017.[88] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXivpreprint arXiv:1804.02767, 2018.[89] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. Yolov4: Optimalspeed and accuracy of object detection. arXiv preprint arXiv:2004.10934, 2020.[90] Glenn Jocher, Ayush Chaurasia, and Jing Qiu. YOLO by Ultralytics, January 2023.112Bibliography[91] Gang Li, Biao Ma, Shuanhai He, Xueli Ren, and Qiangwei Liu. Automatic tunnelcrack detection based on u-net and a convolutional neural network with alternatelyupdated clique. Sensors, 20(3):717, 2020.[92] Arcady Soloviev, Boris Sobol, and Pavel Vasiliev. Identification of defects in pavementimages using deep convolutional neural networks. In Advanced Materials: Proceedingsof the International Conference on \u201cPhysics and Mechanics of New Materials andTheir Applications\u201d, PHENMA 2018, pages 615\u2013626. Springer, 2019.[93] Baoxian Li, Kelvin CP Wang, Allen Zhang, Enhui Yang, and Guolong Wang. Au-tomatic classification of pavement crack using deep convolutional neural network.International Journal of Pavement Engineering, 21(4):457\u2013463, 2020.[94] Zheng Tong, Jie Gao, Zhenqiang Han, and Zhenjun Wang. Recognition of asphaltpavement crack length using deep convolutional neural networks. Road Materials andPavement Design, 19(6):1334\u20131349, 2018.[95] Zhun Fan, Chong Li, Ying Chen, Paola Di Mascio, Xiaopeng Chen, Guijie Zhu, andGiuseppe Loprencipe. Ensemble of deep convolutional neural networks for automaticpavement crack detection and measurement. Coatings, 10(2):152, 2020.[96] Zhun Fan, Yuming Wu, Jiewei Lu, and Wenji Li. Automatic pavement crack detectionbased on structured prediction with the convolutional neural network. arXiv preprintarXiv:1802.02208, 2018.[97] Kelvin CP Wang, Allen Zhang, Joshua Qiang Li, Yue Fei, Cheng Chen, and BaoxianLi. Deep learning for asphalt pavement cracking recognition using convolutional neuralnetwork. In Airfield and Highway Pavements 2017, pages 166\u2013177. 2017.[98] Wenjun Liu, Yuchun Huang, Ying Li, and Qi Chen. Fpcnet: Fast pavementcrack detection network based on encoder-decoder architecture. arXiv preprintarXiv:1907.02248, 2019.[99] Byunghyun Kim and Soojin Cho. Automated vision-based detection of cracks onconcrete surfaces using a deep learning technique. Sensors, 18(10):3452, 2018.[100] Young-Jin Cha and Wooram Choi. Vision-based concrete crack detection using aconvolutional neural network. In Dynamics of Civil Structures, Volume 2: Proceedingsof the 35th IMAC, A Conference and Exposition on Structural Dynamics 2017, pages71\u201373. Springer, 2017.[101] Yun-Kyu An, Keunyoung Jang, Byunghyun Kim, and Soojin Cho. Deep learning-based concrete crack detection using hybrid images. In Sensors and smart structurestechnologies for civil, mechanical, and aerospace systems 2018, volume 10598, pages273\u2013284. SPIE, 2018.[102] Keunyoung Jang, Yun-Kyu An, Byunghyun Kim, and Soojin Cho. Automated crackevaluation of a high-rise bridge pier using a ring-type climbing robot. Computer-AidedCivil and Infrastructure Engineering, 36(1):14\u201329, 2021.113Bibliography[103] Rahmat Ali, Dharshan Lokekere Gopal, and Young-Jin Cha. Vision-based concretecrack detection technique using cascade features. In Sensors and Smart StructuresTechnologies for Civil, Mechanical, and Aerospace Systems 2018, volume 10598, pages147\u2013153. SPIE, 2018.[104] Lovedeep Ramana, Wooram Choi, and Young-Jin Cha. Fully automated vision-basedloosened bolt detection using the viola\u2013jones algorithm. Structural Health Monitoring,18(2):422\u2013434, 2019.[105] Lovedeep Ramana, Wooram Choi, and Young-Jin Cha. Automated vision-based loos-ened bolt detection using the cascade detector. In Sensors and Instrumentation,Volume 5: Proceedings of the 35th IMAC, A Conference and Exposition on StructuralDynamics 2017, pages 23\u201328. Springer, 2017.[106] Young-Jin Cha, Kisung You, and Wooram Choi. Vision-based detection of loosenedbolts using the hough transform and support vector machines. Automation in Con-struction, 71:181\u2013188, 2016.[107] Chul Min Yeum, Shirley J Dyke, and Julio Ramirez. Visual data classification inpost-event building reconnaissance. Engineering Structures, 155:16\u201324, 2018.[108] Yang Xu, Shiyin Wei, Yuequan Bao, and Hui Li. Automatic seismic damage identifica-tion of reinforced concrete columns from images by a region-based deep convolutionalneural network. Structural Control and Health Monitoring, 26(3):e2313, 2019.[109] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE international conference oncomputer vision, pages 1440\u20131448, 2015.[110] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural informationprocessing systems, 28, 2015.[111] Niannian Wang, Xuefeng Zhao, Zheng Zou, Peng Zhao, and Fei Qi. Autonomousdamage segmentation and measurement of glazed tiles in historic buildings via deeplearning. Computer-Aided Civil and Infrastructure Engineering, 35(3):277\u2013291, 2020.[112] Lu Deng, Hong-Hu Chu, Peng Shi, Wei Wang, and Xuan Kong. Region-based cnnmethod with deformable modules for visually classifying concrete cracks. Appliedsciences, 10(7):2528, 2020.[113] Xinxiang Zhang, Dinesh Rajan, and Brett Story. Concrete crack detection usingcontext-aware deep semantic segmentation network. Computer-Aided Civil and In-frastructure Engineering, 34(11):951\u2013971, 2019.[114] Allen Zhang, Kelvin CP Wang, Baoxian Li, Enhui Yang, Xianxing Dai, Yi Peng, YueFei, Yang Liu, Joshua Q Li, and Cheng Chen. Automated pixel-level pavement crackdetection on 3d asphalt surfaces using a deep-learning network. Computer-Aided Civiland Infrastructure Engineering, 32(10):805\u2013819, 2017.114Bibliography[115] Yang Xu, Shunlong Li, Dongyu Zhang, Yao Jin, Fujian Zhang, Na Li, and Hui Li.Identification framework for cracks on a steel structure surface by a restricted boltz-mann machines algorithm based on consumer-grade camera images. Structural Controland Health Monitoring, 25(2):e2075, 2018.[116] Vedhus Hoskere, Yasutaka Narazaki, Tu Hoang, and BillieF Spencer Jr. Vision-based structural inspection using multiscale deep convolutional neural networks. arXivpreprint arXiv:1805.01055, 2018.[117] Vedhus Hoskere, Yasutaka Narazaki, Tu A Hoang, and Billie F Spencer Jr. To-wards automated post-earthquake inspections with deep learning-based condition-aware models. arXiv preprint arXiv:1809.09195, 2018.[118] Chul Min Yeum, Jongseong Choi, and Shirley J Dyke. Automated region-of-interestlocalization and classification for vision-based visual assessment of civil infrastructure.Structural Health Monitoring, 18(3):675\u2013689, 2019.[119] Yuqing Gao and Khalid M Mosalam. Deep transfer learning for image-based structuraldamage recognition. Computer-Aided Civil and Infrastructure Engineering, 33(9):748\u2013768, 2018.[120] Xiao Liang. Image-based post-disaster inspection of reinforced concrete bridge sys-tems using deep learning with bayesian optimization. Computer-Aided Civil and In-frastructure Engineering, 34(5):415\u2013430, 2019.[121] Yasutaka Narazaki, Vedhus Hoskere, Tu A Hoang, and Billie F Spencer Jr. Automatedbridge component recognition using video data. arXiv preprint arXiv:1806.06820,2018.[122] Pablo F Alcantarilla, Simon Stent, German Ros, Roberto Arroyo, and RiccardoGherardi. Street-view change detection with deconvolutional networks. AutonomousRobots, 42:1301\u20131322, 2018.[123] Simon Stent, Riccardo Gherardi, Bjo\u00a8rn Stenger, Kenichi Soga, and Roberto Cipolla.Visual change detection on tunnel linings. Machine Vision and Applications, 27:319\u2013330, 2016.[124] Ali Du\u00a8zova, Sinem Akgu\u00a8l, GU\u00a8LEN U\u00a8TI\u02d9NE, and YILMAZ YILDIZ. The tu\u00a8rkiye-syriaearthquake: a response from the editors of the turkish journal of pediatrics, 2023.[125] Leila Katebi, Mohsen Tehranizadeh, and Negar Mohammadgholibeyki. A generalizedflexibility matrix-based model updating method for damage detection of plane trussand frame structures. Journal of Civil Structural Health Monitoring, 8:301\u2013314, 2018.[126] Billie F Spencer Jr, Vedhus Hoskere, and Yasutaka Narazaki. Advances in computervision-based civil infrastructure inspection and monitoring. Engineering, 2019.[127] Da Hu, Shuai Li, Jing Du, and Jiannan Cai. Automating building damage reconnais-sance to optimize drone mission planning for disaster response. Journal of Computingin Civil Engineering, 37(3):04023006, 2023.115Bibliography[128] Hong Pan, Mohsen Azimi, Guoqing Gui, Fei Yan, and Zhibin Lin. Vibration-basedsupport vector machine for structural health monitoring. In International Conferenceon Experimental Vibration Analysis for Civil Engineering Structures, pages 167\u2013178.Springer.[129] Y.; Narazaki, V.; Hoskere, T.A.; Hoang, and BF Jr. Spencer. Vision-based automatedbridge component recognition with high-level scene consistency. Computuer-AidedCivil and Infrastructure Engineering, 2019.[130] Mohsen Azimi, Armin Dadras Eslamlou, and Gokhan Pekcan. Data-driven structuralhealth monitoring and damage detection through deep learning: State-of-the-art re-view. Sensors, 20(10):2778, 2020.[131] Mohsen Azimi and Gokhan Pekcan. Structural health monitoring using extremelycompressed data through deep learning. Computer-Aided Civil and InfrastructureEngineering, 35(6):597\u2013614, 2020.[132] Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shah-baz Khan, and Mubarak Shah. Transformers in vision: A survey. ACM ComputingSurveys (CSUR), 2021.[133] Kyle Gao, Yina Gao, Hongjie He, Denning Lu, Linlin Xu, and Jonathan Li.Nerf: Neural radiance field in 3d vision, a comprehensive review. arXiv preprintarXiv:2210.00379, 2022.[134] Agostino Goretti and Giacomo Di Pasquale. An overview of post-earthquake dam-age assessment in italy. In Eeri invitational workshop. An action plan to developearthquake damage and loss data protocols, California, 2002.[135] Navjot Kaur, Cheng-Chun Lee, Ali Mostafavi, and Ali Mahdavi-Amiri. Large-scalebuilding damage assessment using a novel hierarchical transformer architecture onsatellite images. Computer-Aided Civil and Infrastructure Engineering, 2023.[136] Rohit Gupta and Mubarak Shah. Rescuenet: Joint building segmentation and damageassessment from satellite imagery. In 2020 25th International Conference on PatternRecognition (ICPR), pages 4405\u20134411. IEEE.[137] Hanxiang Hao, Sriram Baireddy, Emily R Bartusiak, Latisha Konz, Kevin LaTourette,Michael Gribbons, Moses Chan, Edward J Delp, and Mary L Comer. An attention-based system for damage assessment using satellite imagery. In 2021 IEEE Interna-tional Geoscience and Remote Sensing Symposium IGARSS, pages 4396\u20134399. IEEE,2021.[138] Chuyi Wu, Feng Zhang, Junshi Xia, Yichen Xu, Guoqing Li, Jibo Xie, ZhenhongDu, and Renyi Liu. Building damage detection using u-net with attention mechanismfrom pre-and post-disaster remote sensing datasets. Remote Sensing, 13(5):905, 2021.[139] Hao Chen, Zipeng Qi, and Zhenwei Shi. Remote sensing image change detection withtransformers. IEEE Transactions on Geoscience and Remote Sensing, 60:1\u201314, 2021.116Bibliography[140] Joseph Z Xu, Wenhan Lu, Zebo Li, Pranav Khaitan, and Valeriya Zaytseva. Buildingdamage detection in satellite imagery using convolutional neural networks. arXivpreprint arXiv:1910.06444, 2019.[141] MAXAR. Open data program, 2023.[142] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, XiaohuaZhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,and Sylvain Gelly. An image is worth 16x16 words: Transformers for image recognitionat scale. arXiv preprint arXiv:2010.11929, 2020.[143] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Re-thinking atrous convolution for semantic image segmentation. arXiv preprintarXiv:1706.05587, 2017.[144] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyra-mid scene parsing network. In Proceedings of the IEEE conference on computer visionand pattern recognition, pages 2881\u20132890.[145] Philipe Ambrozio Dias and Henry Medeiros. Semantic segmentation refinement bymonte carlo region growing of high confidence detections. In Computer Vision\u2013ACCV2018: 14th Asian Conference on Computer Vision, Perth, Australia, December 2\u20136,2018, Revised Selected Papers, Part II 14, pages 131\u2013146. Springer, 2019.[146] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convo-lutional encoder-decoder architecture for image segmentation. IEEE transactions onpattern analysis and machine intelligence, 39(12):2481\u20132495, 2017.[147] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and HartwigAdam. Encoder-decoder with atrous separable convolution for semantic image seg-mentation. In Proceedings of the European conference on computer vision (ECCV),pages 801\u2013818, 2018.[148] Michal Drozdzal, Eugene Vorontsov, Gabriel Chartrand, Samuel Kadoury, and ChrisPal. The importance of skip connections in biomedical image segmentation, pages179\u2013187. Springer, 2016.[149] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan LYuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrousconvolution, and fully connected crfs. IEEE transactions on pattern analysis andmachine intelligence, 40(4):834\u2013848, 2017.[150] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan NGomez,  Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances inneural information processing systems, 30, 2017.[151] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kir-illov, and Sergey Zagoruyko. End-to-end object detection with transformers. InEuropean conference on computer vision, pages 213\u2013229. Springer, 2020.117Bibliography[152] Linwei Ye, Mrigank Rochan, Zhi Liu, and Yang Wang. Cross-modal self-attentionnetwork for referring image segmentation. In Proceedings of the IEEE\/CVF conferenceon computer vision and pattern recognition, pages 10502\u201310511, 2019.[153] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, LauraGustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.Segment anything. arXiv preprint arXiv:2304.02643, 2023.[154] Sina Poorghasem and Yi Bao. Review of robot-based automated measurement ofvibration for civil engineering structures. Measurement, 207:112382, 2023.[155] Photchara Ratsamee, Pakpoom Kriengkomol, Tatsuo Arai, Kazuto Kamiyama, Ya-sushi Mae, Kiyoshi Kiyokawa, Tomohiro Mashita, Yuki Uranishi, and Haruo Take-mura. A hybrid flying and walking robot for steel bridge inspection. In 2016 IEEEinternational symposium on safety, security, and rescue robotics (SSRR), pages 62\u201367.IEEE, 2016.[156] Ronny Salim Lim, Hung Manh La, Zeyong Shan, and Weihua Sheng. Developing acrack inspection robot for bridge maintenance. In 2011 IEEE International Conferenceon Robotics and Automation, pages 6288\u20136293. IEEE, 2011.[157] Nicholas Charron, Evan McLaughlin, Stephen Phillips, Kevin Goorts, SriramNarasimhan, and Steven L Waslander. Automated bridge inspection using mobileground robotics. Journal of Structural Engineering, 145(11):04019137, 2019.[158] Alexis Leibbrandt, Gilles Caprari, Ueli Angst, Roland Y Siegwart, Robert J Flatt,and Bernhard Elsener. Climbing robot for corrosion monitoring of reinforced concretestructures. In 2012 2nd International Conference on Applied Robotics for the PowerIndustry (CARPI), pages 10\u201315. IEEE, 2012.[159] Nhan H Pham and Hung M La. Design and implementation of an autonomous robotfor steel bridge inspection. In 2016 54th Annual Allerton Conference on Communi-cation, Control, and Computing (Allerton), pages 556\u2013562. IEEE, 2016.[160] Roya Nasimi, Solomon Atcitty, Dominic Thompson, Joshua Murillo, Marlan Ball,John Stormont, and Fernando Moreu. Use of remote structural tap testing devicesdeployed via ground vehicle for health monitoring of transportation infrastructure.Sensors, 22(4):1458, 2022.[161] Amit Ojha, Yizhi Liu, Shayan Shayesteh, Houtan Jebelli, and William E Sitzabee. Af-fordable multiagent robotic system for same-level fall hazard detection in indoor con-struction environments. Journal of Computing in Civil Engineering, 37(1):04022042,2023.[162] Clearpath Robotics. Umass amherst drives computer vision research with jackal ugv,2019. Accessed on September 6, 2023.[163] Nicholas Charron, Evan McLaughlin, Stephen Phillips, Kevin Goorts, SriramNarasimhan, and Steven L Waslander. Automated bridge inspection using mobileground robotics. Journal of Structural Engineering, 145(11):04019137, 2019.118Bibliography[164] Srijeet Halder, Kereshmeh Afsari, John Serdakowski, Stephen DeVito, Mahnaz Ensafi,and Walid Thabet. Real-time and remote construction progress monitoring with aquadruped robot using augmented reality. Buildings, 12(11):2027, 2022.[165] Sina Tavasoli, Xiao Pan, and TY Yang. Real-time autonomous indoor navigation andvision-based damage assessment of reinforced concrete structures using low-cost nanoaerial vehicles. Journal of Building Engineering, 68:106193, 2023.[166] Xiao Pan, Sina Tavasoli, and TY Yang. Autonomous 3d vision-based bolt loosen-ing assessment using micro aerial vehicles. Computer-Aided Civil and InfrastructureEngineering, 38(17):2443\u20132454, 2023.[167] Barritt Lovelace and J Zink. Unmanned aerial vehicle bridge inspection demonstrationproject. Research Project. Final Report, 40, 2015.[168] Jennifer Wells, Barritt Lovelace, et al. Unmanned aircraft system bridge inspectiondemonstration project phase ii final report. Technical report, Minnesota. Dept. ofTransportation. Research Services & Library, 2017.[169] Andrew P Tomiczek, Jennifer A Bridge, Peter G Ifju, Travis J Whitley, Chad S Tripp,Andrew E Ortega, Jeroen J Poelstra, and Stephen A Gonzalez. Small unmanned aerialvehicle (suav) inspections in gps denied area beneath bridges. In Structures Congress2018, pages 205\u2013216. American Society of Civil Engineers Reston, VA, 2018.[170] Luis Duque, Junwon Seo, and James Wacker. Synthesis of unmanned aerial vehicleapplications for infrastructures. Journal of Performance of Constructed Facilities,32(4):04018046, 2018.[171] D. Kang and Y.J. Cha. Autonomous uavs for structural health monitoring using deeplearning and an ultrasonic beacon system with geo-tagging. Comput.-Aided CivilInfrastruct. Eng., 18:99\u2013107, 2018.[172] Hyunjun Kim, Junhwa Lee, Eunjong Ahn, Soojin Cho, Myoungsu Shin, and Sung-HanSim. Concrete crack identification using a uav incorporating hybrid image processing.Sensors, 17(9):2052, 2017.[173] G Morgenthal and N Hallermann. Quality assessment of unmanned aerial vehicle (uav)based visual inspection of structures. Advances in Structural Engineering, 17(3):289\u2013302, 2014.[174] Hyungchul Yoon, Jaeho Shin, and Billie F Spencer Jr. Structural displacement mea-surement using an unmanned aerial system. Computer-Aided Civil and InfrastructureEngineering, 33(3):183\u2013192, 2018.[175] Fa\u00b4bio Celestino Pereira and Carlos Eduardo Pereira. Embedded image processingsystems for automatic recognition of cracks using uavs. Ifac-PapersOnline, 48(10):16\u201321, 2015.119Bibliography[176] Keunyoung Jang, Namgyu Kim, and Yun-Kyu An. Deep learning\u2013based autonomousconcrete crack evaluation through hybrid image scanning. Structural Health Monitor-ing, 18(5-6):1722\u20131737, 2019.[177] In-Ho Kim, Haemin Jeon, Seung-Chan Baek, Won-Hwa Hong, and Hyung-Jo Jung.Application of crack identification techniques for an aging concrete bridge inspectionusing an unmanned aerial vehicle. Sensors, 18(6):1881, 2018.[178] Dongho Kang and Young-Jin Cha. Damage detection with an autonomous uav usingdeep learning. In Sensors and Smart Structures Technologies for Civil, Mechanical,and Aerospace Systems 2018, volume 10598, pages 7\u201314. SPIE, 2018.[179] Thanh-Canh Huynh, Jae-Hyung Park, Hyung-Jo Jung, and Jeong-Tae Kim. Quasi-autonomous bolt-loosening detection method using vision-based deep learning andimage processing. Automation in Construction, 105:102844, 2019.[180] Muhammad Rakeh Saleem, Jong-Woong Park, Jin-Hwan Lee, Hyung-Jo Jung, andMuhammad Zohaib Sarwar. Instant bridge visual inspection using an unmannedaerial vehicle by image capturing and geo-tagging system and deep convolutionalneural network. Structural Health Monitoring, 20(4):1760\u20131777, 2021.[181] Roya Nasimi, Fernando Moreu, and G Matthew Fricke. Sensor equipped uas fornon-contact bridge inspections: field application. Sensors, 23(1):470, 2023.[182] Krisada Chaiyasarn, Wasif Khan, Luqman Ali, Mayank Sharma, Daniel Brackenbury,and Matthew DeJong. Crack detection in masonry structures using convolutionalneural networks and support vector machines. In ISARC. Proceedings of the Inter-national Symposium on Automation and Robotics in Construction, volume 35, pages1\u20138. IAARC Publications, 2018.[183] Bin Lei, Ning Wang, Pengcheng Xu, and Gangbing Song. New crack detection methodfor bridge inspection using uav incorporating image processing. Journal of AerospaceEngineering, 31(5):04018058, 2018.[184] Diogo Duarte, Francesco Nex, Norman Kerle, and George Vosselman. Multi-resolutionfeature fusion for image classification of building damages with convolutional neuralnetworks. Remote sensing, 10(10):1636, 2018.[185] Vedhus Hoskere, Jong-Woong Park, Hyungchul Yoon, and Billie F Spencer Jr. Vision-based modal survey of civil infrastructure using unmanned aerial vehicles. Journal ofStructural Engineering, 145(7):04019062, 2019.[186] Kasthurirangan Gopalakrishnan. Deep learning in data-driven pavement image anal-ysis and automated distress detection: A review. Data, 3(3):28, 2018.[187] Yang Zhang, Xiaowei Sun, Kenneth J Loh, Wensheng Su, Zhigang Xue, and XuefengZhao. Autonomous bolt loosening detection using deep learning. Structural HealthMonitoring, 19(1):105\u2013122, 2020.120Bibliography[188] Gustavo H Beckman, Dimos Polyzois, and Young-Jin Cha. Deep learning-based au-tomatic volumetric damage quantification using depth camera. Automation in Con-struction, 99:114\u2013124, 2019.[189] Xuefeng Zhao, Ruicong Han, Yan Yu, Weitong Hu, Dong Jiao, Xingquan Mao,Mingchu Li, and Jinping Ou. Smartphone-based mobile testing technique for quickbridge cable\u2013force measurement. Journal of Bridge Engineering, 22(4):06016012,2017.[190] Shengyuan Li and Xuefeng Zhao. Convolutional neural networks-based crack detec-tion for real concrete surface. In Sensors and Smart Structures Technologies for Civil,Mechanical, and Aerospace Systems 2018, volume 10598, pages 955\u2013961. SPIE, 2018.[191] Shengyuan Li, Xuefeng Zhao, et al. Image-based concrete crack detection using con-volutional neural network and exhaustive search technique. Advances in civil engi-neering, 2019, 2019.[192] Niannian Wang, Xuefeng Zhao, Peng Zhao, Yang Zhang, Zheng Zou, and JinpingOu. Automatic damage detection of historic masonry buildings based on mobile deeplearning. Automation in Construction, 103:53\u201366, 2019.[193] Leo Pauly, D Hogg, R Fuentes, and H Peel. Deeper networks for pavement crackdetection. In Proceedings of the 34th ISARC, pages 479\u2013485. IAARC, 2017.[194] Hiroya Maeda, Yoshihide Sekimoto, Toshikazu Seto, Takehiro Kashiyama, and Hi-roshi Omata. Road damage detection and classification using deep neural networkswith smartphone images. Computer-Aided Civil and Infrastructure Engineering,33(12):1127\u20131141, 2018.[195] Nenad Gucunski, S Kee, Hung La, Basily Basily, and Ali Maher. Delamination andconcrete quality assessment of concrete bridge decks using a fully autonomous rabitplatform. Structural Monitoring and Maintenance, 2(1):19\u201334, 2015.[196] Hung Manh La, Nenad Gucunski, Seong-Hoon Kee, Jingang Yi, Turgay Senlet, andLuan Nguyen. Autonomous robotic system for bridge deck data collection and analy-sis. In 2014 IEEE\/RSJ International Conference on Intelligent Robots and Systems,pages 1950\u20131955. IEEE, 2014.[197] Ronny Salim Lim, Hung Manh La, and Weihua Sheng. A robotic crack inspectionand mapping system for bridge deck maintenance. IEEE Transactions on AutomationScience and Engineering, 11(2):367\u2013378, 2014.[198] Tuan Le, Spencer Gibb, Nhan Pham, Hung Manh La, Logan Falk, and Tony Berend-sen. Autonomous robotic system using non-destructive evaluation methods for bridgedeck inspection. In 2017 IEEE International Conference on Robotics and Automation(ICRA), pages 3672\u20133677. IEEE, 2017.[199] Aaron Sirken, Gedaliah Knizhnik, Jessica McWilliams, and Sarah Bergbreiter. Bridgerisk investigation diagnostic grouped exploratory (bridge) bot. In 2017 IEEE\/RSJ121BibliographyInternational Conference on Intelligent Robots and Systems (IROS), pages 6526\u20136532.IEEE, 2017.[200] Son Thanh Nguyen and Hung Manh La. Development of a steel bridge climbingrobot. In 2019 IEEE\/RSJ International Conference on Intelligent Robots and Systems(IROS), pages 1912\u20131917. IEEE, 2019.[201] Kyeong Ho Cho, Young Hoon Jin, Ho Moon Kim, Hyungpil Moon, Ja Choon Koo, andHyouk Ryeol Choi. Caterpillar-based cable climbing robot for inspection of suspensionbridge hanger rope. In 2013 IEEE International Conference on Automation Scienceand Engineering (CASE), pages 1059\u20131062. IEEE, 2013.[202] Nicholas Morozovsky and Thomas Bewley. Skysweeper: A low dof, dynamic highwire robot. In 2013 IEEE\/RSJ International Conference on Intelligent Robots andSystems, pages 2339\u20132344. IEEE, 2013.[203] Xinke Li, Chao Gao, Yongcai Guo, Fuliang He, and Yanhua Shao. Cable surface dam-age detection in cable-stayed bridges using optical techniques and image mosaicking.Optics & Laser Technology, 110:36\u201343, 2019.[204] Zhenliang Zheng and Ning Ding. Design and implementation of ccrobot-ii: a palm-based cable climbing robot for cable-stayed bridge inspection. In 2019 InternationalConference on Robotics and Automation (ICRA), pages 9747\u20139753. IEEE, 2019.[205] Kyeong Ho Cho, Young Hoon Jin, Ho Moon Kim, Hyungpil Moon, Ja Choon Koo,and Hyouk Ryeol Choi. Multifunctional robotic crawler for inspection of suspensionbridge hanger cables: Mechanism design and performance validation. IEEE\/ASMETransactions on Mechatronics, 22(1):236\u2013246, 2016.[206] Antonio E Jimenez-Cano, Pedro J Sanchez-Cuevas, Pedro Grau, Anibal Ollero, andGuillermo Heredia. Contact-based bridge inspection multirotors: Design, modeling,and control considering the ceiling effect. IEEE Robotics and Automation Letters,4(4):3561\u20133568, 2019.[207] AE Jimenez-Cano, J Braga, Guillermo Heredia, and An\u00b4\u0131bal Ollero. Aerial manip-ulator for structure inspection by contact from the underside. In 2015 IEEE\/RSJinternational conference on intelligent robots and systems (IROS), pages 1879\u20131884.IEEE, 2015.[208] Pedro Jesus Sanchez-Cuevas, Guillermo Heredia, and Anibal Ollero. Multirotor uas forbridge inspection by contact using the ceiling effect. In 2017 International Conferenceon Unmanned Aircraft Systems (ICUAS), pages 767\u2013774. IEEE, 2017.[209] Kotaro Asa, Yuki Funabora, Shinji Doki, and Kae Doki. Measuring position deter-mination for accurate and efficient visual inspection using uav. In 2017 IEEE\/SICEInternational Symposium on System Integration (SII), pages 188\u2013193. IEEE, 2017.[210] Akihiko Ichikawa, Yuki Abe, Takahiro Ikeda, Kenichi Ohara, Jyunpei Kishikawa,Satoshi Ashizawa, Takeo Oomichi, Akihisa Okino, and Toshio Fukuda. Uav with122Bibliographymanipulator for bridge inspection\u2014hammering system for mounting to uav. In 2017IEEE\/SICE International Symposium on System Integration (SII), pages 775\u2013780.IEEE, 2017.[211] Sungwook Jung, Seungwon Song, Sanghyeon Kim, Junho Park, Jongbeom Her,Kyungshik Roh, and Hyun Myung. Toward autonomous bridge inspection: A frame-work and experimental results. In 2019 16th International Conference on UbiquitousRobots (UR), pages 208\u2013211. IEEE, 2019.[212] Junwon Seo, Luis Duque, and Jim Wacker. Drone-enabled bridge inspection method-ology and application. Automation in construction, 94:112\u2013126, 2018.[213] Robert Ladig and Kazuhiro Shimonomura. High precision marker based localizationand movement on the ceiling employing an aerial robot with top mounted omni wheeldrive system. In 2016 IEEE\/RSJ International Conference on Intelligent Robots andSystems (IROS), pages 3081\u20133086. IEEE, 2016.[214] Andrew Ellenberg, Ahsas Kontsos, Franklin Moon, and Ivan Bartoli. Bridge deck de-lamination identification from unmanned aerial vehicle infrared imagery. Automationin Construction, 72:155\u2013165, 2016.[215] James E DeVault. Robotic system for underwater inspection of bridge piers. IEEEInstrumentation & Measurement Magazine, 3(3):32\u201337, 2000.[216] Robin R Murphy, Eric Steimle, Michael Hall, Michael Lindemuth, David Trejo, StefanHurlebaus, Zenon Medina-Cetina, and Daryl Slocum. Robot-assisted bridge inspec-tion. Journal of Intelligent & Robotic Systems, 64(1):77\u201395, 2011.[217] Christian A Mueller, Tobias Fromm, Heiko Buelow, Andreas Birk, Maximilian Garsch,and Norbert Gebbeken. Robotic bridge inspection within strategic flood evacuationplanning. In OCEANS 2017-Aberdeen, pages 1\u20136. IEEE, 2017.[218] Wael Doghri, Ahlem Saddoud, and Lamia Chaari Fourati. Cyber-physical systemsfor structural health monitoring: sensing technologies and intelligent computing. TheJournal of Supercomputing, 78(1):766\u2013809, 2022.[219] Khashayar Asadi, Akshay Kalkunte Suresh, Alper Ender, Siddhesh Gotad, SurajManiyar, Smit Anand, Mojtaba Noghabaei, Kevin Han, Edgar Lobaton, and TianfuWu. An integrated ugv-uav system for construction site data collection. Automationin Construction, 112:103068, 2020.[220] Kumar Ankit, Lima Agnel Tony, Shuvrangshu Jana, and Debasish Ghose. Multi-agentcooperative framework for autonomous wall construction. In Congress on IntelligentSystems: Proceedings of CIS 2021, Volume 1, pages 877\u2013894. Springer, 2022.[221] Kyoungah Choi, Impyeong Lee, Juseok Hong, Taewan Oh, and Sung Woong Shin.Developing a uav-based rapid mapping system for emergency response. In UnmannedSystems Technology XI, volume 7332, pages 75\u201386. SPIE, 2009.123Bibliography[222] Alessandro Sabato, Christopher Niezrecki, Shweta Dabetwar, Nitin Nagesh Kulkarni,Fabio Bottalico, and Tymon Nieduzak. Advancements in structural health monitor-ing using combined computer-vision and unmanned aerial vehicles approaches. InEuropean Workshop on Structural Health Monitoring, pages 417\u2013426. Springer, 2022.[223] Siyuan Chen, Debra F Laefer, Eleni Mangina, SM Iman Zolanvari, and JonathanByrne. Uav bridge inspection through evaluated 3d reconstructions. Journal of BridgeEngineering, 24(4):05019001, 2019.[224] Difeng Hu, Vincent JL Gan, Tao Wang, and Ling Ma. Multi-agent robotic system(mars) for uav-ugv path planning and automatic sensory data collection in clutteredenvironments. Building and Environment, 221:109349, 2022.[225] Y Uny Cao, Andrew B Kahng, and Alex S Fukunaga. Cooperative mobile robotics:Antecedents and directions. Robot colonies, pages 7\u201327, 1997.[226] Barbara Arbanas, Antun Ivanovic, Marko Car, Matko Orsag, Tamara Petrovic, andStjepan Bogdan. Decentralized planning and control for uav\u2013ugv cooperative teams.Autonomous Robots, 42(8):1601\u20131618, 2018.[227] L Cantelli, M Lo Presti, M Mangiameli, CD Melita, and G Muscato. Autonomous co-operation between uav and ugv to improve navigation and environmental monitoringin rough environments. In 10th International Symposium on Humanitarian Deminingcoupled with the 11th IARP WS HUDEM, volume 23, 2013.[228] Shannon Hood, Kelly Benson, Patrick Hamod, Daniel Madison, Jason M O\u2019Kane, andIoannis Rekleitis. Bird\u2019s eye view: Cooperative exploration by ugv and uav. In 2017International Conference on Unmanned Aircraft Systems (ICUAS), pages 247\u2013255.IEEE, 2017.[229] Martin Saska, Tomas Krajnik, and Libor Pfeucil. Cooperative \u00b5uav-ugv autonomousindoor surveillance. In International Multi-Conference on Systems, Signals & Devices,pages 1\u20136. IEEE, 2012.[230] Christian Forster, Matia Pizzoli, and Davide Scaramuzza. Air-ground localizationand map augmentation using monocular dense reconstruction. In 2013 IEEE\/RSJInternational Conference on Intelligent Robots and Systems, pages 3971\u20133978. IEEE,2013.[231] Kevin Yu, Ashish Kumar Budhiraja, and Pratap Tokekar. Algorithms for routing ofunmanned aerial vehicles with mobile recharging stations. In 2018 IEEE internationalconference on robotics and automation (ICRA), pages 5720\u20135725. IEEE, 2018.[232] Pablo R Palafox, Mario Garzo\u00b4n, Joa\u02dco Valente, Juan Jesu\u00b4s Rolda\u00b4n, and Antonio Bar-rientos. Robust visual-aided autonomous takeoff, tracking, and landing of a small uavon a moving landing platform for life-long operation. Applied Sciences, 9(13):2661,2019.124Bibliography[233] Alvaro Cantieri, Matheus Ferraz, Guido Szekir, Marco Anto\u02c6nio Teixeira, Jose\u00b4 Lima,Andre\u00b4 Schneider Oliveira, and Marco Aure\u00b4lio Wehrmeister. Cooperative uav\u2013ugvautonomous power pylon inspection: An investigation of cooperative outdoor vehiclepositioning architecture. Sensors, 20(21):6384, 2020.[234] Vin\u00b4\u0131cius Pacheco Bacheti, Alexandre Santos Branda\u02dco, and Ma\u00b4rio Sarcinelli-Filho.Leader-follower ugv-uav formation as control paradigm for package delivery. In 2022International Conference on Unmanned Aircraft Systems (ICUAS), pages 772\u2013778.IEEE, 2022.[235] Alexander Thoms, Gabriel Earle, Nicholas Charron, and Sriram Narasimhan. Tightlycoupled, graph-based dvl\/imu fusion and decoupled mapping for slam-centric mar-itime infrastructure inspection. IEEE Journal of Oceanic Engineering, 2023.[236] Giorgio Grisetti, Cyrill Stachniss, and Wolfram Burgard. Improved techniques forgrid mapping with rao-blackwellized particle filters. IEEE transactions on Robotics,23(1):34\u201346, 2007.[237] Bruno Steux and Oussama El Hamzaoui. tinyslam: A slam algorithm in less than200 lines c-language program. In 2010 11th International Conference on ControlAutomation Robotics & Vision, pages 1975\u20131979. IEEE, 2010.[238] Stefan Kohlbrecher, Oskar Von Stryk, Johannes Meyer, and Uwe Klingauf. A flexibleand scalable slam system with full 3d motion estimation. In 2011 IEEE internationalsymposium on safety, security, and rescue robotics, pages 155\u2013160. IEEE, 2011.[239] F. Pomerleau, F. Colas, R. Siegwart, and S. Magnenat. Comparing icp variants onreal-world data sets. Autonomous Robots, 34(3):133\u2013148, April 2013.[240] Regis Vincent, Benson Limketkai, and Michael Eriksen. Comparison of indoor robotlocalization techniques in the absence of gps. In Detection and sensing of mines,explosive objects, and obscured targets XV, volume 7664, pages 606\u2013610. SPIE, 2010.[241] Luca Carlone, Rosario Aragues, Jose\u00b4 A Castellanos, and Basilio Bona. A linearapproximation for graph-based simultaneous localization and mapping. In Robotics:Science and Systems, volume 7, pages 41\u201348. MIT Press Cambridge, MA, USA, 2012.[242] Wolfgang Hess, Damon Kohler, Holger Rapp, and Daniel Andor. Real-time loopclosure in 2d lidar slam. In 2016 IEEE international conference on robotics andautomation (ICRA), pages 1271\u20131278. IEEE, 2016.[243] Eric Nelson. Berkeley localization and mapping (blam). https:\/\/github.com\/erik-nelson\/blam, 2016.[244] Frank Dellaert. Factor graphs and gtsam: A hands-on introduction. Georgia Instituteof Technology, Tech. Rep, 2:4, 2012.[245] Renaud Dube\u00b4, Daniel Dugas, Elena Stumm, Juan Nieto, Roland Siegwart, and CesarCadena. Segmatch: Segment based loop-closure for 3d point clouds. arXiv preprintarXiv:1609.07720, 2016.125Bibliography[246] Renaud Dube\u00b4, Abel Gawel, Hannes Sommer, Juan Nieto, Roland Siegwart, and CesarCadena. An online multi-robot slam system for 3d lidars. In 2017 IEEE\/RSJ Interna-tional Conference on Intelligent Robots and Systems (IROS), pages 1004\u20131011. IEEE,2017.[247] Mathieu Labbe\u00b4 and Franc\u00b8ois Michaud. Rtab-map as an open-source lidar and visualsimultaneous localization and mapping library for large-scale and long-term onlineoperation. Journal of field robotics, 36(2):416\u2013446, 2019.[248] Michael Zollho\u00a8fer, Patrick Stotko, Andreas Go\u00a8rlitz, Christian Theobalt, MatthiasNie\u00dfner, Reinhard Klein, and Andreas Kolb. State of the art on 3d reconstructionwith rgb-d cameras. In Computer graphics forum, volume 37, pages 625\u2013652. WileyOnline Library, 2018.[249] Georg Klein and David Murray. Parallel tracking and mapping for small arworkspaces. In 2007 6th IEEE and ACM international symposium on mixed andaugmented reality, pages 225\u2013234. IEEE, 2007.[250] Christian Forster, Matia Pizzoli, and Davide Scaramuzza. Svo: Fast semi-directmonocular visual odometry. In 2014 IEEE international conference on robotics andautomation (ICRA), pages 15\u201322. IEEE, 2014.[251] JAKOB ENGEL, T Scho\u00a8ps, and D Cremers. Lsd-slam: Largescale direct monocularslam [c] proceedings of the 13th european conference on computer vision, 2014.[252] Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan D Tardos. Orb-slam: a versa-tile and accurate monocular slam system. IEEE transactions on robotics, 31(5):1147\u20131163, 2015.[253] Yi Lin, Fei Gao, Tong Qin, Wenliang Gao, Tianbo Liu, William Wu, Zhenfei Yang, andShaojie Shen. Autonomous aerial navigation using monocular visual-inertial fusion.Journal of Field Robotics, 35(1):23\u201351, 2018.[254] Thomas Schneider, Marcin Dymczyk, Marius Fehr, Kevin Egger, Simon Lynen, IgorGilitschenski, and Roland Siegwart. maplab: An open framework for research invisual-inertial mapping and localization. IEEE Robotics and Automation Letters,3(3):1418\u20131425, 2018.[255] Mathieu Labbe and Francois Michaud. Appearance-based loop closure detectionfor online large-scale and long-term operation. IEEE Transactions on Robotics,29(3):734\u2013745, 2013.[256] Shayan Shayesteh and Houtan Jebelli. Toward human-in-the-loop constructionrobotics: Understanding workers\u2019 response through trust measurement during human-robot collaboration. In Construction Research Congress 2022, pages 631\u2013639, 2022.[257] Shayan Shayesteh, Amit Ojha, and Houtan Jebelli. Workers\u2019 trust in collaborativeconstruction robots: Eeg-based trust recognition in an immersive environment. Au-tomation and robotics in the architecture, engineering, and construction industry,pages 201\u2013215, 2022.126Bibliography[258] Kun Hu, Hazhir Rahmandad, Tonya Smith-Jackson, and Woodrow Winchester. Fac-tors influencing the risk of falls in the construction industry: a review of the evidence.Construction Management and Economics, 29(4):397\u2013416, 2011.[259] Youngcheol Kang, Sohaib Siddiqui, Sung Joon Suk, Seokho Chi, and Changwan Kim.Trends of fall accidents in the us construction industry. Journal of ConstructionEngineering and Management, 143(8):04017043, 2017.[260] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprintarXiv:1810.04805, 2018.[261] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, PrafullaDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, and Amanda Askell.Language models are few-shot learners. Advances in neural information processingsystems, 33:1877\u20131901, 2020.[262] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de OliveiraPinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, and GregBrockman. Evaluating large language models trained on code. arXiv preprintarXiv:2107.03374, 2021.[263] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-mine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, and ShrutiBhosale. Llama 2: Open foundation and fine-tuned chat models. arXiv preprintarXiv:2307.09288, 2023.[264] Sai Vemprala, Rogerio Bonatti, Arthur Bucker, and Ashish Kapoor. Chatgpt forrobotics: Design principles and model abilities. 2023.[265] OpenAI. Chatgpt (august 3 version). Large language model, 2023.[266] Sepp Hochreiter and Ju\u00a8rgen Schmidhuber. Long short-term memory. Neural compu-tation, 9(8):1735\u20131780, 1997.[267] Sneha Chaudhari, Varun Mithal, Gungor Polatkan, and Rohan Ramanath. An at-tentive survey of attention models. ACM Transactions on Intelligent Systems andTechnology (TIST), 12(5):1\u201332, 2021.[268] Alana de Santana Correia and Esther Luna Colombini. Attention, please! a surveyof neural attention models in deep learning. arXiv preprint arXiv:2103.16775, 2021.[269] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature,521(7553):436, 2015.[270] M. Azimi and T.Y. Yang. Transformer-based framework for accurate segmentationof high-resolution images in structural health monitoring. Computer-Aided Civil andInfrastructure Engineering, 2024.127Bibliography[271] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisitingunreasonable effectiveness of data in deep learning era. In Proceedings of the IEEEinternational conference on computer vision, pages 843\u2013852, 2017.[272] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablay-rolles, and Herve\u00b4 Je\u00b4gou. Training data-efficient image transformers & distillationthrough attention. In International conference on machine learning, pages 10347\u201310357. PMLR, 2021.[273] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, OmerLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustlyoptimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.[274] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attentiongenerative adversarial networks. In International conference on machine learning,pages 7354\u20137363. PMLR, 2019.[275] Carole H Sudre, Wenqi Li, Tom Vercauteren, Sebastien Ourselin, and M Jorge Car-doso. Generalised dice overlap as a deep learning loss function for highly unbalancedsegmentations. In Deep Learning in Medical Image Analysis and Multimodal Learningfor Clinical Decision Support: Third International Workshop, DLMIA 2017, and 7thInternational Workshop, ML-CDS 2017, Held in Conjunction with MICCAI 2017,Que\u00b4bec City, QC, Canada, September 14, Proceedings 3, pages 240\u2013248. Springer.[276] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dolla\u00b4r. Focal lossfor dense object detection. In Proceedings of the IEEE international conference oncomputer vision, pages 2980\u20132988.[277] Rahmat Ali, Dharshan Lokekere Gopal, and Young-Jin Cha. Vision-based concretecrack detection technique using cascade features. In Sensors and Smart StructuresTechnologies for Civil, Mechanical, and Aerospace Systems 2018, volume 10598, page105980L. International Society for Optics and Photonics.[278] Ho Kei Cheng, Jihoon Chung, Yu-Wing Tai, and Chi-Keung Tang. Cascadepsp: To-ward class-agnostic and very high-resolution segmentation via global and local refine-ment. In Proceedings of the IEEE\/CVF Conference on Computer Vision and PatternRecognition, pages 8890\u20138899, 2020.[279] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learningfor image recognition. In Proceedings of the IEEE conference on computer vision andpattern recognition, pages 770\u2013778, 2016.[280] Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmannmachines. In Proceedings of the 27th international conference on machine learning(ICML-10), pages 807\u2013814.[281] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang,Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Au-tomatic differentiation in pytorch. 2017.128Bibliography[282] Nick Kanopoulos, Nagesh Vasanthavada, and Robert L Baker. Design of an imageedge detection filter using the sobel operator. IEEE Journal of solid-state circuits,23(2):358\u2013367, 1988.[283] Russell Stewart, Mykhaylo Andriluka, and Andrew Y Ng. End-to-end people detectionin crowded scenes. In Proceedings of the IEEE conference on computer vision andpattern recognition, pages 2325\u20132333, 2016.[284] Yong Shi, Limeng Cui, Zhiquan Qi, Fan Meng, and Zhensong Chen. Automatic roadcrack detection using random structured forests. IEEE Transactions on IntelligentTransportation Systems, 17(12):3434\u20133445, 2016.[285] Eric Bianchi and Matthew Hebdon. Structural material semantic segmentationdataset, 2021.[286] Jianping Shi, Qiong Yan, Li Xu, and Jiaya Jia. Hierarchical image saliency detectionon extended cssd. IEEE transactions on pattern analysis and machine intelligence,38(4):717\u2013729, 2015.[287] Ming-Ming Cheng, Niloy J Mitra, Xiaolei Huang, Philip HS Torr, and Shi-Min Hu.Global contrast based salient region detection. IEEE transactions on pattern analysisand machine intelligence, 37(3):569\u2013582, 2014.[288] Xiang Li, Tianhan Wei, Yau Pun Chen, Yu-Wing Tai, and Chi-Keung Tang. Fss-1000:A 1000-class dataset for few-shot segmentation. In Proceedings of the IEEE\/CVFconference on computer vision and pattern recognition, pages 2869\u20132878.[289] Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, and Ming-Hsuan Yang. Saliencydetection via graph-based manifold ranking. In Proceedings of the IEEE conferenceon computer vision and pattern recognition, pages 3166\u20133173, 2013.[290] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization.arXiv preprint arXiv:1412.6980, 2014.[291] Kaiming He, Georgia Gkioxari, Piotr Dolla\u00b4r, and Ross Girshick. Mask r-cnn. InProceedings of the IEEE international conference on computer vision, pages 2961\u20132969, 2017.[292] Gary Bradski and Adrian Kaehler. Learning OpenCV: Computer vision with theOpenCV library. \u201d O\u2019Reilly Media, Inc.\u201d, 2008.[293] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networksfor biomedical image segmentation. In International Conference on Medical imagecomputing and computer-assisted intervention, pages 234\u2013241. Springer.[294] Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, and JianmingLiang. Unet++: A nested u-net architecture for medical image segmentation. In DeepLearning in Medical Image Analysis and Multimodal Learning for Clinical DecisionSupport: 4th International Workshop, DLMIA 2018, and 8th International Workshop,129BibliographyML-CDS 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September20, 2018, Proceedings 4, pages 3\u201311. Springer, 2018.[295] Tongle Fan, Guanglei Wang, Yan Li, and Hongrui Wang. Ma-net: A multi-scaleattention network for liver and tumor segmentation. IEEE Access, 8:179656\u2013179665,2020.[296] Abhishek Chaurasia and Eugenio Culurciello. Linknet: Exploiting encoder represen-tations for efficient semantic segmentation. In 2017 IEEE visual communications andimage processing (VCIP), pages 1\u20134. IEEE, 2017.[297] Alexander Kirillov, Kaiming He, Ross Girshick, and Piotr Dolla\u00b4r. A unified architec-ture for instance and semantic segmentation. In CVPR, 2017.[298] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr Dolla\u00b4r. Panoptic featurepyramid networks. In Proceedings of the IEEE\/CVF conference on computer visionand pattern recognition, pages 6399\u20136408, 2019.[299] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and HartwigAdam. Encoder-decoder with atrous separable convolution for semantic image seg-mentation. In Proceedings of the European conference on computer vision (ECCV),pages 801\u2013818, 2018.[300] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutionalneural networks. In International conference on machine learning, pages 6105\u20136114.PMLR.[301] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, WeijunWang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Effi-cient convolutional neural networks for mobile vision applications. arXiv preprintarXiv:1704.04861, 2017.[302] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-ChiehChen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of theIEEE conference on computer vision and pattern recognition, pages 4510\u20134520, 2018.[303] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve\u00b4 Je\u00b4gou. Fixing the train-test resolution discrepancy. Advances in neural information processing systems, 32,2019.[304] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.[305] ASM Shihavuddin and Xiao Chen. Dtu-drone inspection images of wind turbine.2018.[306] Carlo Tomasi and Takeo Kanade. Detection and tracking of point features. Int JComput Vis, 9:137\u2013154, 1991.130Bibliography[307] Xiao Pan, TY Yang, Yifei Xiao, Hongcan Yao, and Hojjat Adeli. Vision-based real-time structural vibration measurement through deep-learning-based detection andtracking methods. Engineering Structures, 281:115676, 2023.[308] Sajad Javadinasab Hormozabad, Alejandro Palacio, and Mariantonieta Gutier-rez Soto. Real-time floor tracking, 2022.[309] Ahmad M Alshorman, Omar Alshorman, Muhammad Irfan, Adam Glowacz, FazalMuhammad, and Wahyu Caesarendra. Fuzzy-based fault-tolerant control for omnidi-rectional mobile robot. Machines, 8(3):55, 2020.[310] Hamza Mahdi. Developing a semi-autonomous robot to engage children with specialneeds and their peers in robot-assisted play. Master\u2019s thesis, University of Waterloo,2021.[311] Han Wang, Chen Wang, and Lihua Xie. Lightweight 3-d localization and mappingfor solid-state lidar. IEEE Robotics and Automation Letters, 6(2):1801\u20131807, 2021.[312] Jeremy Castagno and Ella Atkins. Polylidar3d-fast polygon extraction from 3d data.Sensors, 20(17):4819, 2020.[313] Osamu Takahashi and Robert J Schilling. Motion planning in a plane using generalizedvoronoi diagrams. IEEE Transactions on robotics and automation, 5(2):143\u2013150, 1989.[314] Steven LaValle. Rapidly-exploring random trees: A new tool for path planning.Research Report 9811, 1998.[315] Sertac Karaman and Emilio Frazzoli. Sampling-based algorithms for optimal motionplanning. The international journal of robotics research, 30(7):846\u2013894, 2011.[316] Dorin Comaniciu and Peter Meer. Mean shift: A robust approach toward feature spaceanalysis. IEEE Transactions on pattern analysis and machine intelligence, 24(5):603\u2013619, 2002.[317] Shikuan Yu, Fei Yan, Yan Zhuang, and Dongbing Gu. A deep-learning-based strategyfor kidnapped robot problem in similar indoor environment. Journal of Intelligent &Robotic Systems, 100:765\u2013775, 2020.[318] Rauf Yagfarov, Mikhail Ivanou, and Ilya Afanasyev. Map comparison of lidar-based 2dslam algorithms using precise ground truth. In 2018 15th International Conferenceon Control, Automation, Robotics and Vision (ICARCV), pages 1979\u20131983. IEEE,2018.[319] Doris M Turnage. Simulation results for localization and mapping algorithms. In2016 Winter Simulation Conference (WSC), pages 3040\u20133051. IEEE, 2016.[320] Zuo-syuan Wang. Implementation of Odometry with EKF for Localization of HectorSLAM Method. Thesis, 2016.131Bibliography[321] Hyeon Cheol Jo, Hong-Gyoo Sohn, and Yun Mook Lim. A lidar point cloud data-basedmethod for evaluating strain on a curved steel plate subjected to lateral pressure.Sensors, 20(3):721, 2020.[322] Ji Zhang and Sanjiv Singh. Loam: Lidar odometry and mapping in real-time. InRobotics: Science and systems, volume 2, pages 1\u20139. Berkeley, CA, 2014.[323] S. Shivaram. Structural health monitoring of wind turbine blades using unmannedair vehicles. Master\u2019s thesis, Trinity College Dublin, Dublin, Ireland, 2015. Accessedon 30 August 2023.[324] Matthew J Westoby, James Brasington, Niel F Glasser, Michael J Hambrey, andJennifer M Reynolds. \u2018structure-from-motion\u2019photogrammetry: A low-cost, effectivetool for geoscience applications. Geomorphology, 179:300\u2013314, 2012.[325] Richard Szeliski. Computer vision: algorithms and applications. Springer Nature,2022.[326] Noah Snavely, Steven M Seitz, and Richard Szeliski. Modeling the world from internetphoto collections. International journal of computer vision, 80:189\u2013210, 2008.[327] David G Lowe. Distinctive image features from scale-invariant keypoints. Interna-tional journal of computer vision, 60:91\u2013110, 2004.[328] Fang Zhu, Shuai Guo, Li Song, Ke Xu, and Jiayu Hu. Deep review and analysisof recent nerfs. APSIPA Transactions on Signal and Information Processing, 12(1),2023.[329] Thomas Mu\u00a8ller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neuralgraphics primitives with a multiresolution hash encoding. ACM Transactions onGraphics (ToG), 41(4):1\u201315, 2022.[330] Ntv drone kameras\u0131: I\u02d9lan\u0131nda \u201ddepreme dayan\u0131kl\u0131\u201d yaz\u0131yordu, 2023. Retrieved April7, 2023.[331] Christian Knauer, Maarten Lo\u00a8ffler, Marc Scherfenberg, and Thomas Wolle. The di-rected hausdorff distance between imprecise point sets. Theoretical Computer Science,412(32):4173\u20134186, 2011.[332] Matthew Walter, Franz Hover, and John Leonard. Slam for ship hull inspection usingexactly sparse extended information filters. In 2008 IEEE international conferenceon robotics and automation, pages 1463\u20131470. IEEE, 2008.[333] Xiao Pan and T. Y. Yang. 3d vision-based bolt loosening assessment using photogram-metry, deep neural networks, and 3d point-cloud processing. Journal of BuildingEngineering, 70:106326, 2023.[334] AJ Lacey, N Pinitkarn, and Neil A Thacker. An evaluation of the performance ofransac algorithms for stereo camera calibrarion. In BMVC, pages 1\u201310. Citeseer,2000.132Bibliography[335] Martin Ester, Hans-Peter Kriegel, Jo\u00a8rg Sander, Xiaowei Xu, et al. A density-basedalgorithm for discovering clusters in large spatial databases with noise. In kdd, vol-ume 96, pages 226\u2013231, 1996.[336] Kamran Khan, Saif Ur Rehman, Kamran Aziz, Simon Fong, and Sababady Saras-vady. Dbscan: Past, present and future. In The fifth international conference onthe applications of digital information and web technologies (ICADIWT 2014), pages232\u2013238. IEEE, 2014.[337] Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. Open3d: A modern library for 3ddata processing. arXiv preprint arXiv:1801.09847, 2018.[338] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learningon point sets for 3d classification and segmentation. In Proceedings of the IEEEconference on computer vision and pattern recognition, pages 652\u2013660, 2017.[339] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deephierarchical feature learning on point sets in a metric space. Advances in neuralinformation processing systems, 30, 2017.[340] Anass Yarroudh. Lidar automatic unsupervised segmentation using segment-anythingmodel (sam) from meta ai. GitHub Repository, 2023.[341] Wuming Zhang, Jianbo Qi, Peng Wan, Hongtao Wang, Donghui Xie, Xiaoyan Wang,and Guangjian Yan. An easy-to-use airborne lidar data filtering method based oncloth simulation. Remote sensing, 8(6):501, 2016.[342] OpenAI. Chatgpt (august 3 version) [large language model], 2023.[343] Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. Reward designwith language models. arXiv preprint arXiv:2303.00001, 2023.[344] Stefanie Tellex, Nakul Gopalan, Hadas Kress-Gazit, and Cynthia Matuszek. Robotsthat use language. Annual Review of Control, Robotics, and Autonomous Systems,3:25\u201355, 2020.[345] Justin Fu, Anoop Korattikara, Sergey Levine, and Sergio Guadarrama. From languageto goals: Inverse reinforcement learning for vision-based instruction following. arXivpreprint arXiv:1902.07742, 2019.[346] Yicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-Opazo, and Stephen Gould. Arecurrent vision-and-language bert for navigation. arXiv preprint arXiv:2011.13922,2020.[347] Arthur Bucker, Luis Figueredo, Sami Haddadinl, Ashish Kapoor, Shuang Ma, andRogerio Bonatti. Reshaping robot trajectories using natural language commands: Astudy of multi-modal data alignment using transformers. In 2022 IEEE\/RSJ Interna-tional Conference on Intelligent Robots and Systems (IROS), pages 978\u2013984. IEEE,2022.133Bibliography[348] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, YanjunChen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. Vima: General robotmanipulation with multimodal prompts. arXiv preprint arXiv:2210.03094, 2022.[349] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, PeteFlorence, and Andy Zeng. Code as policies: Language model programs for embodiedcontrol. arXiv preprint arXiv:2209.07753, 2022.[350] Hongmei Mary He. Robotgpt: From chatgpt to robot intelligence. 2023.[351] Zaid Abbas Al-Sabbag, Chul Min Yeum, and Sriram Narasimhan. Enabling human\u2013machine collaboration in infrastructure inspections through mixed reality. AdvancedEngineering Informatics, 53:101709, 2022.[352] Sunwoong Choi, Zaid Abbas Al-Sabbag, Sriram Narasimhan, and Chul Min Yeum.Gaze-based human-robot interaction system for infrastructure inspections. arXivpreprint arXiv:2403.08061, 2024.[353] Francesco Giuliari, Irtiza Hasan, Marco Cristani, and Fabio Galasso. Transformernetworks for trajectory forecasting. In 2020 25th international conference on patternrecognition (ICPR), pages 10335\u201310342. IEEE.[354] Mohsen Azimi, Sina Tavasoli, Kasra Jabbari, and Yang T.Y. Integrated roboticssystem for autonomous mapping and 3d scene reconstruction in structural healthmonitoring applications. In Canadian Conference - Pacific Conference on EarthquakeEngineering, 2023.[355] Rogerio Bonatti, Sai Vemprala, Shuang Ma, Felipe Frujeri, Shuhang Chen, and AshishKapoor. Pact: Perception-action causal transformer for autoregressive robotics pre-training. arXiv preprint arXiv:2209.11133, 2022.[356] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, ByronDavid, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, and Alex Herzog.Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprintarXiv:2204.01691, 2022.[357] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis,Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, and Jas-mine Hsu. Rt-1: Robotics transformer for real-world control at scale. arXiv preprintarXiv:2212.06817, 2022.[358] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, JonathanTremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. Progprompt: Gen-erating situated robot task plans using large language models. arXiv preprintarXiv:2209.11302, 2022.[359] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language mod-els as zero-shot planners: Extracting actionable knowledge for embodied agents. InInternational Conference on Machine Learning, pages 9118\u20139147. PMLR, 2022.134[360] Pileun Kim, Jingdao Chen, Jitae Kim, and Yong K Cho. Slam-driven intelligent au-tonomous mobile robot navigation for construction applications. In Advanced Comput-ing Strategies for Engineering: 25th EG-ICE International Workshop 2018, Lausanne,Switzerland, June 10-13, 2018, Proceedings, Part I 25, pages 254\u2013269. Springer, 2018.[361] Telegram chatbot. https:\/\/telegram.org, 2023. Accessed 2023-09-17.[362] Anthony Zhang. Speech recognition, 2017.[363] C\u00b8ag\u02d8lar F\u0131rat O\u00a8zgenel. Concrete crack segmentation dataset. Mendeley Data, 1:2019,2019.[364] Peter E Hart, Nils J Nilsson, and Bertram Raphael. A formal basis for the heuristicdetermination of minimum cost paths. IEEE transactions on Systems Science andCybernetics, 4(2):100\u2013107, 1968.[365] Weidong Wu, Murad A. Qurishee, Joseph Owino, Ignatius Fomunung, MbakisyaOnyango, and Babatunde Atolagbe. Coupling Deep Learning and UAV for Infras-tructure Condition Assessment Automation. 2018.[366] Damia` Fuentes Escote\u00b4. Djitellopy. https:\/\/github.com\/damiafuentes\/DJITelloPy, 2023. Retrieved 2023-08-29.[367] Tama\u00b4s Szepessy. Optical control for tello drone. https:\/\/github.com\/TamasSzepessy\/DJITelloOpticalControl, 2023. Retrieved 2023-08-29.135AppendicesAppendix AToward Generative Cyber-PhysicalSystemsThis appendix explores the utilization of natural language processing (NLP) as an essentialpart of human-robot interaction. There are numerous cases of leveraging NLP to enhancethe performance of robots, such as providing task descriptions and navigational commands.Using the traditional methods in language processing, modeling human-robot interactionposes challenges, as it requires the user to follow strict instructions, and the developerneeds to use complex mathematical algorithms to process several probability distributionsto choose an action [344]. On the other hand, the recent works in applications of neuralnetworks, such as reinforcement learning [345], for mapping from language to action requiresa significant amount of data for training. Several researchers have attempted to integratelanguage models into robotics systems during the past few years. However, most focuson specific scenarios using LLM features and are restrained to limited functions [346\u2013348].There is a growing demand for more sophisticated language models that interact with hu-mans more intuitively and naturally. Recent studies have demonstrated the great potentialof LLMs such as GPT-3 in robotics [349, 350], which have motivated the authors to explorethe capabilities of LLMs in combining the strengths of both natural language processingand code generation for cyber-physical systems.A.1 OverviewA seamless collaboration between inspectors and robots is critical [351, 352]. Since theintroduction of transformers [150], a new stream of applications has emerged based ontransformer models for robotics (e.g., control [353], object detection [26, 354], and naviga-tion [355]. The SayCan [356] and RT-1 [357] are two notable examples that successfullyincorporated LLM in robotics to map free-form text commands to value functions for takingactions. The former ranks different actions for a robot-specific library, while the latter usesan end-to-end approach without requiring high-level functions. In recent years, researchershave investigated the potential of LLMs for high-level robotics task planning without theneed for task-specific training data [358]. Such zero-shot models take user prompts andgenerate answers based on the pre-defined models [359].The present study differs from the earlier works because it utilizes ChatGPT (OpenAI),which leverages the capabilities of LLMs for mapping conversation to actions and inter-136A.2. Methodactively improves the behavior without re-engineering the system. This novel approachwould overcome the challenges of traditional symbolic AI for rule-based models and com-pose new functions for out-of-distribution data while keeping the classical AI and high-levelApplication Programming Interfaces (APIs) in the process.A.2 MethodThis study presents a novel pipeline for developing ChatGPT-based generative cyber-physical systems for structural health monitoring applications. This pipeline addressesvarious challenges, such as accurately describing the problem or task, listing appropriatefunctions and allowed APIs, and incorporating arguments to shape the response structure.For efficient utilization of ChatGPT, the following modules are considered in the currentstudy to construct generative robotics. The first module of the pipeline is to define a task-specific API with descriptions for ChatGPT to understand and map the logic between thetask and the chain of functions. The following module is engineering the prompt for Chat-GPT. The prompt should describe the task and provide the list of allowed functions and theformat of the responses. The final module involves human-robot interaction to evaluate andimprove the quality of the results by providing feedback to ChatGPT. Therefore, the finaloutput is deployed on the real robot after checking the quality and safety considerations.137A.2. Method     Develop API Library I want the UGV to inspect the site for damages and notify me. class UGV:   def __init__(self):        . . .    def get_distance(self, object_name): # Return the distance from the UGV to the detected object   def capture_rgb_image(self): # Capture an RGB image    def scan_surroundings(self): # Perform a 360-degree scan using the LiDAR sensor   def navigate_to(self, destination): # Plan and execute a path to navigate the UGV to the specified destination   . . .  As a 4WD robot on an inspection site, the goal is to move around and inspect the structural component map. The appropriate APIs were designed to perform the task. The following methods\/functions are allowed to be used:  detect_components(comp_name): takes the structural component name, and returns the location as a tuple of (bounding box, distance).  navigate_to(destination): takes destination coordinates as (x,y, z), and executes UGV.navigate_to() record_video(duration): takes the video duration, and starts recording a video  Engineer the initial prompt  Iteration on solution import cv2  while True:   cap = cv2.VideoCapture(0)   ...   ...   ChatGPT Code Extractor Prompt\/Chat Evaluate & Deploy Figure A.1: The pipeline of the proposed ChatGPT-enabled generative robotics.A.2.1 Prompt Engineering and API DevelopmentThe robotics field has undergone a significant transformation during the last decade. Giventhe field\u2019s maturity, it provides many well-established open-source libraries to support tasksin either perception or action. Such tasks include object detection [354], grasping ob-jects [26], path-planning [161], navigation [360], and mapping [197, 311]. An adequatelydesigned prompt would enable an LLM to efficiently utilize a list of pre-defined functions orgenerate functions according to the given information to perform reasoning and executionfor the robot.138A.2. MethodA.2.2 Engineering the reasoningThe most critical requirement in prompt engineering for LLMs is using descriptive namesfor all APIs. Therefore, clear and informative variable and function names will enableLLMs to generate the desired outcomes by finding the connections between the queriesand APIs. High-level functions are wrappers that encapsulate the implementation fromthe corresponding libraries. A list of such high-level functions and APIs must be givento the LLM to allow for creating logical connections between each function and behaveaccordingly. For instance, for a ground robot with vision sensors, a function named \u2019detectobject(objectName)\u2019 may only link computer vision-based modules such as OpenCV [292]and YOLO [90] for generating the results; in contrast, a function named \u2018move to(x,y,\u03b8)\u2019 mayinternally trigger a series of modules such as path-planning, obstacle avoidance, as well as thelow-level motion control algorithms for the motors. Figure A.2 illustrates a proper approachfor constructing an API and functions for a ground robotic system, which is expected toinspect an indoor environment, generate a map, or detect structural damage [354]. To ensurethe effective utilization of APIs for different scenarios, it is also necessary to provide clearand comprehensive explanations of all modules and sub-modules. This strategy ensures thatthe LLM is bounded by the functions that a system can execute and avoids the inevitablehallucination when no API is specified.  Imagine you are a 4WD ground robot equipped with an RGB-D camera and an RPLIDAR sensor. Your task is to autonomously navigate an indoor environment to create a map and perform various navigation-related tasks, such as reaching specific locations, avoiding obstacles, and identifying objects of interest.   At any time, i) you can ask a question for clarification; ii) explain what you did and why; iii) generate the Python code to perform a task. You can only use the following functions to control your ground robot.   move_forward(distance): Move the robot forward by the specified distance (in meters). turn_left( angle): Turn the robot left by the specified angle (in degrees). turn_right( angle): Turn the robot right by the specified angle (in degrees). get_robot_position(): Return the current position of the robot as (x, y) coordinates. get_object_distance(object_name): Return the distance from the robot to the specified object in meters. capture_rgb_image(): Capture an RGB image using the RGB-D camera. capture_depth_image(): Capture a depth image using the RGB-D camera. scan_surroundings(): Perform a 360-degree scan using the RPLIDAR sensor and return the scan data. navigate_to(destination): Plan and execute a path to navigate the robot to the specified destination (given as (x, y) coordinates).   Figure A.2: An instance of a practical approach for defining a task-specific API with explicitdescriptions.Unlike classical robotics, where all the relationships between the functions and tasksare well-defined, LLMs can generate new APIs relevant to a defined problem. For example,instead of using the functions described in Figure A.2, a user may ask a robot to \u201cmove3.6 m to the right and take 360\u00b0 rotation after every 1.2 m\u201d. This flexibility of LLMsindicates their robustness for robotics applications in different fields, which allows the user139A.2. Methodto re-design an existing strategy and improve it iteratively for a specific task. The proposedprinciples and an example of ChatGPT-enabled robotics applications are given in Table A.1.Unlike the previous LLMs, such as GPT-3 models [358], that require re-designing the singleprompt for new outputs, the dialog ability of ChatGPT allows initiating a chat with minimalhigh-level information about the problem and adding more details and clarifications in thesubsequent prompts for behavior correction in an interactive way.Table A.1: Principles for creating a robust initial prompt for an LLM.Prompt structure ExampleRobot API High-level navigation functionsConstraints Dimensions of an objectInitial state Position and orientation of the robotGoals and objectives Goal: scan a wall; objective: video frame rate & resolutionDiverse response ex-amplesHow\/when the robot can ask for clarificationA.2.3 Engineering the response structureDifferent methodologies would be utilized to add biases to the output of the LLMs, de-pending on the problem. For instance, a user may force the LLM to generate the responsesin specific formats or programming languages (i.e., Python). This strategy may lead tostandardized outputs that are efficient for parsing and real-time execution for robotic ap-plications. However, the free-form responses are usually less efficient for automatic parsing.For example, code blocks are generated after text paragraphs. Therefore, machine-readableformats such as XML tags would be significantly efficient for automatically parsing theoutputs. Such differences are illustrated in Figure A.3.140A.2. Method  Imagine you are helping me interact with a mobile ground robot deployed at a construction site. The robot is equipped with an RGB-D camera and LiDAR, and its mission is to perform tasks such as mapping, detecting specific objects, and inspecting the site for safety compliance.  All of your outputs need to be identified by one of the following tags: <question> Always ask questions for clarification if you are unsure <\/question> <reason> State your reason for certain actions <\/reason> <command> Output the Python codes toward the given task <\/command>  For example: Me: Update the map of the area around the concrete columns. You: <question> Could you please specify which part of the construction site you'd like me to map around the concrete columns? <\/question> Me: The area near the south entrance. You: <command> target_location = get_location(\"south_entrance\"); perform_mapping(target_location); <\/command> <reason> I identified the location of the south entrance and initiated the mapping process in that area. <\/reason>      Figure A.3: Efficient tagging of the outputs for automatic parsing.A.2.4 Speech recognitionTo enable the system to respond to voice commands more naturally, a Telegram Chat-bot [361] was designed with a multi-purpose API to receive text and voice from the user.This chatbot serves as the interface between the user and the computer, allowing for con-venient and intuitive input of commands. Once the main computer receives the voicecommands, they are processed using the Google Speech Recognition Engine through theSpeech Recognition library [362], which converts the audio input to text. With significantlyhigh confidence values, the generated text blocks are passed to ChatGPT to process andunderstand the user\u2019s intent and generate appropriate responses. Overall, this methodologyprovides a streamlined and efficient means of inputting voice commands to a computer whileensuring accurate interpretation and processing of those commands. The proposed methodenables the robot to understand better and respond to user voice commands by leveragingspeech recognition and LLM.141A.3. Evaluating LLM Capabilities for Enhanced Robotic Solutions: A Scenario-Based Analysis   LLM API Reason\/Question Code Code Extractor & Executer Microphone Recorded Audio Speech Recognition API Text Output Figure A.4: Communicating with the robot through a Telegram bot.A.3 Evaluating LLM Capabilities for Enhanced RoboticSolutions: A Scenario-Based AnalysisThis section presents different scenarios, from simple reasoning to advanced inspectionproblems, where the abilities of LLMs to solve robotics problems are investigated. It isworth mentioning that the proposed approach in this study enhances human capabilitiesinstead of replacing human involvement and making a fully autonomous cyber-physicalsystem. Despite the impressive capabilities of LLMs, it is important to prioritize the safetyconsiderations for real robotics problems. For this reason, a Secure Shell (SSH) terminalwas established to monitor the operations and send necessary safety commands to therobots. Alternatively, a simulated environment would be used for primitive experimentsand performance evaluations before deploying the model on a physical robot. A portablelaptop with 32 GB of RAM, Intel Core i7-12800H CPU, and NVIDIA RTX 3080Ti GPU isused as the Operation Control Unit (OCU) to deploy the computer vision algorithms.A.3.1 Scenario I: Zero-shot problem-solving using ChatGPTZero-shot learning and problem-solving refer to the ability of LLMs to generate meaningfuland accurate information based on the given prompt and without explicit training. There-fore, in a zero-shot method, an LLM cannot access the code examples and APIs, and theoutputs are generated solely based on the initial prompt. By leveraging this capability ofLLMs, it is possible to answer questions on novel topics, which makes it a valuable tool forinformation retrieval across a broad range of domains without specific training.Zero-shot reasoning: In this experiment, the user asked ChatGPT to generate theskeleton of concrete crack masks, which can be used as a preliminary tool to calculate142A.3. Evaluating LLM Capabilities for Enhanced Robotic Solutions: A Scenario-Based Analysiscrack length. The initial prompt and the generated response are shown in Figure A.5 andFigure A.6, which illustrate the zero-shot reasoning capability of LLMs. User Load the binary mask of a concrete crack (crack.png) and calculate the crack length using the skeleton of the mask.  ChatGPT import cv2 import numpy as np  # Load the binary mask mask = cv2.imread('crack.png', cv2.IMREAD_GRAYSCALE)  # Apply binary thresholding to create a binary image _, binary_image = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)  # Create a skeleton of the binary image element = cv2.getStructuringElement(cv2.MORPH_CROSS, (3, 3)) skeleton = np.zeros(binary_image.shape, np.uint8) done = False  while not done:     eroded = cv2.erode(binary_image, element)     temp = cv2.dilate(eroded, element)     temp = cv2.subtract(binary_image, temp)     skeleton = cv2.bitwise_or(skeleton, temp)     binary_image = eroded.copy()      zeros = np.size(binary_image) - cv2.countNonZero(binary_image)     if zeros == np.size(binary_image):         done = True  # Calculate the length of the crack crack_length = cv2.countNonZero(skeleton)  # Save the skeleton of the crack cv2.imwrite('crack_skeleton.png', skeleton)  # Report the length of the crack print(f\"Crack length: {crack_length} pixels\")  Figure A.5: Zero-shot code generation for measuring concrete crack length using ChatGPT.143A.3. Evaluating LLM Capabilities for Enhanced Robotic Solutions: A Scenario-Based Analysis(RGB) (Ground truth) (Output)Figure A.6: Code verification for generated codes for measuring concrete crack length (datafrom [363]).Zero-shot path planning: In this experiment, the user asked ChatGPT to find theoptimal path for a ground robot while avoiding obstacles. Using the traditional methods foroptimal path planning for a robot would be time-consuming and may differ from one case toanother. By providing an appropriate initial prompt, ChatGPT was able to find the shortestpath from \u201cSTART\u201d to \u201cGOAL\u201d positions given the \u201cMAP\u201d of obstacles (Figure A.7).144A.3. Evaluating LLM Capabilities for Enhanced Robotic Solutions: A Scenario-Based Analysis User Load the binary mask of the obstacle map (map.png), and find the best path for a robot between START and GOAL.  ChatGPT import heapq import cv2  # Define a class to represent nodes in the grid class Node:     def __init__(self, x, y):         self.x = x         self.y = y         self.parent = None         self.g = 0  # Cost from the start node to this node         self.h = 0  # Heuristic (estimated cost from this node to the goal)         self.f = 0  # Total cost (g + h)      # Define comparison operators for heapq     def __lt__(self, other):         return self.f < other.f  # A* algorithm function def astar(grid, start, goal):     # Define possible movement directions (4-connected neighbors)     directions = [(0, 1), (0, -1), (1, 0), (-1, 0)]     .     .     .  # Load the map Grid_map = cv2.imread('map.png', cv2.IMREAD_GRAYSCALE) ... # Find the optimum path using A* path = astar(grid_map, start_position, goal_position)  Figure A.7: Zero-shot code generation for robot path planning (The code is shortened forbrevity).In a zero-shot reasoning mode, the A* algorithm was used by ChatGPT by default [364].This behavior indicates that an LLM can re-produce a well-known algorithm by understand-ing the problem. Figure A.8 illustrates the results by executing and visualizing the Pythoncodes generated by ChatGPT.       (a) Input: map of obstacles (b) Output: optimal pathFigure A.8: Zero-shot path-planning between two points using ChatGPT.145A.3. Evaluating LLM Capabilities for Enhanced Robotic Solutions: A Scenario-Based AnalysisZero-shot real-world MAV flight control: Over the last decade, a growing interesthas been in utilizing marker-based navigation systems for autonomous flight control ofunmanned aerial vehicles (UAVs) in structural health monitoring [365]. Compared to thelarge UAVs, MAVs have gained popularity due to their size and maneuverability for indoorinspections. However, despite their advantages, MAVs pose limitations on the onboardsensors. Therefore, in MAVs, such as DJI Tello, a single built-in camera sends the videoframe to an external computer to perform image processing and vision-based control.In the last zero-shot experiment, the user asked ChatGPT to control a drone. The droneis DJI Tello Drone and is expected to do the primary navigation code-free. The initialprompt and the generated response are given in Figure A.9. It can be seen that ChatGPTcan reason about the model of the MAV and use the appropriate API functions. The modelcould generate complex code for navigation using the user\u2019s initial non-technical promptsolely. This behavior hints at the potential of LLMs beyond only text-based reasoning. User Generate Python codes to control a DJI Tello Drone. The drone will 1) take off, 2)make a 360 \u00b0 turn, and 3) land.  ChatGPT from djitellopy import Tello import time  tello = Tello()# Initialize Tello tello.connect()# Connect to Tello tello.streamon()# Enable video streaming  tello.takeoff()# Take off time.sleep(5)  # Time for takeoff tello.rotate_clockwise(360)  # Make a 360\u00b0 turn (yaw) time.sleep(5)  # Time for the turn tello.land()# Land time.sleep(5)  # Time for landing tello.streamoff()# Disable video streaming tello.disconnect()# Disconnect from Tello  Figure A.9: Zero-shot code generation for flight control using ChatGPT.A.3.2 Scenario II: User-LLM interaction for solving complex problemsThis scenario investigates when ChatGPT requires more information to generate a suitableresponse for complex problems. This interactive approach allows the system to learn small-scale skills to perform more complex tasks. Moreover, the textual feedback regarding thegenerated executable codes would help the LLM to map the non-technical inputs to thelow-level codes and apply changes.Marker-based navigation and data collection for 3D reconstruction: Visualmarkers are essential for vision-based methods, particularly in robotics navigation and mo-tion analysis. ArUco markers [292] have emerged as one of the prominent markers suitablefor tasks such as accurate pose estimation, localization, and navigation in robotics. Fur-thermore, ArUco markers are computationally efficient, making them highly suitable forreal-time tracking, human-machine interaction, and collaborative robotics. This illustra-tive example shows the user\u2019s request to generate a printable ArUco marker using Chat-GPT without further information. Camera calibration is required for vision-based decision-making. As shown in Figure A.10, ChatGPT can generate visual elements with precisionand accuracy by specifying the desired marker ID. Such seamless integration of NPL and146A.3. Evaluating LLM Capabilities for Enhanced Robotic Solutions: A Scenario-Based Analysiscomputer vision holds promise for further innovations. User Generate *.png files for 10 ArUco markers.  ChatGPT import cv2  import cv2.aruco as aruco # opencv-contrib-python==4.6.0.66  dict = aruco.Dictionary_get(aruco.DICT_6X6_250) for i in range(10):     marker = aruco.drawMarker(dict, i, 700)     cv2.imwrite(\"markers\/marker\" + str(i) + \".png\", marker)     ...  Outputs  Figure A.10: Generating ArUco marker using ChatGPT.As the follow-up prompts, the user asked ChatGPT to control the aerial and groundrobots using the ArUco markers in this experiment. As shown in Figure A.11, ChatGPTgenerates the appropriate Python script using OpenCV [292] and DJITelloPy [366] librariesfor this task. Although this experiment was conducted in a controlled environment (i.e.,the initial pose), a similar framework can be developed for arbitrary marker positions usingPID-based controllers [367]. Camera calibration is necessary to implement marker-basedcontrol for robots successfully. Figure A.12(b) shows the augmented reality cubes and thecorresponding local axes for efficient navigation, which can be used for detecting the safedistance and the accurate direction of the next move (Figure A.12(c)).147A.3. Evaluating LLM Capabilities for Enhanced Robotic Solutions: A Scenario-Based Analysis User Using the generated markers, control the DJI Tello drone. Follow the rules below: \u2022 Save every frame to \u201c.\/frames\/\u201d \u2022 Land if not marker detected after two 360 \u00b0 scans \u2022 Make the next move based on the marker ID: 1 (forward), 2 (forward), 3 (up),\u2026  ChatGPT from djitellopy import Tello import cv2 import cv2.aruco as aruco  aruco_dict = aruco.Dictionary_get(aruco.DICT_6X6_250)  tello = Tello() tello.connect() tello.streamon() tello.takeoff()  no_marker_count = 0  while True:     frame = tello.get_frame_read().frame      _, ids, _ = aruco.detectMarkers(frame, aruco_dict)      if ids is not None:         no_marker_count = 0         for i in range(len(ids)):             if ids[i] == 1: # Marker ID 1: go forward                 tello.move_forward(20)              elif ids[i] == 2: # Marker ID 2: go forward                 tello.move_forward(20)              elif ids[i] == 3: # Marker ID 3: turn left and go forward                 tello.rotate_counter_clockwise(90)                  tello.move_forward(20)                .                .                .     else:         no_marker_count += 1         if no_marker_count >= 2:             tello.land()      cv2.imshow(\"Tello\", frame)     if cv2.waitKey(1) & 0xFF == ord('q'):         break  cv2.destroyAllWindows() tello.streamoff() tello.end()  Figure A.11: Generating Python codes for controlling DJI Tello drone using ChatGPT.148A.3. Evaluating LLM Capabilities for Enhanced Robotic Solutions: A Scenario-Based Analysis   (a) MAV Navigation path (b) MAV and ArUco marker  (c) MAV FPV and direction    (a) UGV Navigation path (b) UGV and ArUco markers (c) UGV FPV and direction  Next Move: Up Next Move: Forward Figure A.12: Interactive marker-based navigation for UAVs using ChatGPT.As the final step, the user asks for a follow-up prompt to perform a 3D scene reconstruc-tion and point cloud generation using the recorded frames using the Structural-from-Motion(SfM) approach.149A.3. Evaluating LLM Capabilities for Enhanced Robotic Solutions: A Scenario-Based Analysis User Using the recorded video frames and COLMAP (.\/colmap\/bin), reconstruct dense point cloud PLY files and save to .\/output\/.  ChatGPT import os  colmap_bin_path = \".\/colmap\/bin\"   input_images_path = \"\/frames\"      output_folder = \".\/output\"          output_ply_file = output_folder + \"output.ply\"    # Step 1: Feature extraction and matching os.system(f'{colmap_bin_path}\/feature_extractor --database_path {output_folder}\/database.db --image_path {input_images_path}')  os.system(f'{colmap_bin_path}\/exhaustive_matcher --database_path {output_folder}\/database.db')  # Step 2: Sparse reconstruction os.system(f'{colmap_bin_path}\/mapper --database_path {output_folder}\/database.db --image_path {input_images_path} --output_path {output_folder}')  # Step 3: Dense reconstruction os.system(f'{colmap_bin_path}\/image_undistorter --image_path {input_images_path} --input_path {output_folder}\/dense\/0\/sparse --output_path {output_folder}\/dense\/0\/undistorted')  os.system(f'{colmap_bin_path}\/patch_match_stereo --workspace_path {output_folder}\/dense\/0 --workspace_format COLMAP')  os.system(f'{colmap_bin_path}\/stereo_fusion --workspace_path {output_folder}\/dense\/0 --workspace_format COLMAP')  # Step 4: Export dense point cloud as PLY os.system(f'{colmap_bin_path}\/poisson_mesher --input_path {output_folder}\/dense\/0\/fused.ply --output_path {output_ply_file}')   Figure A.13: Generating Python codes for 3D reconstruction using ChatGPT.(UAV) (UGV)Figure A.14: 3D reconstruction using the recorded video frame and the generated codes forCOLMAP.150A.3. Evaluating LLM Capabilities for Enhanced Robotic Solutions: A Scenario-Based AnalysisA.3.3 Scenario III: Embodied intelligence using deep learningTo further evaluate the potential of LLMs in reasoning about complex perception and actionproblems, a deep learning model was trained to detect and segment a specific componentduring a 3D construction site mapping (Structural Laboratory at The University of BritishColumbia). The overall framework of the ROS network for this experiment is illustrated inFigure A.15 for the custom-built UGV, and the object detection and segmentation modulesare explained in Figure A.16. Therefore, in the input prompt, the user must provide the out-line of the robot\u2019s mission, the inputs\/output, and the appropriate API library to generatethe executable Python codes. During the action phase, the detected objects were geo-taggedusing the depth camera and the robot\u2019s position. This approach can be generalized for fallhazard monitoring in construction sites.151A.3. Evaluating LLM Capabilities for Enhanced Robotic Solutions: A Scenario-Based Analysis User As a ground robot that is connected to a ROS network, the mission is to generate 3D map of the suroundings while inspecting.   Only the following following ROS launch files, under a catkin workspace name \u2018ugv_slam\u2019, are allowed to perform the tasks:  1. 'init_sensors.launch': initiates the realsense camera for L515 Lidar to publish RGB-D and point cloud data 2. 'init_navigation.launch': initiates the robot navigation for a given path 3. 'init_mapping.launch': starts 3D mapping using realsense lidar camera 4. 'detection_segmentation.launch':  publishes the topics for detected object, '\/obj_bbox', '\/obj_mask' during the mission.   Generate a single python code to: a) roslaunch to files listed above sensors.  b) watch if the ROS topic '\/obj_bbox' is not None (an object is detected); then, save the robot\u2019s position, current camera view, and segmentaiton mask  to '.\/detection_results\u2019.  Below are the list of available ROS topics: \/camera\/color\/image_raw; \/camera\/depth\/color\/points; \/clock; \/initialpose; \/odom; \/slam\/trajectory; \/tf; \/tf_static; \/obj_bbox; \/obj_mask;   ChatGPT import rospy import os import subprocess import rospkg import time from sensor_msgs.msg import Image from cv_bridge import CvBridge import cv2  rospy.init_node('mapping_and_detection', anonymous=True)  launch_files = ['init_sensors.launch','init_navigation.launch',     'init_mapping.launch','detection_segmentation.launch']  for launch_file in launch_files:     subprocess.Popen(['roslaunch', 'ugv_slam', launch_file])  bridge = CvBridge()  def save_detection_results(data):     timestamp = int(time.time())    ...    ...    ... while not rospy.is_shutdown():     try:         rospy.wait_for_message('\/obj_bbox', BoundingBox, timeout=10.0)           save_detection_results()     except rospy.ROSException:         rospy.loginfo(\"Waiting for object detection on '\/obj_bbox'...\")    Figure A.15: Generating Python codes for 3D mapping and detection using ChatGPT.152A.4. Conclusions  UGV\u2019s Camera UComponent Detection Component Segmentation Figure A.16: Validation of the real-time 3D mapping with integrated component detectionand segmentation.A.4 ConclusionsIn this study, an investigation into the feasibility of integrating large language models(LLMs) into cyber-physical systems (CPS) for structural health monitoring (SHM) appli-cations has been presented and evaluated. The research has introduced a novel frameworkfor achieving this integration, specifically focusing on generative aerial and ground roboticssystems designed to enhance human-machine interaction through natural language-basedcommunication. The development of task-specific APIs and engineering prompts has beenoutlined as the key components of the proposed pipeline. The experimental validation of theapproach involved utilizing a custom-built unmanned ground vehicle (UGV) and a micro-aerial vehicle (MAV) to execute a range of tasks, from simple control commands to complexAI-based sequences of actions.153Appendix BGround Robot DevelopmentResourcesIn this appendix, the Python class for the Unmanned Ground Vehicle (UGV) navigationand control, as well as the Arduino code for the UGV (Figure B.1), are provided to facilitatea comprehensive understanding and replication of the ground robot\u2019s development. Specif-ically, Code B.1 includes detailed documentation of the UGV Python class, the criticalcomponent of the robot\u2019s functionality. Furthermore, Code B.2 presents the Arduino coderesponsible for the hardware control and integration of the robot\u2019s motion.Figure B.1: The 4WD Ground Robot.Code B.1: Python code for the ground robot.1 import numpy as np23 import serial # pip install pyserial4 import time5 from pynput import keyboard678 # for Linux:9 # ser = serial.Serial(port=\u2019\/dev\/ttyAMC0 \u2019,10 # baudrate =9600 ,11 # bytesize=serial.EIGHTBITS ,12 # parity=serial.PARITY_NONE ,13 # stopbits=serial.STOPBITS_ONE ,14 # timeout=5,15 # xonxoff=False ,16 # rtscts=False ,17 # dsrdtr=False ,18 # writeTimeout =2)154Appendix B. Ground Robot Development Resources192021 class AGV:22 def __init__(self , port=\u2019COM11\u2019, baudrate =9600, timeout =.1,wheel_speed =30):23 self.max_speed = 20024 self.wheel_speed = wheel_speed25 self.motion_time = 1 # second26 self.cache = None27 self.moves = [\"moveForward\", \"moveSidewaysLeft\",\"moveSidewaysRight\", \"moveSidewaysLeft\", \"moveRightForward\",28 \"moveLeftForward\", \"moveRightBackward\",\"moveLeftBackward\", \"rotateRight\", \"rotateLeft\",29 \"rotateLeft\",30 \"stopMoving\"]3132 # self.move_uav = None33 print(\"wait ...\")34 time.sleep (1)35 print(\"Robot is ready!\")3637 # Default direction: LeftFrontWheel , LeftBackWheel ,RightFrontWheel ,RightBackWheel38 self.dir = np.array ([0, 0, 0, 0])39 self.serial = serial.Serial(port=port , baudrate=baudrate ,timeout=timeout)404142 def move(self , direction=\u2019rotate_right \u2019):43 if direction == self.cache:44 pass45 elif direction == \u2019moveForward \u2019:46 self.dir = np.array ([1, 1, 1, 1])47 elif direction == \u2019moveBackward \u2019:48 self.dir = np.array([-1, -1, -1, -1])49 elif direction == \u2019moveSidewaysRight \u2019:50 self.dir = np.array ([1, -1, -1, 1])51 elif direction == \u2019moveSidewaysLeft \u2019:52 self.dir = np.array([-1, 1, 1, -1])53 elif direction == \u2019rotateLeft \u2019:54 self.dir = np.array([-1, -1, 1, 1])55 elif direction == \u2019rotateRight \u2019:56 self.dir = np.array ([1, 1, -1, -1])57 elif direction == \u2019moveRightForward \u2019:58 self.dir = np.array ([1, 0, 0, 1])59 elif direction == \u2019moveRightBackward \u2019:60 self.dir = np.array ([0, -1, -1, 0])61 elif direction == \u2019moveLeftForward \u2019:62 self.dir = np.array ([0, 1, 1, 0])63 elif direction == \u2019moveLeftBackward \u2019:64 self.dir = np.array([-1, 0, 0, -1])65 return self6667 def stop(self):68 self.dir = np.array ([0, 0, 0, 0])155Appendix B. Ground Robot Development Resources69 return self7071 def test_move(self , wheel =0):72 self.dir = np.array ([0, 0, 0, 0])73 self.dir[wheel] = 174 return self7576 def keyboard_on_press(self , key):77 print(key)78 if key == self.cache:79 pass80 elif key == keyboard.Key.up:81 self.cache = key82 self.move(direction=\u2019moveForward \u2019).to_arduino ()83 elif key == keyboard.Key.down:84 self.cache = key85 self.move(direction=\u2019moveBackward \u2019).to_arduino ()86 elif key == keyboard.Key.right:87 self.cache = key88 self.move(direction=\u2019rotateRight \u2019).to_arduino ()89 # self.drone_move (20, 0, 0)90 elif key == keyboard.Key.left:91 self.cache = key92 self.move(direction=\u2019rotateLeft \u2019).to_arduino ()9394 elif key == keyboard.KeyCode.from_char(\u2019l\u2019):95 self.cache = key96 self.move(direction=\u2019moveSidewaysLeft \u2019).to_arduino ()97 elif key == keyboard.KeyCode.from_char(\u2019r\u2019):98 self.cache = key99 self.move(direction=\u2019moveSidewaysRight \u2019).to_arduino ()100101 elif key == keyboard.KeyCode.from_char(\u2019]\u2019):102 self.wheel_speed += 5103 elif key == keyboard.KeyCode.from_char(\u2019[\u2019):104 self.wheel_speed -= 5105106 def keyboard_on_release(self , key):107 self.stop().to_arduino ()108 self.cache = None109110 if key == keyboard.Key.esc:111 # Stop listener112 return False113114 def control_by_keyboard(self):115 \"\"\"116 Control the robots using the keyboard keys (listed above)117 \"\"\"118 with keyboard.Listener(119 on_press=self.keyboard_on_press ,120 on_release=self.keyboard_on_release) as listener:121 listener.join()122123 def calibrate(self):156Appendix B. Ground Robot Development Resources124 self.dir = [1, 1, -1, -1] # rotate right125 \u2019\u2019\u2019codes removed \u2019\u2019\u2019126 self.stop() # stop after calibrating127 return self128129 def to_arduino(self):130 servo_speeds = self.dir * self.wheel_speed131 serial_command = format(servo_speeds [0], \u201904d\u2019) +format(servo_speeds [1], \u201904d\u2019) + \\132 format(servo_speeds [2], \u201904d\u2019) +format(servo_speeds [3], \u201904d\u2019)133134 self.serial.write(bytes(serial_command , \u2019utf -8\u2019))135 received_data = self.serial.readline ()136 print(received_data)Code B.2: Arduino code for the ground robot1 #include <Encoder.h>23 int wheelSpeed = 20;4 int timedelay = 100;5 String readString , servo1 , servo2 , servo3 , servo4;67 \/\/ --- SPD Motor ---8 class SPDMotor {9 public:10 SPDMotor( int encoderA , int encoderB , bool encoderReversed , int motorPWM ,int motorDir1 , int motorDir2 );1112 \/\/\/ Set the PWM speed and direction pins13 void speed( int pwm );1415 \/\/\/ Activate a SHORT BRAKE mode16 void hardStop ();1718 \/\/\/ Get the current speed19 int getSpeed ();2021 \/\/\/ Get the current rotation position.22 long getEncoderPosition ();2324 private:25 Encoder *_encoder;26 bool _encoderReversed;27 int _motorPWM , _motorDir1 , _motorDir2;2829 \/\/ Current speed30 int _speed;31 };3233 SPDMotor :: SPDMotor( int encoderA , int encoderB , bool encoderReversed , intmotorPWM , int motorDir1 , int motorDir2 ) {34 _encoder = new Encoder(encoderA , encoderB);35 _encoderReversed = encoderReversed;36157Appendix B. Ground Robot Development Resources37 _motorPWM = motorPWM;38 pinMode( _motorPWM , OUTPUT );39 _motorDir1 = motorDir1;40 pinMode( _motorDir1 , OUTPUT );41 _motorDir2 = motorDir2;42 pinMode( _motorDir2 , OUTPUT );43 }4445 \/\/\/ Set the PWM speed and direction pins.46 void SPDMotor ::speed( int speedPWM ) {47 _speed = speedPWM;48 if( speedPWM == 0 ) {49 digitalWrite(_motorDir1 ,LOW);50 digitalWrite(_motorDir2 ,LOW);51 analogWrite( _motorPWM , 255);52 } else if( speedPWM > 0 ) {53 digitalWrite(_motorDir1 , LOW );54 digitalWrite(_motorDir2 , HIGH );55 analogWrite( _motorPWM , speedPWM < 255 ? speedPWM : 255);56 } else if( speedPWM < 0 ) {57 digitalWrite(_motorDir1 , HIGH );58 digitalWrite(_motorDir2 , LOW );59 analogWrite( _motorPWM , (-speedPWM) < 255 ? (-speedPWM): 255);60 }61 }6263 \/\/\/ Activate a SHORT BRAKE64 void SPDMotor :: hardStop () {65 _speed = 0;66 digitalWrite(_motorDir1 ,HIGH);67 digitalWrite(_motorDir2 ,HIGH);68 analogWrite( _motorPWM , 0);69 }7071 \/\/\/ Get the current speed72 int SPDMotor :: getSpeed () {73 return _speed;74 }7576 \/\/\/ Get the current rotation position.77 long SPDMotor :: getEncoderPosition () {78 long position = _encoder ->read();79 return _encoderReversed ? -position : position;80 }8182 SPDMotor *motorLF = new SPDMotor (18, 31, true , 12, 34, 35);83 SPDMotor *motorRF = new SPDMotor (19, 38, false , 8, 36, 37);84 SPDMotor *motorLR = new SPDMotor( 3, 49, true , 9, 43, 42);85 SPDMotor *motorRR = new SPDMotor( 2, A1, false , 5, A5, A4);868788 void setup (){8990 Serial.begin (9600);91 Serial.setTimeout (.1);158Appendix B. Ground Robot Development Resources92 }9394 void moveRobot(int LF, int LR, int RF, int RR)95 {9697 motorLF ->speed(LF); motorRF ->speed(RF);98 motorLR ->speed(LR); motorRR ->speed(RR);99 }100101 void loop() {102103 while (Serial.available ()) {104 delay (3);105 if (Serial.available () >0) {106 char c = Serial.read(); \/\/one byte from buffer107 readString += c; \/\/ makes the string readString108 }109 }110111 if (readString.length () >0) {112113 servo1 = readString.substring (0, 4);114 servo2 = readString.substring (4, 8);115 servo3 = readString.substring (8, 12);116 servo4 = readString.substring (12, 16);117118 int n1 , n2 , n3 , n4; \/\/ declare as number119 char carray1 [6], carray2 [6], carray3 [6], carray4 [6];120121 servo1.toCharArray(carray1 , sizeof(carray1));122 n1 = atoi(carray1);123124 servo2.toCharArray(carray2 , sizeof(carray2));125 n2 = atoi(carray2);126127 servo3.toCharArray(carray3 , sizeof(carray3));128 n3 = atoi(carray3);129130 servo4.toCharArray(carray4 , sizeof(carray4));131 n4 = atoi(carray4);132133134 moveRobot(n1 , n2 , n3 , n4);135 readString=\"\";136 }137 }159Appendix CSolid-state LiDAR UtilizationCodesThis appendix contains the Python class for the RealSense L515 LiDAR camera. Acquireddata from each frame are stored using the defined class Frame. The configuration of thecamera can be saved in a file (i.e., config.yaml). The code can read data from .bag filesor store the frames for offline post-processing. These codes were used in data collection andprocessing using the RealSense cameras (Figure C.1).Figure C.1: The RealSense RGB-D Cameras.Code C.1: Python class for utilizing RealSense L515 LiDAR camera.1 import time2 import logging3 import numpy as np4 import pyrealsense2 as rs5 import yaml6 from tkinter import *7 from tkinter import filedialog891011 class Frame(object):12 def __init__(self , color_frame , color_image , depth_frame , depth_image ,ir_frame , ir_image , point_cloud , accel ,13 gyro):14 self.color_frame = color_frame15 self.color_image = color_image1617 self.depth_frame = depth_frame18 self.depth_image = depth_image19160Appendix C. Solid-state LiDAR Utilization Codes20 self.ir_frame = ir_frame21 self.ir_image = ir_image2223 self.point_cloud = point_cloud24 self.accel = accel25 self.gyro = gyro262728 class L515:29 \"\"\" You can copy and adjust this class for other cameras \"\"\"30 def __init__(self , enable_rgbd=True , enable_imu=None , save_png=None ,31 record_bag=None , read_bag=None):3233 # self.reset = reset34 self.save_png = save_png3536 if enable_rgbd:37 self.enable_imu = True38 self.enable_rgbd = True39 else:40 self.enable_imu = enable_imu41 self.enable_rgbd = enable_rgbd4243 self.read_bag = read_bag44 self.read_bag_path = None45 self.record_bag = record_bag46 self.record_bag_path = None4748 self._cfg = yaml.load(\\49 open(\u2019.\/ sensors\/realsense_l515\/camera_config.yaml\u2019, \u2019r\u2019)\\50 , Loader=yaml.FullLoader)51 self.imu_fps = self._cfg[\u2019imu\u2019][\u2019fps\u2019]52 self.rgb_fps = self._cfg[\u2019rgb\u2019][\u2019fps\u2019]53 self.rgb_res = (self._cfg[\u2019rgb\u2019][\u2019width \u2019],self._cfg[\u2019rgb\u2019][\u2019height \u2019])54 self.depth_ir_res = (self._cfg[\u2019depth_infrared \u2019][\u2019width \u2019],self._cfg[\u2019depth_infrared \u2019][\u2019height \u2019])5556 # Configure streams57 self.points = rs.points ()58 self.pipeline = rs.pipeline ()59 config = rs.config ()6061 # Get device product line for setting a supporting resolution62 if not self.read_bag:63 profile = config.resolve(rs.pipeline_wrapper(self.pipeline))64 device = profile.get_device ()65 for i, s in enumerate(device.sensors):66 print(f\"Sensor ({i}): \", s.get_info(rs.camera_info.name))676869 if self.enable_rgbd:70 config.enable_stream(rs.stream.color , self.rgb_res [0],self.rgb_res [1], rs.format.bgr8 , self.rgb_fps)71 config.enable_stream(rs.stream.depth , self.depth_ir_res [0],161Appendix C. Solid-state LiDAR Utilization Codesself.depth_ir_res [1], rs.format.z16 ,72 self.rgb_fps)73 config.enable_stream(rs.stream.infrared , self.depth_ir_res [0],self.depth_ir_res [1], rs.format.y8,74 self.rgb_fps)75 config.enable_stream(rs.stream.confidence ,self.depth_ir_res [0], self.depth_ir_res [1], rs.format.raw8 ,76 self.rgb_fps)7778 if self.enable_imu:79 config.enable_stream(rs.stream.accel , rs.format.motion_xyz32f ,self.imu_fps) # acceleration80 config.enable_stream(rs.stream.gyro , rs.format.motion_xyz32f ,self.imu_fps) # gyroscope81 # config.enable_stream(rs.stream.pose , rs.format.six_dof ,self.imu_fps) #8283 if self.read_bag:84 root = Tk()85 # root.withdraw ()86 self.read_bag_path = filedialog.askopenfilename(initialdir =\\87 \u2019outputs\/bag\/\u2019)88 root.destroy ()89 config.enable_device_from_file(self.read_bag_path ,repeat_playback=True)90 print(f\u2019Reading data from {self.read_bag_path}\u2019)9192 elif self.record_bag:93 self.record_bag_path =time.strftime(\u2019outputs \\\\bag \\\\%Y%m%d_%H%M%S\u2019) + \u2019.bag\u201994 config.enable_record_to_file(self.record_bag_path)95 print(f\u2019Recording data to {self.record_bag_path}\u2019)9697 else:98 pass99100 # Start streaming101 profile = self.pipeline.start(config)102103 # Getting the depth sensor \u2019s depth scale104 if self.enable_rgbd:105 depth_sensor = profile.get_device ().first_depth_sensor ()106 self.depth_scale = depth_sensor.get_depth_scale ()107108 # Create an align object109 align_to = rs.stream.color110 self.align = rs.align(align_to)111112 # eat some frames to allow auto -exposure to settle113 for i in range(0, 5):114 self.pipeline.wait_for_frames ()115116117118162Appendix C. Solid-state LiDAR Utilization Codes119 def get_options(self):120 sensor = self.pipeline.get_active_profile ().get_device ().\\121 query_sensors ()[1]122 print(sensor.get_option(rs.option.exposure))123124 def set_options(self):125 # Get the sensor once at the beginning. (Sensor index: 1)126 sensor = self.pipeline.get_active_profile ().get_device ().\\127 query_sensors ()[1]128129 # set options from config file.130 for opt in self._cfg[\u2019options \u2019]:131 if not self._cfg[\u2019options \u2019][opt] == \u2019None\u2019:132 for key in rs.option.__dict__.keys():133 if str(key) == opt:134 sensor.set_option(rs.option.exposure ,self._cfg[\u2019options \u2019][opt])135 print(f\"setting {key}={ self._cfg[\u2019options \u2019][opt]}\")136137138 def get_frame(self):139 # Get frameset of color and depth140 frameset = self.pipeline.wait_for_frames ()141142 color_frame = frameset.get_color_frame ()143 depth_frame = frameset.get_depth_frame ()144 ir_frame = frameset.get_infrared_frame ()145146 # Align the depth frame to color frame147 aligned_frames = self.align.process(frameset)148149 # Get aligned frames150 aligned_depth_frame = aligned_frames.get_depth_frame ()151 aligned_ir_frame = aligned_frames.get_infrared_frame ()152153 # Validate that both frames are valid154 if not aligned_depth_frame or not color_frame:155 print(\" Not Valid!\")156 pass157158 depth_image = np.asanyarray(aligned_depth_frame.get_data ())159 color_image = np.asanyarray(color_frame.get_data ())160 ir_image = np.asanyarray(aligned_ir_frame.get_data ())161162 pc = rs.pointcloud ()163 point_cloud = pc.calculate(aligned_depth_frame)164165 if self.enable_imu:166167 acceleration = aligned_frames.first_or_default(rs.stream.accel ,\\168 rs.format.motion_xyz32f).as_motion_frame ().\\169 get_motion_data ()170 accel = [acceleration.x, acceleration.y, acceleration.z]171172 gyroscope = aligned_frames.first_or_default(rs.stream.gyro ,\\163Appendix C. Solid-state LiDAR Utilization Codes173 rs.format.motion_xyz32f).as_motion_frame ().get_motion_data ()174 gyro = [gyroscope.x, gyroscope.y, gyroscope.z]175176177 return Frame(color_frame , color_image , aligned_depth_frame ,depth_image , aligned_ir_frame , ir_image ,178 point_cloud , accel , gyro)179180 else:181 return Frame(color_frame , color_image , aligned_depth_frame ,depth_image , aligned_ir_frame , ir_image ,182 point_cloud , None , None)183184185 def clip_distance(self , depth_image , color_image ,186 clipping_distance1 , clipping_distance2):187188 clipping_distance1 = clipping_distance1 \/ self.depth_scale189 clipping_distance2 = clipping_distance2 \/ self.depth_scale190 grey_color = 153191 depth_image_3d = np.dstack (( depth_image , depth_image , depth_image))192 depth_clipped = np.where(( depth_image_3d <= clipping_distance1) |(depth_image_3d > clipping_distance2),193 grey_color , color_image)194195 return depth_clipped196197 def get_w_h(self):198 # Get stream profile and camera intrinsics199 profile = self.pipeline.get_active_profile ()200 depth_profile = rs.video_stream_profile (\\201 profile.get_stream(rs.stream.depth))202 depth_intrinsics = depth_profile.get_intrinsics ()203 w, h = depth_intrinsics.width , depth_intrinsics.height204205 return w, h206207208209 def get_gyro_data(gyro):210 return np.asarray ([gyro.x, gyro.y, gyro.z])211212 def get_accel_data(accel):213 return np.asarray ([accel.x, accel.y, accel.z])164Appendix DCodes for Creating COCOAnnotation Files (*.json)This appendix provides the Python for generating COCO annotation files (*.json) us-ing segmentation masks. This code facilitates training object detection models that useCOCO-style data annotation, such as DETR. These codes were used for ground truth dataannotations using masks only (green screen).(a) Input mask (b) Output annotationsFigure D.1: COCO annotation results.Code D.1: Python class to create COCO annotation files.1 import glob2 import os3 import json4 from PIL import Image5 from shapely.geometry import Polygon , MultiPolygon6 from skimage import measure7 import numpy as np8 import cv29 from itertools import chain1011 class COCO():12 def __init__(self):13 self.coco_instance = {}14 self.coco_instance[\"info\"] = {}15 self.coco_instance[\"licenses\"] = []16 self.coco_instance[\"images\"] = []165Appendix D. Codes for Creating COCO Annotation Files (*.json)17 self.coco_instance[\"annotations\"] = []18 self.coco_instance[\"categories\"] = []1920 self.coco_panoptic = {}21 self.coco_panoptic[\"info\"] = {}22 self.coco_panoptic[\"licenses\"] = []23 self.coco_panoptic[\"images\"] = []24 self.coco_panoptic[\"annotations\"] = []25 self.coco_panoptic[\"categories\"] = []2627 self.color_palette = {}28 self.cache_image_id = 029 self.cache_file_name = None30 self.cache_category_id = 031 self.cache_insSeg_id = 032 self.cache_panSeg_id = 033 self.images_info = None #3435 def load_categories(self):36 with open(\u2019panoptic_coco_categories.json\u2019, \u2019r\u2019) as f:37 self.images_info = json.load(f)38 for cat in self.images_info[\u2019categories \u2019]:39 instance_category = {40 \"supercategory\": cat[\u2019supercategory \u2019],41 \"isthing\": cat[\u2019isthing \u2019],42 \"id\": cat[\u2019id\u2019],43 \"name\": cat[\u2019name\u2019],44 \"color\": cat[\u2019color\u2019]45 }46 panoptic_category = {47 \"supercategory\": cat[\u2019supercategory \u2019],48 \"isthing\": cat[\u2019isthing \u2019],49 \"id\": cat[\u2019id\u2019],50 \"name\": cat[\u2019name\u2019],51 \"color\": cat[\u2019color\u2019]52 }5354 self.coco_instance[\"categories\"].\\55 append(instance_category)56 self.coco_panoptic[\"categories\"].\\57 append(panoptic_category)58 self.color_palette[cat[\u2019id\u2019]] = cat[\u2019color \u2019]5960 self.color_palette_list = list(chain.from_iterable (\\61 [self.color_palette[key] for key in self.color_palette ]))6263 def create_annotations(self , mask_image , imshow=False):64 self.cache_file_name = os.path.basename(mask_image)\\65 .split(\".\")[0] + \".jpg\"666768 # 1. Load mask image69 mask = cv2.imread(mask_image , cv2.IMREAD_GRAYSCALE)70 # update the coco format with the image info71 image = {166Appendix D. Codes for Creating COCO Annotation Files (*.json)72 \"file_name\": self.cache_file_name ,73 \"height\": mask.shape[0],74 \"width\": mask.shape[1],75 \"id\": self.cache_image_id76 }77 self.coco_instance[\"images\"]. append(image)78 self.coco_panoptic[\"images\"]. append(image)7980 if imshow:81 cv2.imshow(f\"mask {self.cache_file_name}\", mask)82 cv2.waitKey (0)83 cv2.destroyAllWindows ()8485 # 2. Create sub -masks86 sub_masks = self.create_sub_masks(mask)8788 # 3. Create annotations89 self.create_annotations_from_sub_masks(sub_masks)9091 def create_sub_masks(self , mask):92 sub_masks = {}93 for sub_mask_id in np.unique(mask):94 sub_mask = mask == sub_mask_id95 sub_mask = sub_mask.astype(np.uint8)96 sub_masks[str(sub_mask_id)] = sub_mask97 # print(np.unique(mask))98 return sub_masks99100 def create_annotations_from_sub_masks(self , sub_masks):101102 pan_annotation = {\u2019segments_info \u2019: [],103 \u2019file_name \u2019: self.cache_file_name .\\104 split(\".\")[0]+\u2019.png\u2019,105 \u2019image_id \u2019: self.cache_image_id}106107 # 4. Create annotations108 for sub_mask_id , sub_mask in sub_masks.items():109 self.cache_category_id = int(sub_mask_id) \/\/self.images_info[\u2019info\u2019][110 \u2019categories_color_bin_size \u2019]111112 contours , hierarchy = \\113 cv2.findContours(sub_mask , cv2.RETR_EXTERNAL ,cv2.CHAIN_APPROX_SIMPLE)114115 for contour in contours:116 if cv2.contourArea(contour) > 0:117 self.coco_instance[\"annotations\"]. append ({118 \"iscrowd\": 0,119 \"id\": self.cache_insSeg_id ,120 \"image_id\": self.cache_image_id ,121 \"category_id\": self.cache_category_id ,122 \"bbox\": cv2.boundingRect(contour),123 \"area\": cv2.contourArea(contour),124 \"segmentation\": [contour.flatten ().tolist ()]})167Appendix D. Codes for Creating COCO Annotation Files (*.json)125 self.cache_insSeg_id += 1126127 pan_annotation[\u2019segments_info \u2019]. append ({128 \"id\": self.cache_panSeg_id ,129 \"category_id\": self.cache_category_id ,130 \"iscrowd\": 0,131 \"bbox\": cv2.boundingRect(contour),132 \"area\": cv2.contourArea(contour)})133 self.cache_panSeg_id += 1134135 self.coco_panoptic[\"annotations\"]. append(pan_annotation)136137138 if __name__ == \"__main__\":139140 subsets = [\"train\", \"val\", \"test\"]141142143 for subset in subsets:144145 my_coco = COCO()146 my_coco.load_categories () #147148 for mask_image in glob.glob(f\"coco_panoptic \/{ subset }\/*. png\"):149 my_coco.create_annotations(mask_image , imshow=False)150 my_coco.cache_image_id += 1151152153 # Save the coco format154 with open(f\"annotations\/instances_{subset }.json\", \u2019w\u2019) as f:155 json.dump(my_coco.coco_instance , f)156 with open(f\"annotations\/panoptic_{subset }.json\", \u2019w\u2019) as f:157 json.dump(my_coco.coco_panoptic , f)158159 if my_coco.cache_image_id == 0:160 print(\"No annotations created!\")168","attrs":{"lang":"en","ns":"http:\/\/www.w3.org\/2009\/08\/skos-reference\/skos.html#note","classmap":"oc:AnnotationContainer"},"iri":"http:\/\/www.w3.org\/2009\/08\/skos-reference\/skos.html#note","explain":"Simple Knowledge Organisation System; Notes are used to provide information relating to SKOS concepts. There is no restriction on the nature of this information, e.g., it could be plain text, hypertext, or an image; it could be a definition, information about the scope of a concept, editorial information, or any other type of information."}],"Genre":[{"label":"Genre","value":"Thesis\/Dissertation","attrs":{"lang":"en","ns":"http:\/\/www.europeana.eu\/schemas\/edm\/hasType","classmap":"dpla:SourceResource","property":"edm:hasType"},"iri":"http:\/\/www.europeana.eu\/schemas\/edm\/hasType","explain":"A Europeana Data Model Property; This property relates a resource with the concepts it belongs to in a suitable type system such as MIME or any thesaurus that captures categories of objects in a given field. It does NOT capture aboutness"}],"GraduationDate":[{"label":"GraduationDate","value":"2024-05","attrs":{"lang":"en","ns":"http:\/\/vivoweb.org\/ontology\/core#dateIssued","classmap":"vivo:DateTimeValue","property":"vivo:dateIssued"},"iri":"http:\/\/vivoweb.org\/ontology\/core#dateIssued","explain":"VIVO-ISF Ontology V1.6 Property; Date Optional Time Value, DateTime+Timezone Preferred "}],"IsShownAt":[{"label":"IsShownAt","value":"10.14288\/1.0441289","attrs":{"lang":"en","ns":"http:\/\/www.europeana.eu\/schemas\/edm\/isShownAt","classmap":"edm:WebResource","property":"edm:isShownAt"},"iri":"http:\/\/www.europeana.eu\/schemas\/edm\/isShownAt","explain":"A Europeana Data Model Property; An unambiguous URL reference to the digital object on the provider\u2019s website in its full information context."}],"Language":[{"label":"Language","value":"eng","attrs":{"lang":"en","ns":"http:\/\/purl.org\/dc\/terms\/language","classmap":"dpla:SourceResource","property":"dcterms:language"},"iri":"http:\/\/purl.org\/dc\/terms\/language","explain":"A Dublin Core Terms Property; A language of the resource.; Recommended best practice is to use a controlled vocabulary such as RFC 4646 [RFC4646]."}],"Program":[{"label":"Program","value":"Civil Engineering","attrs":{"lang":"en","ns":"https:\/\/open.library.ubc.ca\/terms#degreeDiscipline","classmap":"oc:ThesisDescription","property":"oc:degreeDiscipline"},"iri":"https:\/\/open.library.ubc.ca\/terms#degreeDiscipline","explain":"UBC Open Collections Metadata Components; Local Field; Indicates the program for which the degree was granted."}],"Provider":[{"label":"Provider","value":"Vancouver : University of British Columbia Library","attrs":{"lang":"en","ns":"http:\/\/www.europeana.eu\/schemas\/edm\/provider","classmap":"ore:Aggregation","property":"edm:provider"},"iri":"http:\/\/www.europeana.eu\/schemas\/edm\/provider","explain":"A Europeana Data Model Property; The name or identifier of the organization who delivers data directly to an aggregation service (e.g. Europeana)"}],"Publisher":[{"label":"Publisher","value":"University of British Columbia","attrs":{"lang":"en","ns":"http:\/\/purl.org\/dc\/terms\/publisher","classmap":"dpla:SourceResource","property":"dcterms:publisher"},"iri":"http:\/\/purl.org\/dc\/terms\/publisher","explain":"A Dublin Core Terms Property; An entity responsible for making the resource available.; Examples of a Publisher include a person, an organization, or a service."}],"Rights":[{"label":"Rights","value":"Attribution-NonCommercial-NoDerivatives 4.0 International","attrs":{"lang":"*","ns":"http:\/\/purl.org\/dc\/terms\/rights","classmap":"edm:WebResource","property":"dcterms:rights"},"iri":"http:\/\/purl.org\/dc\/terms\/rights","explain":"A Dublin Core Terms Property; Information about rights held in and over the resource.; Typically, rights information includes a statement about various property rights associated with the resource, including intellectual property rights."}],"RightsURI":[{"label":"RightsURI","value":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","attrs":{"lang":"*","ns":"https:\/\/open.library.ubc.ca\/terms#rightsURI","classmap":"oc:PublicationDescription","property":"oc:rightsURI"},"iri":"https:\/\/open.library.ubc.ca\/terms#rightsURI","explain":"UBC Open Collections Metadata Components; Local Field; Indicates the Creative Commons license url."}],"ScholarlyLevel":[{"label":"ScholarlyLevel","value":"Graduate","attrs":{"lang":"en","ns":"https:\/\/open.library.ubc.ca\/terms#scholarLevel","classmap":"oc:PublicationDescription","property":"oc:scholarLevel"},"iri":"https:\/\/open.library.ubc.ca\/terms#scholarLevel","explain":"UBC Open Collections Metadata Components; Local Field; Identifies the scholarly level of the author(s)\/creator(s)."}],"Supervisor":[{"label":"Supervisor","value":"Yang, Tony T. Y.","attrs":{"lang":"en","ns":"http:\/\/purl.org\/dc\/terms\/contributor","classmap":"vivo:AdvisingRelationship","property":"dcterms:contributor"},"iri":"http:\/\/purl.org\/dc\/terms\/contributor","explain":"A Dublin Core Terms Property; An entity responsible for making contributions to the resource.; Examples of a Contributor include a person, an organization, or a service."}],"Title":[{"label":"Title","value":"Development and implementation of multi-agent cyber-physical systems for vision-based structural health monitoring","attrs":{"lang":"en","ns":"http:\/\/purl.org\/dc\/terms\/title","classmap":"dpla:SourceResource","property":"dcterms:title"},"iri":"http:\/\/purl.org\/dc\/terms\/title","explain":"A Dublin Core Terms Property; The name given to the resource."}],"Type":[{"label":"Type","value":"Text","attrs":{"lang":"en","ns":"http:\/\/purl.org\/dc\/terms\/type","classmap":"dpla:SourceResource","property":"dcterms:type"},"iri":"http:\/\/purl.org\/dc\/terms\/type","explain":"A Dublin Core Terms Property; The nature or genre of the resource.; Recommended best practice is to use a controlled vocabulary such as the DCMI Type Vocabulary [DCMITYPE]. To describe the file format, physical medium, or dimensions of the resource, use the Format element."}],"URI":[{"label":"URI","value":"http:\/\/hdl.handle.net\/2429\/87752","attrs":{"lang":"en","ns":"https:\/\/open.library.ubc.ca\/terms#identifierURI","classmap":"oc:PublicationDescription","property":"oc:identifierURI"},"iri":"https:\/\/open.library.ubc.ca\/terms#identifierURI","explain":"UBC Open Collections Metadata Components; Local Field; Indicates the handle for item record."}],"SortDate":[{"label":"Sort Date","value":"2024-12-31 AD","attrs":{"lang":"en","ns":"http:\/\/purl.org\/dc\/terms\/date","classmap":"oc:InternalResource","property":"dcterms:date"},"iri":"http:\/\/purl.org\/dc\/terms\/date","explain":"A Dublin Core Elements Property; A point or period of time associated with an event in the lifecycle of the resource.; Date may be used to express temporal information at any level of granularity. Recommended best practice is to use an encoding scheme, such as the W3CDTF profile of ISO 8601 [W3CDTF].; A point or period of time associated with an event in the lifecycle of the resource.; Date may be used to express temporal information at any level of granularity. Recommended best practice is to use an encoding scheme, such as the W3CDTF profile of ISO 8601 [W3CDTF]."}]}